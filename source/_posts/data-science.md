---
title: 数据科学家养成计划手册(持续更新中)
date: 2021-01-02 13:58:55
tags: data science
categories: 数据科学
---


本人是一名数据爱好者，梦想是成为一名数据科学家。这里记录我从零开始的学习过程。内容若有什么不对的，欢迎大家批评指正，也希望能和感兴趣的读者一起探讨。

> 注1：数据科学家有两条发展路径，一条是偏向业务可视化的商业分析师（数据分析师），一条是偏向于建模算法的数据挖掘工程师。这里不予严格区分。本文旨在展示数据获取、处理、建模、分析、可视化等全过程。读者可根据自己的喜好选择一个方向深入。

> 注2：本文长期维护

## 数据分析全貌

数据分析是一门利用统计学知识，从数据中提取有用信息，进行总结和概括的学科。

一名优秀的数据分析师应该具备的能力：`好奇`、`谨慎`、`责任`。

在实际的工作中，每个分析师应该有自己处理问题的工作流程，并在实际的工作中不断的完善和迭代。最基本的流程如下所示：  
`数据工作流`：抛出问题 -> 获取数据 -> 数据研究 -> 问题结论 -> 解决办法

`数据建模和挖掘具体流程`：数据获取 -> 探索性分析及可视化 -> 数据预处理 -> 数据挖掘建模 -> 模型评估

前两个流程（数据获取、探索性分析与可视化）也是狭义的数据分析。

`前修知识`：数学(高数、概率论与数理统计)、统计学、python(或者 R)；（sql、excel 本文暂不探讨）

涉及的python第三方库：

* 数据科学包：Numpy(数据基础包)、Pnadas(数据处理神器)、Scipy(科学计算包)、Matplotlib(可视化)、Seaborn(可视化)
* 机器学习包：Scikit-learn(传统机器学习)、TensorFlow(深度学习)

`工具`：Visual Studio Code(Jupyter插件)

## 数据获取

### 数据仓库

`定义`：将所有的业务数据汇总处理，构成数据仓库(DW)

* 全面、完备、尽可能详细的记录全部事实
    * 如某人几分几秒浏览了什么页面
* 方便对部分维度和数据的整理(数据集市-DM)

`数据仓库与文件和日志的区别`：文件、日志只能顺序记录，不方便查找、比较、抽取特征等操作

`数据仓库与数据库的区别`：

* 数据仓库：
    * 面向主题存储
        * 主题：较高层次上对分析对象数据的一个完整并且一致的描述
        * 如购买图书这个行为就是个主题，在几时几分几秒以什么样的价格购买了什么样的书就是这个购买主题的一个记录，记录里有时间有用户信息，有图书信息等各个维度的信息。主题就是各个数据相互联系的描述。
    * 针对分析(OLAP)
    * 可能冗余、相对变化较大、数据量大
* 数据库
    * 面向业务存储
        * 高并发、快速读写、数据结构精简
    * 针对应用(OLTP)
        * 为用户提供数据上的支持和服务
    * 组织规范

### 检测与抓取(爬虫)

`检测`：用检测设备和检测算法直接获取数据。如传感器网络

`抓取(爬虫)`：直接通过获取网页内容进行解析和分析，直接解析网页、接口和文件的信息。

* python常用库：
    * 抓取：urllib、urllib2、requests、scrap
    * 渲染：PhantomJS
    * 解析：beautifulSoup、Xpath(lxml)

### 填写、日志、埋点

`用户填写`：用户填写的信息。如调查问卷、用户注册时填写的信息

`操作日志`：以文件形式记录。

* 前端日志：网页和APP中记录的日志
* 后端日志：服务器的日志

`埋点`：在APP或网页应用中针对特定的流程收集一定的信息，用来跟踪APP或网页被使用的情况，以便后继用来进一步优化产品或进行运营支持。

* 比较常用的记录项：访问、访客、停留时间、页面查看和跳出率。
* 分为页面统计和统计操作行为。
* 可自己开发也可选择第三方工具，如友盟。

### 计算

`计算`：通过已有的数据计算生成衍生数据。如企业的投入产出比

### 数据学习网站

`数据竞赛网站`：kaggle & 天池  
`图片数据集网站`：ImageNet、Open Images  
`各领域统计数据`：统计局、政府机构、公司财报等

## 探索性分析与数据可视化

### 统计分析方法基础

`集中趋势`：均值、中位数与分位数、众数  
四分位数的计算方法：

```
# n为数据的个数  
Q1位置 = (n+1)*25  
Q2位置 = (n+1)*5  
Q3位置 = (n+1)*75
```

`离中趋势`：标准差、方差

`数据分布`：偏态与峰态、正态分布与三大分布

* 偏态系数：数据平均值偏离状态的一种衡量
* 峰态系数：数据分布集中强度的衡量
* 分布概率
    * 正态分布
    * 卡方分布: 标准正态分布的平方和
    * $T$分布：用于根据小样本来估计呈正态分布且方差未知的总体的均值
    * $F$分布

`抽样理论`

* 抽样原因：
    * 数据量异常大，全量计算的时间、成本都很大。(大数据发展，不再是问题)
    * 全量检测不现实。比如测灯泡的寿命
* 抽样类型：
    * 重复抽样(有放回的抽样)
    * 不重复抽样(无放回的抽样)
* 抽样方式
    * 完全随机抽样
    * 等差距抽样：某个属性从低到高排列，等间距抽样
    * 分类的分层抽样：根据各个类别的比例抽样
* 抽样平均误差计算公式
    * 重复抽样
    * 不重复抽样
* 估计总体时抽样数量的确定
    * 重复抽样
    * 不重复抽样

`数据分类`

* 定类(类别)：根据事物离散、无差别属性进行分类。如性别、名族
* 定序(顺序)：可以界定数据的大小、但不能测定差值。如收入的高、中、低
* 定距(间距)：可以界定数据大小同时可测定差距，但无绝对零点(乘除比率无意义)。如温度
* 定比(比率)：可以界定数据大小同时可测定差距，有绝对零点。如身高、体重

`假设检验与方差检验`  
假设检验：做出一个假设，根据数据或已知的分布性质来推断这个假设成立的概率有多大。

* 建立原假设 H0 (包含等号)，H1是H0的反命题，也称备择假设
* 选择检验统计量
* 根据显著性水平(一般为0.05)，确定拒绝域
* 计算`P值`或`样本统计量`，做出判断

检验统计量：

* $\\mu$ 分布
* 卡方分布：用于检测两个因素之间有无强联系
* $T$检验：比较两组样本分布是否一致；两组值的均值有无较大差异
* $F$检验（方差检验)：用于方差分析。多样本两两之间是否有差异。
    * 总变差平方和
    * 平均平方和/组内平方和
    * 残差平方和/组内平方差
    * 统计量

> 注：也可以通过qq图来判断一个分布是否符合一个已知的分布，比如找到该分布的分位数做纵轴，正态分布的分位数做横轴，若连线接近角平分线，则符合

`相关系数`：衡量两组数据或两组样本的分布趋势、变化趋势一致性程度的因子。

* 皮尔逊(Pearson)相关系数
* 斯皮尔(Spearman)曼相关系数

`回归：线性回归`：确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法

* 关键度量指标：
    * 一元决定系数
    * 多元决定系数
* 残差不相关(DW检验)

`PCA与奇异值分解`：尽可能少的失真情况下，线性降维，成分提取

* 主成分分析法(PCA)

    * 求特征协方差矩阵
    * 求协方差矩阵的特征值和特征向量
    * 将特征值按照从大到小的顺序排序，选取其中最大的k个
    * 将样本点投影到选取的特征向量上
* 奇异值分解(SVD)

    * 特征矩阵 A 分解为 m_m 的酉阵，m_n 半正定矩阵(奇异矩阵)，n*n 酉阵转置 v

### 探索性分析

#### 单因子探索

——展现数据全貌

* 异常值分析(可用箱线图展示)
    * 连续异常值
        * 处理办法：舍去；异常值用边界值代替(四分位数)
    * 离散异常值：离散属性定义范围外的所有值均为异常值。如空值；收入离散成高、中、低之外的值
        * 处理办法：舍去；把所有异常值当作单独的一个值处理
    * 常识异常值：在限定知识和常识范围外的所有值均为异常值。如身高20m
* 对比分析
    * 比什么：
        * 绝对数比较：数值直接比较。如比较收入、身高
        * 相对数比较：把几个有联系的指标联合成新的指标
            * 结构相对数：部分与整体。如合格率、通过率
            * 比例相对数：总体内用不同部分的数值进行比较。如三大产业相互比较
            * 比较相对数：同一时空下相似或同质的指标进行比较。如不同时期同一产品的价格对比；不同互联网电商公司的待遇水平对比
            * 动态相对数：一般包含时间概念。如用户增速
            * 强度相对数：性质不同但又互相联系的属性进行联合。如人均、亩产、密度
    * 怎么比：
        * 时间
            * 同比：今年2月同去年2月比
            * 环比：今年2月同今年1月比
        * 空间
            * 现实方位：如不同国家、不同城市
            * 逻辑上空间：如一家公司的不同部门；不同家公司之间的比较
        * 经验和计划：如历史上失业率达到百分之几回发生暴乱，把国家的失业率与之比较；工作排期与实际进度之间的比较
* 结构分析：各组成部分的分布与规律
    * 静态：直接分析总体的组成。如十一五时间三大产业的比例
    * 动态：时间为轴分析结构变化的趋势。如十一五期间三大产业比的变化
* 分布分析：数据分布频率的显示分析
    * 直接获取的概率分布
    * 判断是否为正态分布
        * 偏态系数
        * 峰态系数
    * 极大似然

#### 多因子探索

——探索属性与属性之间的联系

* 交叉分析
    * 热力图
    * 透视表
* 分组与钻取
* 相关分析
* 因子分析
* 聚类分析(建模中也会用到)
* 回归分析(建模中也会用到)

### 数据可视化

* 柱状图：横坐标表示离散值
* 直方图：横坐标表示范围
* 箱线图
* 折线图
* 饼图

## 数据预处理

### 缺失值、离群值

`缺失值`：

* 删除：缺失样本非常大(>75%)，则删除整条数据
* 填充：缺失量<10%
    * 若为正态分布，取均值
    * 若为偏态，取中位数
* 预测样本值：使用与缺失值相比相关性非常高的特征建立模型，预测缺失值

`离群点`：远离数据主要部分的样本(极大值或极小值)

* 同单因子探索分析的异常值处理相同：删除或填充

### 标准化、纠偏

`标准化`：去除数量纲(单位)的影响，提高模型的解释度，加快模型的收敛速度。具体方法如下：

* 中心化：减去均再除以标准差(之后均值为0，标准差为1)
* 01标准化：减去最小值再除以最大值和最小值的差

`纠偏`

* 正态分布：数据呈现对称的钟态分布
* 右偏态：样本大量集中在均值的左边(均值偏到了右边)
* 左偏态：样本大量集中在均值的右边(均值偏到了左边)
* 处理方法：
    * 右偏态：常用对数函数处理
    * 左偏态：常用指数函数处理
* 通用变换方法：以降低数据的偏态系数为目的，使得数据分布更加接近正态分布的变换方法。
    * yeo-johnson 变换：可以处理包含正数、负数和零的变量
    * box-cox 变换：只能处理数值皆为正的变量

### 特征工程：共线性、将维、扩展

`共线性`

* 特征间共线性：两个或多个特征包含了相似的信息，相互之间存在强烈的相关关系。
* 常用的判断标准：两个或两个以上的特征之间的相关系数高于0.8
* 共线性的影响：降低运算效率；降低一些模型的稳定性；弱化一些模型的预测能力
* 处理办法：
    * 删除：一组相互共线的特征中只保留与因变量相关性最高的一个
    * 变换：对共线的两列特征进行求比值、求差值等计算

### 特征工程

`数据降维和特征提取`  
目的：降低不相关特征对模型准确性的干扰，降低模型的复杂度，提高模型的泛化能力，减少模型特征，提高模型训练和预测数据  
处理办法：

* 基于数据的理解，直接删除
* 使用主成分分析法(PCA)对特征进行提取
* 使用机器学习模型对特征进行筛选

`特征扩展`  
目的：解决模型欠拟合，捕捉自变量和因变量之间的非线性关系  
常用方法：多项式扩展。举例如下：

* 假设数据集中包含自变量a、b
* 如果对自变量做多项式二次扩展
* 自变量集从两个变量扩展为5个变量(a、b、a_a、b_b、a*b)

## 数据挖掘建模

### 数据集的划分方法

训练集：用来训练和拟合模型  
验证集：当通过训练集训练出多个模型后，使用验证集数据纠偏或比较预测  
测试集：模型泛化能力的考量。  
注：有时候数据集只划分两类，将验证集和测试机视为同一个数据集。

`数据集划分的基本原则`

* 保持训练集和验证集之间的互斥性
* 测试样本尽量不在训练样本中出现，以保证验证集上的表现能代表模型的泛化能力

`数据集划分方法`

* `留出法`：直接将数据集划分成两个互斥的集合，一个做训练集，一个做验证集。常用的划分比例：7:3、7.5:2.5、8:2。若划分三类：6:2:2

```
//python实现，供参考，具体见官网  
from sklearn.model_selection import train_test_split
```

* `交叉验证法`：将数据集划分成k个大小相似的互斥子集，每次把k-1个子集做训练，1个子集做验证，训练k次，最终返回k在、次训练结果的均值。因此交叉验证法又被称为k次交叉法(k-fold)。


```
//python实现，供参考，具体见官网  
from sklearn.model_selection import KFold
```

### 传统机器学习算法

传统机器学习算法根据样本集有无标注、是否部分有标注分为三类：监督学习、无监督学习、半监督学习。

讲算法之前先说明几个概念：  
`集成学习`：是指组合多个模型、有效提高模型泛化能力的学习策略。  

| 基础概念 | 适用条件 |  
| – | – |  
| 弱可学习 | 多项式学习算法的效果不很明显 |  
| 强可学习 | 多项式学习算法的效果较为明显 |  

弱可学习可通过集成方法称为强可学习。  
集成方法分类：

* `袋装法`(bagging)：指将训练集分别用不同的模型进行训练，这些模型相互独立，然后将结果进行投票取均值的方法。如随机森林。
* `提升法`(boost)：指训练集用一种模型训练出的结果作为另一个模型的输入，然后将其输出再作为其他模型的输入，如此反复。最后把这些模型进行加权叠加作为最终输出。如Adaboost、XGBoost。
    * 注意这种方式中，子模型对最终结果的影响更大程度上取决于权值，而不是顺序。

#### 监督学习

适用于样本集有标注的情况。

监督学习模型分为`分类`和`回归`两类。

* 分类适用于标注(标签)是离散的情况
* 回归适用于标注是连续数值的情况
* 分类和回归没有绝对的划分界限，分类可视为一种有限的回归；回归可视为一种无限定序数据的分类。有些算法都可运用在这两类模型上。

监督学习使用场景举例：图形识别、房价预测、银行信用评估等

分类常用模型：KNN、朴素贝叶斯、Adaboost、随机森林  
回归常用模型：线性回归、回归树和提升树  
可同时用于分类和回归的模型：决策树、支持向量机(SVM)、Logistic模型、人工神经网络

##### 分类

具体的，分类模型也可以分为两种类型：生成模型、判别模型。

* `生成模型`：通过求输入输出的联合概率分布，在求解类别归类的概率。如朴素贝叶斯
* `判别模型`：不通过求联合概率分布，直接可以获得输出最大分类的概率。如KNN

两者的区别：判别模型较生成模型，对数据的要求低一点，对数据的容忍度大一些，速度相对慢一些，适用范围更广一些。

###### KNN

KNN：K-Nearest Neighbors，最邻近结点算法。  
算法思想：每个样本都可以用它最接近的K个邻近值来代表。  
适用条件：用于标注在空间隔离性较好的情况。

基础知识：

* 欧式距离
* 曼哈顿距离
* 闵可夫斯基距离
* KD-Tree：点作为叶子节点，线作为分枝节点

```
//python实现，供参考，具体见官网  
from sklearn.neighbors import KNeighborsClassifier  
knn_clf= KNeighborsClassifier(n_neighbors=5) //最近5个点  
knn_clf.fit(X_train,Y_train)  
Y_pred = knn_clf.predict(X_validation)
```

###### 朴素贝叶斯

朴素：特征间相互独立。  
算法思想：先通过已给定的训练集，以特征之间独立作为前提假设，学习从输入到输出的联合概率分布，再基于学习到的模型，输入 X 求出使得后验概率最大的输出 Y 。  
适用条件：特征最好是离散的。

基础知识：

* 概率
* 条件概率
* 联合概率
* 全概率公式
* 贝叶斯公式
* 拉普拉斯平滑：若条件概率为0，导致整个式子为0，则在所有值都加1.


```
//python实现，供参考，具体见官网  
from sklearn.naive_bayes import GaussianNB,BernoulliNB  
//高斯模贝叶斯，假设特征是高斯分布  
GaussianNB( ).fit( ).predict( )  
//伯努利贝叶斯，适用于离散值是二值的情况  
BernoulliNB( ).fit( ).predict( )
```

###### 决策树

算法思想：  
特征判别先后顺序的依据或评价手段：

* 信息增益-ID3：适用于离散值较多的分类问题
    * 值越大，该特征越先比较
* 信息增益率-C4.5：适用于离散值较多的分类问题
    * 考虑到了熵本身值大小的影响
* Gini系数-CART：不纯度。适用于连续值分类问题
    * 不纯度值最低的作当前区分

注意事项：

* 连续值切分方法同探索性分析的离散化方法
* 规则用尽则投票，哪个样本多投哪个
* 若过拟合，需要修建枝叶：
    * 前剪枝：构造决策树之前规定每个叶子节点最多有多少个样本，或规定决策树的最大深度
    * 后剪枝：先构造决策树，然后对样本值比较悬殊的枝叶进行修剪
* 若想生成图示，需下载app：Graphviz

```
//python实现，供参考，具体见官网  
from sklearn.tree import DecisionTreeClassifier  
//默认采用Gini系数(不纯度)  
DecisionTreeClassifier( ).fit( ).predict( )  
//使用信息增益(ID3)  
DecisionTreeClassifier(criterion="entropy").fit( ).predict( )
```

###### 支持向量机(SVM)

SVM: Support Vector Machine

基础概念：

* 高维面
* 分界面
* 拉格朗日乘数法

注意事项：

* 若一些计算结果为无穷大，可容忍部分错误的分类，转换求 min(max(L)) ;也可利用 KKT 条件，求 max(min(L))
* 如需扩维，有两种方式：
    * 线映射，在计算。这样容易造成维度灾难。
    * 先在低维空间计算，在利用核函数扩维
        * 常见核函数：线性核函数、多项式核函数、高斯径向基(RBF)核函数
* 若存在少部分异常，可松弛变量，即为了达到更宽的分界线，允许存在少量错分点
* 若样本不平衡，根据实际业务场景定
* 对于多分类问题：
    * One-Other：有几个分类建几个 SVM ，分成一个分类和其他分类
    * One-One：分类的两两之间分别建立 SVN

```
//python实现，供参考，具体见官网  
from sklearn.svm import SVC  
SVC(c=100000).fit( ).predict( ) //c为分类精度，值越大运行时间越长
```

###### Adaboost

Adaboost：集成方法中提升法的运用。

特点：精度高，灵活可调，几乎不用担心过拟合，简化特征工程流程


```
//python实现，供参考，具体见官网  
from sklearn.ensemble import AdaboostClassifier  
AdaboostClassifier().fit( ).predict( )
```


###### 随机森林

随机森林：集成方法中袋装法的运用。由多个决策树集成。

基本概念：

* 树的个数
    * 考虑到的样本的局部性的可能情况越多，越容易过拟合
    * 树的数量与样本数量、特征数量都有关系，不断的尝试后确定
* 树的特征数
    * 特征少时每棵树用全部特征，特征多时每棵树用部分特征
    * 可增加树的数量和并行计算的能力来平衡特征减少可能带来的损失
* 树的训练集
    * 每棵树的训练集都是模型训练集的一个子集
    * 选取子集的方法有两种：
        * 训练子集和模型训练集数量一样，采用有放回的抽样构成样本差异性
        * 每棵树都用全部样本，通过缩减特征的规模构成样本的差异性

注意事项：

* 每个决策树可以不使用全部特征，减少规模和复杂度
* 不需要剪枝，即可有效避免过拟合


```
//python实现，供参考，具体见官网  
from sklearn.ensemble import RandomForestClassifier  
RandomForestClassifier().fit( ).predict( )
```

##### 回归

###### 线性回归

`一元线性回归`  
适用条件：适用于线性可分的场景

基本概念：

* 损失函数
* 参数优化目标
* 最小二乘法

```
//python实现，供参考，具体见官网  
from sklearn.linear_model import LinearRegression  
//线性回归  
LinearRegression().fit( ).predict( )
```

`多元线性回归`

基本概念：

* 损失函数
* 优化目标
* 矩阵求解
* 惩罚(正则化):通常在模型损失函数中增加一个正则项(惩罚项)来控制模型的复杂度。有两类惩罚项：
    * L1正则系数：Lasso回归
    * L2正则系数：ridge回归(岭回归)

求解方法：`梯度下降法`  
一种无约束多元函数极值求解方法，通过迭代得到最小化的损失函数所对应的模型参数。  
基本思路：在求解目标函数 E(a) 的最小值时，a 沿梯度下降的方向不断变化求解最小值。


```
//python实现，供参考，具体见官网  
from sklearn.linear_model import Ridge,Lasso  
//岭回归  
Ridge(alpha = 5).fit( ).predict( ) //alpha默认为0  
//Lasso回归  
Lasso(alpha = ).fit( ).predict( )
```


###### Logistic回归

基本概念：

* 激活函数
* 损失函数：对数似然损失函数
* 梯度下降

注意事项：同线性回归，也是求最小值，也可用梯度下降方法求解


```
//python实现，供参考，具体见官网  
from sklearn.linear_model import LogisticRegression  
LogisticRegression( ).fit( ).predict( ) //alpha默认为0
```


###### 人工神经网络

适用条件：适用于各种非线性映射。

基本概念：

* 感知器：处理线性映射关系
* 感知器并联
* 神经网络：
    * 输入层：数据必须归一化；
    * 隐含层；
    * 输出层：必须是 one-hot 格式

求解方法：求解所有参数

* 梯度下降算法：参数多，很复杂
* 反向传播算法(PyBrain)
    * 前向计算
    * 计算误差
    * 反向单层调整
    * 传播
    * 不断迭代，直到输出收敛到误差范围内或迭代固定次数
* 随机梯度下降算法(SGD,stochastic Gradient Decent)
    * 每次调整权值时，选取部分样本进行梯度下降
    * 优点是收敛更快，计算开销小；缺点是容易陷入局部最优解。
    * 使用范围广

注意事项：

* 人工神经网络的深度加深就形成深度神经网络。
* 算法易受离群点影响，易过拟合。解决办法有两种：
    * 正则化
    * dropout：每次随机选取部分节点，组成多个神经网络模型，将多个结果投票选出得票最多的模型取其值；对于回归模型取其均值。类似与集成方法。
* 属性特征和结果要在0-1之间，且结果是 one-hot 形式
* 输出结果进行 softmax 转化，确保其和为1


```
//python中没有神经网络的包，需要手写。  
//步骤一：安装keras  
pip install tensorflow  
conda install pip //仅window需要这一步，且已安装 Anconoda  
pip install keras  
//步骤二：python中调用keras  
from keras.models import Sequential // 类似容器  
from keras.models import Dense,Activation //神经网络层，激活函数  
from keras.models import SGD //随机梯度下降算法
```

###### 回归树和提升树

回归树：

* 与分类树(决策树)的区别：
    * 分类树中只需叶子结点有分类的判断值
    * 回归树中每个节点都有一个预测值，一般来说预测值是连续标注的平均值
* 回归树的切分方法：
    * 切分后两部分的方差和最小。其中一个特征可以使用多次，直到满足回归树的停止条件。
* 回归树的停止条件有两种：
    * 剪枝的限制：
        * 树的最大深度
        * 叶子的最大样本数量
        * …
    * 最小方差值
* 回归树最终取叶子节点的平均值作为预测值。

提升树：由多棵回归树集成。其中最佳的一种提升树是 `GBDT` (Gradient boosting Decision)梯度提升决策树

```
//python实现，供参考，具体见官网  
from sklearn.ensemble import GradientBoostingClassifier  
//最佳提升树：梯度提升决策树  
GradientBoostingClassifier(max_depth = 6,n_estimators = 100 ).fit( ).predict( ) //100棵树，每棵树深度为6
```

#### 非监督学习

将集合分成有类似的对象组成的多个类的过程，适用于样本集无标注的情况。

非监督学习模型分为`聚类`和`关联`。

* 分类适用于标注(标签)是离散的情况
* 回归适用于标注是连续数值的情况
* 分类和回归没有绝对的划分界限，分类可视为一种有限的回归；回归可视为一种无限定序数据的分类。有些算法都可运用在这两类模型上。

非监督学习使用场景举例：App客群分类、词向量转化等

聚类常用模型：基于切割的k-means、基于层次的聚类、基于密度的DBSCAN、基于图的split  
关联常用模型：Apriori、Apriori-All

##### 聚类

###### k-means

算法思想：所有类都有一个中心，属于一个类的点到它的中心的距离相比于其他类的中心更近。中心是指质心，距离常用欧式距离。

实现步骤：

* 从n个样本中随机选取k个作为初始化的质心
* 对每个样本测量其到每个质心的距离，并把它归到最近的质心的类
* 重新计算已经得到的各个类的质心
* 迭代第二、三步，直至新的质心与原质心相等或小于阈值，算法结束

注意事项：

* 初始质心位置可能回影响最终结果
    * 多试几次，取最稳定的结果
* 个别离群值会影响整体聚类效果
    * 将取质心换成取中心(k-medoids)。k-medoids 中点与其他同类点的距离和最小
* 必须指定k值
    * 其他衡量因子辅助，如轮廓系数、最小误差…

```
//python实现，供参考，具体见官网  
from sklearn.cluster import KMeans  
KMeans(n_clusters = 2).fit(X) //分成两类  
KMeans.labels_.astype(np.int) //将聚类后得到的labels转换成int格式  
plt.scatter( ) //画散点图
```

###### 层次聚类

算法思想：相近的点尽可能接近。把相近的点视为一个簇，根据分类个数不断迭代。

距离衡量指标：

* ward距离
* 平方残差和：值越小，两个簇越可以合成一个簇

注意事项：

* 层次聚类灵活，但是计算复杂度比较高，离群点影响比较大


```
//python实现，供参考，具体见官网  
from sklearn.cluster import AgglomerativeClustering  
AgglomerativeClustering(n_clusters = 2,linkage="ward") //分成两类
```


###### DBSCAN

算法思想：一定区域内，密度达到一定程度才是一个类，否则是离群点。

基本概念：

* E邻域：给定对象半径为 E 内的区域称为该对象的 E邻域
* 核心对象：如果给定对象 E邻域内的样本点大于或等于 MinPts，则称该对象为核心对象
* 直接密度可达：对于样本集合 D，如果样本点 q在 p的E邻域内，并且 p为核心对象，那么对象 q从对象 p直接密度可达。
* 密度可达：对于样本集合 D，给定一串样本点 p1、p2、…、pn, p = p1, q = pn,假设对象 pi从 pi-1直接密度可达，那么对象 q从对象 p密度可达
* 密度相连：存在样本集合 D中的一点 o，如果对象 o到对象 p和对象 q都是密度可达的，那么 p和 q密度相连

注意事项：

* DBSCAN算法就是找到密度相连对象的最大集合
* DBSCAN算法优点：对离群点不敏感
* 缺点：
    * 计算相邻两个点之间的点不容易
        * 借助 KD-Tree等数据结构的辅助
    * 需要指定两个参数：E 、 MinPts
        * 多尝试

```
//python实现，供参考，具体见官网  
from sklearn.cluster import DBSCAN  
DBSCAN(min_samples = 3,eps= 5) //min_samples：最小点数；eps:E邻域
```


###### 图分裂

实现步骤：

* 根据坐标点位置距离关系形成连通图(可采用DBSCAN等算法思路找到最大范围的点数，然后用边连接起来)
* 将形成的多个连通图进行逐一分裂

分类的依据：

* 承受系数t
* 分裂阈值$\\lambda$
* 若 t > $\\lambda$,则将该组边切分

注意事项：

* 与基于层次的聚类思路相反，是从顶至下
* 图建立方式、分裂方式非常灵活

##### 关联

基本概念：

* 项目：一个字段，对交易来说一般是指一次交易中的一个物品，如：尿布
* 事物：某个客户在一次交易中，发生的所有项目的集合，如：{尿布，啤酒}
* 项集：包含若干个项目的集合(一次事务中)
* 频繁项集：某个项集的支持度大于设定阈值(人为设定或者根据数据分布出经验来定)，即称这个项集为频繁项集
* 支持度：项集{x, y}在总项集中出现的频率(support)
* 置信度：在先决条件 x发生的条件下，有关联规则{x -> y}推出 y 的概率(Confidence)
* 提升度：表示含有 x的条件下同时含有 y的概率，与无论含不含 x都含有 y的概率之比。(Confidence({x} -> {y}) / support({y}))。若提升度大于1，则表示购买 x对购买 y有提升作用。

注意事项：

* 两个频繁项集组成的项集不一定是频繁项集
* 两个非频繁项集组成的项集一定不是频繁项集
* 一个频繁项集和一个非频繁项集组成的项集一定不是频繁项集

关联可分为两类：

* 关联规则：反映一个事物与其他事物之间的相互依存性和关联性，如Apriori算法
* 序列规则：与关联规则相似。不同的是将时间因素考虑进来，剔除关联规则中时间点靠后的项对时间点靠前的项的支持，如Apriori-All算法

###### Apriori

算法思想：先指定支持度的阈值，若一个项集的支持度大于这个阈值，则称其为频繁集，然后找出频繁项集。

找出频繁集的方法：

* 先找出一频繁项集，去掉一非频繁项集
* 然后将一频繁项集组成二频繁项集，根据阈值去掉二非频繁项集
* 再将一频繁项集和二频繁项集组成三频繁项集，根据阈值去掉三非频繁项集
* 以此类推
* 直至找出最高阶的频繁项集，所有组合遍历完毕，整理全部项集

###### Apriori-All

适用场景：预测用户在购买某种东西后，下次购买时还会买其他什么东西作为搭配

实现步骤：

* Forward: Apriori
* Backward: 去掉时间序列之后的项对之前的项的支持

注意事项：

* sklearn中不支持序列规则，自己写

#### 半监督学习

适用于样本集部分有标注，部分无标注的情况。通常无标注样本数量远大于有标注样本的数量。

原因：获取标注的成本较大；无标注样本可能很容易获得。

算法思路：

* 生成模型思路：先对所有有标注的样本计算出一个分布，然后判别无标注的样本如何标注。也可采用分批迭代的方式，如先将与有标注样本比较近的样本进行标注，然后调整分布，在标注接下来的样本。
* 判别模型的思路。也就是指物以类聚，如标签传播算法。

常用算法：标签传播算法

##### 标签传播算法

算法思想：根据没有标注的样本和周围有标注的样本进行相似度比较，相似度高的将其标注为临近的标注。  
其中传播是指迭代由近及远的过程。相似度判别方法：KNN、RBF等

```
//python实现，参数自行查看sklearn官网  
from sklearn.semi_supervised import LabelPropagation  
LabelPropagation( ).fit( ).predict( )
```


#### 深度学习

暂略

* * *

## 模型评估方法

python中用法：

```
from sklearn.metrics import accuracy_score,recall_score,f1_score  
//准确率  
print("ACC",accuracy_score(Y_validation,Ypred))  
//召回率  
print("REC",recall_score(Y_validation,Ypred))  
//F值  
print("F_score",f1_score(Y_validation,Ypred))
```

### 分类模型的常用评价指标

#### 二分类评价指标

分正类(1)和负类(0)  
`基本指标`

* 误差率：错分类样本占总体样本的比例
* 准确率(正确率)(Accuracy Rate)：正确分类样本占总体样本的比例

`混淆矩阵`  
| 真实情况 | 预测为正例 | 预测为负例  
| —- | —- | —- |  
| 正例 | TP(真正例) | FN(假反例) 漏  
| 负例 | FP(假正例) 错 | TN(真反例)

`衍生指标`

* 查准率(precision):所有真正例占所有预测为正的样本的比例
* 查全率(招回率,Recall,TPR):所有真正例占所有真实为正的样本的比例
* F-measure(F-score):

```
2 * Recall * Accuracy / (Reacll + Accuracy)
```


* 错误接收率(FPR,False Postive Rate):

```
FP / (FP + TN)
```

* 错误拒绝率(FRR,False Rejction Rate):

```
FN / (TP + FN)
```

#### 多分类评价指标

* 多元混淆矩阵
* 准确率：同二分类
* 召回率与F值：两种思路处理：
    * 先计算所有的TP、FN等值，再以二值方法计算
    * 分别把每个分类当作正类，各计算一个召回率或F值，然后取加权或不加权的平均值
* ROC曲线与AUC值：衡量分类效果，并且可以限定阈值
    * ROC曲线：以召回率(TPR)为纵轴，错误接收率(FPR)为横轴，采用不同的截断点，绘制ROC曲线。ROC曲线能够很容易地查出任意界限值对性能的识别能力。
    * AUC值：ROC曲线与坐标轴构成的图形面积。AUC值越接近1，说明越准确。
* 增益图和KS图：衡量分类效果
    * 增益图：宏观上反映分类器的效果
    * KS图：反映对正类样本份额例的

### 回归模型的常用评价指标

* 样本误差：衡量模型在一个样本上的预测准确度
    * 样本误差 = 样本预测值 - 样本实际值
* 平均误差方(MSE)：最常用的评价指标
    * 所有样本的样本误差的平方和的均值。MSE越接近0，模型越准确。
* 平均绝对误差(MAE)：较好解释的评价指标
    * 所有样本的样本误差的绝对值的均值。MAE的单位与因变量的单位一致，其越接近0，模型越准确。
* 平均绝对比例误差(MAPE)：平均绝对误差的衍生指标
    * 所有样本的样本误差的绝对值占实际值的比例。指标越接近0，模型越准确。
* 决定系数： R2-score
    * 因变量的方差能被自变量解释的程度。指标越接近1，则代表自变量对于因变量的解释程度越高。通常 >0.5 ，就还不错。

### 聚类模型的常用评价指标

* RMS(Root Mean Square)：值越小，分类效果越好  
    $$ RMS = \\frac{1}{n} \\sqrt{\\sum_{i=0}^{n} (x_i - \\bar{x})^{2}}$$
* 轮廓系数
    * a(i)为样本i与簇内其他样本的平均距离，也称为内聚度
    * b(i)为样本i与其他某簇样本的平均距离，也称为分离度

```
//s(i)越接近1，分类效果越好；越接近-1，分类效果最差  
s(i) = (b(i) - a(i)) / max{a(i),b(i)}
```


## 关联模型的常用评价指标

* 支持度：项集{x, y}在总项集中出现的频率(support)
* 置信度：在先决条件 x发生的条件下，有关联规则{x -> y}推出 y 的概率(Confidence)
* 提升度：表示含有 x的条件下同时含有 y的概率，与无论含不含 x都含有 y的概率之比。(Confidence({x} -> {y}) / support({y}))。若提升度大于1，则表示购买 x对购买 y有提升作用。

## 其他

python中保存和加载模型的方式：

```
from sklearn.externals import joblib  
//保存模型  
joblib.dump(knn_clf,"knn_clf")  
//加载模型  
joblib.load(knn_clf,"knn_clf")
```

## 后续

> 注：本文持续更新中