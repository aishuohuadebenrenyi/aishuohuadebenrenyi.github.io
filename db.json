{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-landscape/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/fancybox/jquery.fancybox.min.css","path":"fancybox/jquery.fancybox.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/fancybox/jquery.fancybox.min.js","path":"fancybox/jquery.fancybox.min.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/js/jquery-3.4.1.min.js","path":"js/jquery-3.4.1.min.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.woff2","path":"css/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":0,"renderable":1},{"_id":"themes/keep/source/css/font-awesome.min.css","path":"css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/keep/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/keep/source/webfonts/fa-brands-400.svg","path":"webfonts/fa-brands-400.svg","modified":0,"renderable":1},{"_id":"themes/keep/source/webfonts/fa-brands-400.ttf","path":"webfonts/fa-brands-400.ttf","modified":0,"renderable":1},{"_id":"themes/keep/source/webfonts/fa-brands-400.woff","path":"webfonts/fa-brands-400.woff","modified":0,"renderable":1},{"_id":"themes/keep/source/webfonts/fa-brands-400.eot","path":"webfonts/fa-brands-400.eot","modified":0,"renderable":1},{"_id":"themes/keep/source/webfonts/fa-brands-400.woff2","path":"webfonts/fa-brands-400.woff2","modified":0,"renderable":1},{"_id":"themes/keep/source/webfonts/fa-regular-400.eot","path":"webfonts/fa-regular-400.eot","modified":0,"renderable":1},{"_id":"themes/keep/source/webfonts/fa-regular-400.svg","path":"webfonts/fa-regular-400.svg","modified":0,"renderable":1},{"_id":"themes/keep/source/webfonts/fa-regular-400.ttf","path":"webfonts/fa-regular-400.ttf","modified":0,"renderable":1},{"_id":"themes/keep/source/webfonts/fa-regular-400.woff","path":"webfonts/fa-regular-400.woff","modified":0,"renderable":1},{"_id":"themes/keep/source/webfonts/fa-regular-400.woff2","path":"webfonts/fa-regular-400.woff2","modified":0,"renderable":1},{"_id":"themes/keep/source/webfonts/fa-solid-900.eot","path":"webfonts/fa-solid-900.eot","modified":0,"renderable":1},{"_id":"themes/keep/source/webfonts/fa-solid-900.svg","path":"webfonts/fa-solid-900.svg","modified":0,"renderable":1},{"_id":"themes/keep/source/webfonts/fa-solid-900.woff","path":"webfonts/fa-solid-900.woff","modified":0,"renderable":1},{"_id":"themes/keep/source/webfonts/fa-solid-900.woff2","path":"webfonts/fa-solid-900.woff2","modified":0,"renderable":1},{"_id":"themes/keep/source/webfonts/fa-solid-900.ttf","path":"webfonts/fa-solid-900.ttf","modified":0,"renderable":1},{"_id":"themes/keep/source/js/back2top.js","path":"js/back2top.js","modified":0,"renderable":1},{"_id":"themes/keep/source/js/code-copy.js","path":"js/code-copy.js","modified":0,"renderable":1},{"_id":"themes/keep/source/js/dark-light-toggle.js","path":"js/dark-light-toggle.js","modified":0,"renderable":1},{"_id":"themes/keep/source/js/header-shrink.js","path":"js/header-shrink.js","modified":0,"renderable":1},{"_id":"themes/keep/source/js/left-side-toggle.js","path":"js/left-side-toggle.js","modified":0,"renderable":1},{"_id":"themes/keep/source/js/local-search.js","path":"js/local-search.js","modified":0,"renderable":1},{"_id":"themes/keep/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/keep/source/js/toc.js","path":"js/toc.js","modified":0,"renderable":1},{"_id":"themes/keep/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/keep/source/images/bg.svg","path":"images/bg.svg","modified":0,"renderable":1},{"_id":"themes/keep/source/images/avatar.png","path":"images/avatar.png","modified":0,"renderable":1},{"_id":"themes/keep/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/keep/source/css/layout/category-content.styl","path":"css/layout/category-content.styl","modified":0,"renderable":1},{"_id":"themes/keep/source/css/layout/article-content.styl","path":"css/layout/article-content.styl","modified":0,"renderable":1},{"_id":"themes/keep/source/css/layout/category-list.styl","path":"css/layout/category-list.styl","modified":0,"renderable":1},{"_id":"themes/keep/source/css/layout/archive-content.styl","path":"css/layout/archive-content.styl","modified":0,"renderable":1},{"_id":"themes/keep/source/css/layout/home-content.styl","path":"css/layout/home-content.styl","modified":0,"renderable":1},{"_id":"themes/keep/source/css/layout/page.styl","path":"css/layout/page.styl","modified":0,"renderable":1},{"_id":"themes/keep/source/css/layout/tag-content.styl","path":"css/layout/tag-content.styl","modified":0,"renderable":1},{"_id":"themes/keep/source/js/libs/anime.min.js","path":"js/libs/anime.min.js","modified":0,"renderable":1},{"_id":"themes/keep/source/css/layout/common/animated.styl","path":"css/layout/common/animated.styl","modified":0,"renderable":1},{"_id":"themes/keep/source/css/layout/common/basic.styl","path":"css/layout/common/basic.styl","modified":0,"renderable":1},{"_id":"themes/keep/source/css/layout/common/keep-theme.styl","path":"css/layout/common/keep-theme.styl","modified":0,"renderable":1},{"_id":"themes/keep/source/css/layout/common/markdown.styl","path":"css/layout/common/markdown.styl","modified":0,"renderable":1},{"_id":"themes/keep/source/css/layout/common/variables.styl","path":"css/layout/common/variables.styl","modified":0,"renderable":1},{"_id":"themes/keep/source/css/layout/common/codeblock/copy-code.styl","path":"css/layout/common/codeblock/copy-code.styl","modified":0,"renderable":1},{"_id":"themes/keep/source/css/layout/common/codeblock/highlight.styl","path":"css/layout/common/codeblock/highlight.styl","modified":0,"renderable":1},{"_id":"themes/keep/source/css/layout/common/codeblock/code-theme.styl","path":"css/layout/common/codeblock/code-theme.styl","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/hello-world.md","hash":"674ce785a9369474a0f5505b27ea2b75e1cd4516","modified":1609235783847},{"_id":"node_modules/hexo-theme-landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/README.md","hash":"d2772ece6d4422ccdaa0359c3e07588834044052","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/_config.yml","hash":"b608c1f1322760dce9805285a602a95832730a2e","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/package.json","hash":"a4f837cd7d9b88e69afa4206db59e6d34fa3607f","modified":1609226063160},{"_id":"node_modules/hexo-theme-landscape/languages/de.yml","hash":"3ebf0775abbee928c8d7bda943c191d166ded0d3","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/languages/en.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/languages/es.yml","hash":"76edb1171b86532ef12cfd15f5f2c1ac3949f061","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/languages/fr.yml","hash":"415e1c580ced8e4ce20b3b0aeedc3610341c76fb","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/languages/hu.yml","hash":"284d557130bf54a74e7dcef9d42096130e4d9550","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/languages/it.yml","hash":"89b7d91306b2c1a0f3ac023b657bf974f798a1e8","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/languages/ko.yml","hash":"881d6a0a101706e0452af81c580218e0bfddd9cf","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/languages/mn.yml","hash":"2e7523951072a9403ead3840ad823edd1084c116","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/languages/pt.yml","hash":"57d07b75d434fbfc33b0ddb543021cb5f53318a8","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/languages/ja.yml","hash":"a73e1b9c80fd6e930e2628b393bfe3fb716a21a9","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/languages/tr.yml","hash":"a1cdbfa17682d7a971de8ab8588bf57c74224b5b","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/languages/zh-CN.yml","hash":"1efd95774f401c80193eac6ee3f1794bfe93dc5a","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/layout.ejs","hash":"0d1765036e4874500e68256fedb7470e96eeb6ee","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/scripts/fancybox.js","hash":"c857d7a5e4a5d71c743a009c5932bf84229db428","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/recent_posts.ejs","hash":"60c4b012dcc656438ff59997e60367e5a21ab746","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/after-footer.ejs","hash":"414914ebb159fac1922b056b905e570ac7521925","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/archive.ejs","hash":"7cb70a7a54f8c7ae49b10d1f37c0a9b74eab8826","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/article.ejs","hash":"dfd555c00e85ffc4207c88968d12b219c1f086ec","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/footer.ejs","hash":"3656eb692254346671abc03cb3ba1459829e0dce","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/gauges-analytics.ejs","hash":"21a1e2a3907d1a3dad1cd0ab855fe6735f233c74","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/google-analytics.ejs","hash":"2ea7442ea1e1a8ab4e41e26c563f58413b59a3d0","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/head.ejs","hash":"f215d92a882247a7cc5ea80b241bedfcec0ea6ca","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/header.ejs","hash":"c1acd247e14588cdf101a69460cb8319c18cd078","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/_variables.styl","hash":"581b0cbefdaa5f894922133989dd2d3bf71ded79","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/style.styl","hash":"9c451e5efd72c5bb8b56e8c2b94be731e99db05b","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/fancybox/jquery.fancybox.min.css","hash":"1be9b79be02a1cfc5d96c4a5e0feb8f472babd95","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/js/script.js","hash":"998ed4c5b147e1299bf62beebf33514474f28112","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/date.ejs","hash":"f1458584b679545830b75bef2526e2f3eb931045","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/title.ejs","hash":"4d7e62574ddf46de9b41605fe3140d77b5ddb26d","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/article.styl","hash":"80759482d07063c091e940f964a1cf6693d3d406","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/header.styl","hash":"85ab11e082f4dd86dde72bed653d57ec5381f30c","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/highlight.styl","hash":"bf4e7be1968dad495b04e83c95eac14c4d0ad7c0","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/fancybox/jquery.fancybox.min.js","hash":"6181412e73966696d08e1e5b1243a572d0f22ba6","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/js/jquery-3.4.1.min.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":499162500000},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":499162500000},{"_id":"public/2020/12/29/hello-world/index.html","hash":"e4f069854d9b4ae7291682b106013ec7b4dba689","modified":1650725202430},{"_id":"public/index.html","hash":"7477912b8b1fb1a020a30f110e802dc19da5cd51","modified":1650725202430},{"_id":"public/archives/index.html","hash":"673d70bdd75f0018b72c52f6a07b71c4fb5948f3","modified":1650725202430},{"_id":"public/archives/2020/index.html","hash":"cc5cd3e062a9904ab67f2317207c19a446a579b7","modified":1650725202430},{"_id":"public/archives/2020/12/index.html","hash":"2953bec7d8dd441d1978c8528b14fb51a367a230","modified":1650725202430},{"_id":"public/css/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1609226241955},{"_id":"public/css/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1609226241955},{"_id":"public/fancybox/jquery.fancybox.min.css","hash":"1be9b79be02a1cfc5d96c4a5e0feb8f472babd95","modified":1609226241955},{"_id":"public/css/style.css","hash":"02b663f8fe1d482b509b57833779d85b64754076","modified":1609231969337},{"_id":"public/css/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1609226241955},{"_id":"public/js/script.js","hash":"998ed4c5b147e1299bf62beebf33514474f28112","modified":1609226241955},{"_id":"public/css/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1609226241955},{"_id":"public/css/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1609226241955},{"_id":"public/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1609226241955},{"_id":"public/fancybox/jquery.fancybox.min.js","hash":"6181412e73966696d08e1e5b1243a572d0f22ba6","modified":1609226241955},{"_id":"public/js/jquery-3.4.1.min.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1609226241955},{"_id":"public/css/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1609226241955},{"_id":"source/_posts/temporary.md","hash":"03cfc2ea660c800bedb7fd26b090c2a8be097b61","modified":1609235707884},{"_id":"public/2020/12/29/temporary/index.html","hash":"4eef1c33aea59a8a7321243da45cb3840764c1d9","modified":1609235719594},{"_id":"public/tags/hahahah/index.html","hash":"8a14a2e09bc2cbeac52c180739294a94fc98d412","modified":1609233678754},{"_id":"themes/keep/LICENSE","hash":"98b8bd28e54ab36ee12396687dfdf88807c6cfdd","modified":1609231168210},{"_id":"themes/keep/.editorconfig","hash":"a1c91f0a086bf92fddb02ccf23578ec2b51c099c","modified":1609231168210},{"_id":"themes/keep/.gitignore","hash":"b49daa26b8121cc0c7074d61b377c8e7e0880d03","modified":1609231168210},{"_id":"themes/keep/package.json","hash":"5af9cce25165612ce934d75bc25b6a563538200b","modified":1609231168222},{"_id":"themes/keep/docs/README_zh-CN.md","hash":"af95ee24abfa98b94b47cba45e0c3177332d8a25","modified":1609231168211},{"_id":"themes/keep/README.md","hash":"b3ad6290f910e67294bd05cc84268b408a8ff28d","modified":1609231168210},{"_id":"themes/keep/layout/archive-content.ejs","hash":"cfa8f29b8863534f407136ea3deb18a4b2ff722c","modified":1609231168219},{"_id":"themes/keep/layout/archive.ejs","hash":"8456f112fc12bbb1c83cd190d0ce83ee474bd297","modified":1609231168219},{"_id":"themes/keep/layout/article-content.ejs","hash":"ed349d23f2ae7c4ec448a33e43656b9cc3bdf637","modified":1609231168219},{"_id":"themes/keep/layout/category-content.ejs","hash":"f11fc5c372957f6efc6187d49a8020d098420647","modified":1609231168219},{"_id":"themes/keep/_config.yml","hash":"05f38cc04bf7c09cb1ccab8e94ae6ca89117add0","modified":1609235371978},{"_id":"themes/keep/layout/category.ejs","hash":"8456f112fc12bbb1c83cd190d0ce83ee474bd297","modified":1609231168220},{"_id":"themes/keep/layout/home-content.ejs","hash":"79e16c3baa6569afbc07ac6a5d2e3f37b273016a","modified":1609231168220},{"_id":"themes/keep/layout/category-list.ejs","hash":"a9390b25238332417fd554d32563a4d9999db90f","modified":1609231168219},{"_id":"themes/keep/layout/page.ejs","hash":"ccf9ae6fb89d73aa0b8c2d2e9a8fd6289e87da64","modified":1609231168221},{"_id":"themes/keep/layout/tag-content.ejs","hash":"9409aa1d954bdb76979bb839d3d185ec8df828be","modified":1609231168221},{"_id":"themes/keep/layout/tags.ejs","hash":"8456f112fc12bbb1c83cd190d0ce83ee474bd297","modified":1609231168221},{"_id":"themes/keep/layout/tag.ejs","hash":"8456f112fc12bbb1c83cd190d0ce83ee474bd297","modified":1609231168221},{"_id":"themes/keep/layout/index.ejs","hash":"8456f112fc12bbb1c83cd190d0ce83ee474bd297","modified":1609231168220},{"_id":"themes/keep/layout/layout.ejs","hash":"0423aa5c0af2fd6fb80665840359b69be678b5d0","modified":1609231168221},{"_id":"themes/keep/languages/en.yml","hash":"70b5052c0e12955cc54dc417d0a37ce2764bee1d","modified":1609231168212},{"_id":"themes/keep/scripts/use-source-data.js","hash":"21f6a49e47b7e4d792d67503079ea5b509f18311","modified":1609231168223},{"_id":"themes/keep/languages/zh-CN.yml","hash":"bf67013be73d38d3977d6e7197d3c5af5a2b520d","modified":1609231168212},{"_id":"themes/keep/layout/_partial/archive-list.ejs","hash":"c8382072149b5472f9a829c0cf89c13716114ec1","modified":1609231168212},{"_id":"themes/keep/layout/_partial/article-copyright-info.ejs","hash":"77e13720374efb902ee3d4d04d13b62de033a3ae","modified":1609231168213},{"_id":"themes/keep/layout/_partial/article-meta-info.ejs","hash":"56878ad472a507ab8bfe1d2d0fb433b7a1f32a37","modified":1609231168213},{"_id":"themes/keep/layout/_partial/empty-page.ejs","hash":"a35667e92f2dcc06618f289b471bee98964a9654","modified":1609231168214},{"_id":"themes/keep/layout/_partial/first-screen.ejs","hash":"0f08b212e938655aae8891ecbd3113babc66e9e9","modified":1609231168214},{"_id":"themes/keep/layout/_partial/footer.ejs","hash":"c71b542de7b1e843ea44c822e4bd53b4dcf6b6ea","modified":1609231168215},{"_id":"themes/keep/layout/_partial/head.ejs","hash":"d1fbe7aa884c7eb9f4e92f4d25c19e016bb7a064","modified":1609231168216},{"_id":"themes/keep/layout/_partial/friends-link.ejs","hash":"b3dd2feb168c81332845051fe60e9a94cff7fd9a","modified":1609231168215},{"_id":"themes/keep/layout/_partial/header.ejs","hash":"dd5d2c543b5fd7a548855e396f85d3d0675e45e5","modified":1609231168216},{"_id":"themes/keep/layout/_partial/home-article-meta-info.ejs","hash":"9fc7c866091c96c8374149de6d2e83453f5f70be","modified":1609231168216},{"_id":"themes/keep/layout/_partial/image-viewer.ejs","hash":"a1e703bebe045555f01e29c9b58abd9233be5ea1","modified":1609231168217},{"_id":"themes/keep/layout/_partial/local-search.ejs","hash":"b870795770caf1fdf74d53903488011ace620dec","modified":1609231168217},{"_id":"themes/keep/layout/_partial/paginator.ejs","hash":"cfd4d9a30bd20ee0750b79226cb94562439013bf","modified":1609231168217},{"_id":"themes/keep/layout/_partial/scripts.ejs","hash":"bebdc48e1ed1d45218da7ba43a3ad5be395265fd","modified":1609231168217},{"_id":"themes/keep/layout/_partial/side-tools.ejs","hash":"baae2457ec055f07b941420fcd14af74cc789676","modified":1609231168218},{"_id":"themes/keep/layout/_partial/toc.ejs","hash":"aefc9db612c0fceb4a02fdbed9fa47a6c4e144b5","modified":1609231168218},{"_id":"themes/keep/layout/_partial/tagcloud.ejs","hash":"01814ef3f507edfcb6f464cd7d4dbbbdd0a4701f","modified":1609231168218},{"_id":"themes/keep/scripts/filters/link-handle.js","hash":"b39cfa42d156b7ad4d4e8246b6f934cd51d8db55","modified":1609231168222},{"_id":"themes/keep/layout/_partial/tools.ejs","hash":"312f0bf314c3e3dc00cec3295388b8c3b4c6bc7f","modified":1609231168218},{"_id":"themes/keep/scripts/helpers/helper.js","hash":"2efcf3c13bc0ff0cab7316dc992cd67778428c13","modified":1609231168223},{"_id":"themes/keep/scripts/helpers/export-config.js","hash":"4b7cb9eaddd2119ff06b1eb3fc6aad3ab38178a2","modified":1609231168223},{"_id":"themes/keep/source/css/font-awesome.min.css","hash":"c508528feb9fd540454f838653cd4863b290df2e","modified":1609231168224},{"_id":"themes/keep/source/css/style.styl","hash":"96549159d2fe8a031c614c11f0a4c3bab0f6963b","modified":1609231168233},{"_id":"themes/keep/source/webfonts/fa-regular-400.ttf","hash":"d64e58981a419de52bac110c979887d34e366135","modified":1609231168259},{"_id":"themes/keep/source/webfonts/fa-regular-400.woff","hash":"26afc29d39ab9fac6d0b607be7e76db093ff7c3a","modified":1609231168259},{"_id":"themes/keep/source/webfonts/fa-regular-400.woff2","hash":"fb9648469530a05fa9aac80e47d4d6960472a242","modified":1609231168260},{"_id":"themes/keep/source/webfonts/fa-regular-400.eot","hash":"2baa9a8aa68f1d2d4712b3c7205f9105aaf21879","modified":1609231168257},{"_id":"themes/keep/source/js/back2top.js","hash":"cd226f2e01a84c74e859dfd08ca1f5146011ae8b","modified":1609231168235},{"_id":"themes/keep/source/js/code-copy.js","hash":"68587d2b5af9c33a762d76ed7692089580a74d0f","modified":1609231168235},{"_id":"themes/keep/source/js/dark-light-toggle.js","hash":"4d58ef9566f1785286a5ee5bc21c2063b117fe1b","modified":1609231168235},{"_id":"themes/keep/source/js/left-side-toggle.js","hash":"d12ff633952d8ad469e659218617db3c9f199f9e","modified":1609231168236},{"_id":"themes/keep/source/js/toc.js","hash":"cdeba4d4b6c1a63b5541ee59cdf78efa166d71a4","modified":1609231168237},{"_id":"themes/keep/source/js/local-search.js","hash":"e161108a7d315816cd5602a42d1477f93902e0c5","modified":1609231168236},{"_id":"themes/keep/source/images/bg.svg","hash":"f3106cd334dc2ceef885c19012bf59a48498af42","modified":1609231168234},{"_id":"themes/keep/source/js/utils.js","hash":"a1cc18daac12aef8b4293f866a9021e9f41de1d4","modified":1609231168237},{"_id":"themes/keep/source/images/avatar.png","hash":"0e2e964a17d55f5d4f0b54b6889a0d1c5bf6d5e4","modified":1609231168234},{"_id":"themes/keep/layout/_partial/comment/comment.ejs","hash":"ea8560e3d8c226f3e14138237f444c4056c2152e","modified":1609231168213},{"_id":"themes/keep/source/images/logo.svg","hash":"117d4912553914c5e0463e7a4f205156518f4c8d","modified":1609231168234},{"_id":"themes/keep/layout/_partial/comment/gitalk.ejs","hash":"7bcfad63ab79db98f34488357bbe57bff8094248","modified":1609231168213},{"_id":"themes/keep/layout/_partial/comment/valine.ejs","hash":"bd3ac892bb8cc35037f8b89857f052135fdc998a","modified":1609231168214},{"_id":"themes/keep/source/css/layout/category-content.styl","hash":"fd9e4ce978d5289199ecf4144c278a8bf9db9af3","modified":1609231168230},{"_id":"themes/keep/source/css/layout/article-content.styl","hash":"bc19b8d7dd7f9b881c681e145e9f34c786dde064","modified":1609231168229},{"_id":"themes/keep/source/css/layout/archive-content.styl","hash":"8a62d9f588fc199662a6dcb863c9ba1de3b3034c","modified":1609231168229},{"_id":"themes/keep/source/css/layout/home-content.styl","hash":"81979657cc34f515bda1099d46beed8e2cd92dbc","modified":1609231168233},{"_id":"themes/keep/source/css/layout/page.styl","hash":"f3af5c58bfae5cc26a26cbd1ce3409a490b0d0e6","modified":1609231168233},{"_id":"themes/keep/source/css/layout/tag-content.styl","hash":"f2c9974839472b4b1782929e3a80c46d90d12103","modified":1609231168233},{"_id":"themes/keep/source/js/libs/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1609231168236},{"_id":"themes/keep/source/css/layout/_partial/archive-list.styl","hash":"ca92d5a9edb420aaf110dfe3356230ba9970dde4","modified":1609231168224},{"_id":"themes/keep/source/css/layout/_partial/article-copyright-info.styl","hash":"7992d4c844db8b9e6afac6a3b788acfe9c250788","modified":1609231168225},{"_id":"themes/keep/source/css/layout/_partial/empty-page.styl","hash":"59a0a0301a5599d4d0b0540cb2268d57d44dad0c","modified":1609231168226},{"_id":"themes/keep/source/css/layout/_partial/first-screen.styl","hash":"bc94d17b73b968dba5c4c08b4220f54fffd680fc","modified":1609231168226},{"_id":"themes/keep/source/css/layout/_partial/footer.styl","hash":"d38b7dfc38c1208df4f6d8cee9f58621882a360a","modified":1609231168227},{"_id":"themes/keep/source/css/layout/_partial/article-meta-info.styl","hash":"902b4bb3880c3366a548de1a85a480e9cc7b2ec2","modified":1609231168225},{"_id":"themes/keep/source/css/layout/_partial/header.styl","hash":"b3f83c31a59653b90d9c247c6ee40c82f20403c0","modified":1609231168227},{"_id":"themes/keep/source/css/layout/_partial/local-search.styl","hash":"fa63dc7a276eedaff50221204b184e29dd26b80f","modified":1609231168228},{"_id":"themes/keep/source/css/layout/_partial/image-viewer.styl","hash":"8fce59d64a2dab0f6dedf95bda446079835e66f8","modified":1609231168227},{"_id":"themes/keep/source/css/layout/_partial/paginator.styl","hash":"da5df38b09138af28cb8b3fe9a05aeda9dae5011","modified":1609231168228},{"_id":"themes/keep/source/css/layout/_partial/tagcloud.styl","hash":"2de571e07b3359fc5778b5cf4ac3b48d116e1c11","modified":1609231168228},{"_id":"themes/keep/source/css/layout/_partial/side-tools.styl","hash":"1deed1506f97785fb4b1f53e7d6ce36dfdae44f6","modified":1609231168228},{"_id":"themes/keep/source/css/layout/_partial/toc.styl","hash":"5e8c896d9eac2be3fec0324db319bff3cd1faabe","modified":1609231168229},{"_id":"themes/keep/source/css/layout/_partial/tools.styl","hash":"429c6ece19e950cf467efb743c8787714696282f","modified":1609231168229},{"_id":"themes/keep/source/css/layout/common/basic.styl","hash":"5bf08201169c95d0693b6fdc662ecc07940355a1","modified":1609231168231},{"_id":"themes/keep/source/css/layout/common/animated.styl","hash":"3070bf1bec068660de760821d0c98e09e97b36d2","modified":1609231168230},{"_id":"themes/keep/source/css/layout/common/keep-theme.styl","hash":"cb0d4c8d8fdb35a6c2b53b940ebc9e7ac75f1407","modified":1609231168232},{"_id":"themes/keep/source/css/layout/common/markdown.styl","hash":"17eed08f7663480ff3528b5aaf6ec2f90df00c72","modified":1609231168232},{"_id":"themes/keep/source/css/layout/_partial/comment/comment.styl","hash":"fd316d85c864d00c1af62d71aaa82d6d516d544e","modified":1609231168225},{"_id":"themes/keep/source/css/layout/common/variables.styl","hash":"50c149db2195d2babb740060f1cf48eccf1f8284","modified":1609231168232},{"_id":"themes/keep/source/css/layout/_partial/comment/valine.styl","hash":"4bd1a0db32fd0cba040c591c36d9d1220d4a825b","modified":1609231168226},{"_id":"themes/keep/source/css/layout/_partial/comment/gitalk.styl","hash":"b538919d22f63927799a9140b656001c0e15d180","modified":1609231168226},{"_id":"themes/keep/source/js/main.js","hash":"566fe3add92a6058775bb616449299d66ef345d3","modified":1609231168237},{"_id":"themes/keep/source/css/layout/common/codeblock/copy-code.styl","hash":"d7caeb1f583ab0b22db2b82c0a29a01006fa1516","modified":1609231168231},{"_id":"themes/keep/source/css/layout/common/codeblock/highlight.styl","hash":"9c5c2a5f3b2d9c0b97468e366e85b70531777ae7","modified":1609231168231},{"_id":"themes/keep/source/css/layout/common/codeblock/code-theme.styl","hash":"8c0551d53e073384caa8351fdbcbad0d3e744860","modified":1609231168231},{"_id":"themes/keep/source/webfonts/fa-brands-400.woff","hash":"a42ac4a94f4e0e9333cdb1b2fb6c13c0ae690ff2","modified":1609231168256},{"_id":"themes/keep/source/css/layout/category-list.styl","hash":"409edd107f4be72326c9cb778db232bf16eb093b","modified":1609231168230},{"_id":"themes/keep/source/webfonts/fa-solid-900.woff","hash":"3b86a0ce15d8d534b65e98560e321a933d310688","modified":1609231168273},{"_id":"themes/keep/source/webfonts/fa-solid-900.woff2","hash":"9b592048b9062b00f0b2dd782d70a95b7dc69b83","modified":1609231168279},{"_id":"themes/keep/source/webfonts/fa-brands-400.woff2","hash":"f726c4275bb494a045fde059175f072de06c01df","modified":1609231168256},{"_id":"themes/keep/source/webfonts/fa-brands-400.eot","hash":"c719f4375679ee58e156434f9ba9727be669cf07","modified":1609231168240},{"_id":"themes/keep/source/webfonts/fa-regular-400.svg","hash":"c41a688158d577e3a2738137992d49b05b17ab8f","modified":1609231168258},{"_id":"themes/keep/source/js/header-shrink.js","hash":"1cf440c53c202ffe120301766b90b3b2cca89c36","modified":1609231168236},{"_id":"themes/keep/source/webfonts/fa-solid-900.ttf","hash":"d537bf4cc7273ddb3fd754d53f2b241aeafaefe7","modified":1609231168273},{"_id":"themes/keep/source/webfonts/fa-solid-900.eot","hash":"c39b89f7fa7b66931ab02a54002db5ffcbff206c","modified":1609231168262},{"_id":"themes/keep/source/webfonts/fa-brands-400.ttf","hash":"9b49c6c5b0cbdef158ae31b2a4e4814555408389","modified":1609231168254},{"_id":"themes/keep/source/webfonts/fa-solid-900.svg","hash":"36de6a298660ed3feaab37900f7b28c3e981d80e","modified":1609231168270},{"_id":"themes/keep/source/webfonts/fa-brands-400.svg","hash":"69a0ff79a9d93a451e9fa90bd79ae7008d2a9da5","modified":1609231168251},{"_id":"public/webfonts/fa-regular-400.eot","hash":"2baa9a8aa68f1d2d4712b3c7205f9105aaf21879","modified":1609231969337},{"_id":"public/webfonts/fa-regular-400.woff","hash":"26afc29d39ab9fac6d0b607be7e76db093ff7c3a","modified":1609231969337},{"_id":"public/webfonts/fa-regular-400.ttf","hash":"d64e58981a419de52bac110c979887d34e366135","modified":1609231969337},{"_id":"public/webfonts/fa-regular-400.woff2","hash":"fb9648469530a05fa9aac80e47d4d6960472a242","modified":1609231969337},{"_id":"public/images/avatar.png","hash":"0e2e964a17d55f5d4f0b54b6889a0d1c5bf6d5e4","modified":1609231969337},{"_id":"public/images/logo.svg","hash":"117d4912553914c5e0463e7a4f205156518f4c8d","modified":1609231969337},{"_id":"public/images/bg.svg","hash":"f3106cd334dc2ceef885c19012bf59a48498af42","modified":1609231969337},{"_id":"public/webfonts/fa-brands-400.woff","hash":"a42ac4a94f4e0e9333cdb1b2fb6c13c0ae690ff2","modified":1609231969337},{"_id":"public/webfonts/fa-brands-400.woff2","hash":"f726c4275bb494a045fde059175f072de06c01df","modified":1609231969337},{"_id":"public/webfonts/fa-solid-900.woff","hash":"3b86a0ce15d8d534b65e98560e321a933d310688","modified":1609231969337},{"_id":"public/webfonts/fa-solid-900.woff2","hash":"9b592048b9062b00f0b2dd782d70a95b7dc69b83","modified":1609231969337},{"_id":"public/webfonts/fa-brands-400.ttf","hash":"9b49c6c5b0cbdef158ae31b2a4e4814555408389","modified":1609231969337},{"_id":"public/webfonts/fa-brands-400.eot","hash":"c719f4375679ee58e156434f9ba9727be669cf07","modified":1609231969337},{"_id":"public/webfonts/fa-regular-400.svg","hash":"c41a688158d577e3a2738137992d49b05b17ab8f","modified":1609231969337},{"_id":"public/js/code-copy.js","hash":"68587d2b5af9c33a762d76ed7692089580a74d0f","modified":1609231969337},{"_id":"public/js/back2top.js","hash":"cd226f2e01a84c74e859dfd08ca1f5146011ae8b","modified":1609231969337},{"_id":"public/js/header-shrink.js","hash":"1cf440c53c202ffe120301766b90b3b2cca89c36","modified":1609231969337},{"_id":"public/js/left-side-toggle.js","hash":"d12ff633952d8ad469e659218617db3c9f199f9e","modified":1609231969337},{"_id":"public/js/dark-light-toggle.js","hash":"4d58ef9566f1785286a5ee5bc21c2063b117fe1b","modified":1609231969337},{"_id":"public/js/main.js","hash":"566fe3add92a6058775bb616449299d66ef345d3","modified":1609231969337},{"_id":"public/js/toc.js","hash":"cdeba4d4b6c1a63b5541ee59cdf78efa166d71a4","modified":1609231969337},{"_id":"public/js/local-search.js","hash":"e161108a7d315816cd5602a42d1477f93902e0c5","modified":1609231969337},{"_id":"public/js/utils.js","hash":"a1cc18daac12aef8b4293f866a9021e9f41de1d4","modified":1609231969337},{"_id":"public/css/layout/category-content.css","hash":"bd620b11d390a1c6d010ce8dbe9643dc45246a27","modified":1609231969337},{"_id":"public/css/layout/category-list.css","hash":"0d23f6ebc9760a05b8c9e84a750545600090ced4","modified":1609231969337},{"_id":"public/css/layout/archive-content.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1609231969337},{"_id":"public/css/layout/article-content.css","hash":"70cda1e9d9b9205e447c98c24110f637d37c1049","modified":1609231969337},{"_id":"public/css/layout/home-content.css","hash":"4c015eaca142973c5efd8b4945605f03bb51f4b1","modified":1609231969337},{"_id":"public/css/layout/page.css","hash":"314259b0bfb97c323961149a17a4e47f4bf08041","modified":1609231969337},{"_id":"public/css/layout/common/animated.css","hash":"ffc3d829f365f82513e2d501fbe7f74d050e2efc","modified":1609231969337},{"_id":"public/css/layout/tag-content.css","hash":"3d1f21376cf0692a009ee20ca15b7e2886301a1c","modified":1609231969337},{"_id":"public/css/layout/common/basic.css","hash":"63f9c5a1b553abe1bb86d1b4e5f332c984dceac5","modified":1609231969337},{"_id":"public/css/layout/common/keep-theme.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1609231969337},{"_id":"public/css/layout/common/variables.css","hash":"989ec06965322348febce074a56392bf6c955e7e","modified":1609231969337},{"_id":"public/css/layout/common/codeblock/copy-code.css","hash":"c2040882967c341c815a8d11e02a2777560ee9ff","modified":1609231969337},{"_id":"public/css/layout/common/markdown.css","hash":"d5253332b48a74733f2ca0be41dad5a0abf49db3","modified":1609231969337},{"_id":"public/css/layout/common/codeblock/highlight.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1609231969337},{"_id":"public/css/layout/common/codeblock/code-theme.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1609231969337},{"_id":"public/webfonts/fa-solid-900.eot","hash":"c39b89f7fa7b66931ab02a54002db5ffcbff206c","modified":1609231969337},{"_id":"public/webfonts/fa-solid-900.ttf","hash":"d537bf4cc7273ddb3fd754d53f2b241aeafaefe7","modified":1609231969337},{"_id":"public/js/libs/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1609231969337},{"_id":"public/css/font-awesome.min.css","hash":"c508528feb9fd540454f838653cd4863b290df2e","modified":1609231969337},{"_id":"public/webfonts/fa-brands-400.svg","hash":"69a0ff79a9d93a451e9fa90bd79ae7008d2a9da5","modified":1609231969337},{"_id":"public/webfonts/fa-solid-900.svg","hash":"36de6a298660ed3feaab37900f7b28c3e981d80e","modified":1609231969337},{"_id":"source/tags/index.md","hash":"a450e058702e82891be1c5250e85c51cb63f6f19","modified":1609233260429},{"_id":"source/about/index.md","hash":"571410bd0916b048556bef93e04dae84c5aaff80","modified":1650725126914},{"_id":"source/categories/index.md","hash":"e3db1b2a12547e16f53bebc5bb399286c5ec5383","modified":1609233240820},{"_id":"public/search.xml","hash":"51810aceff45f957f2285a4c5107f1fcfa1a4674","modified":1650725202430},{"_id":"public/atom.xml","hash":"a3edfef63b79d16dd34381b1ea8207746c66c10f","modified":1650725202430},{"_id":"public/categories/index.html","hash":"802b608277599c4c6936c6496c22a9ed3fae5168","modified":1650725202430},{"_id":"public/tags/index.html","hash":"8c60274d0766a8ea6ed74e0dc48aa0e609650518","modified":1650725202430},{"_id":"public/about/index.html","hash":"e99902002faaafa202ccefc0773c6bf6b74714eb","modified":1650725202430},{"_id":"public/tags/test/index.html","hash":"49ae876c7fe430745ecacb8414fe0fc1079a0dd9","modified":1609235637961},{"_id":"public/tags/默认/index.html","hash":"12a770b3f3f7032c606eec33a0dbeb5f12d91478","modified":1609235637961},{"_id":"public/categories/未分类/index.html","hash":"58d5ff4361393b245e4f80110726632f2ad515cb","modified":1650725202430},{"_id":"public/categories/测试/index.html","hash":"c3528a17d04998c6d3a8860f006f701936c2e658","modified":1609235719594},{"_id":"public/tags/default/index.html","hash":"caced5384872e4016d53c2177afec3c9fb22caa0","modified":1650725202430},{"_id":"source/_posts/MAC系统下快速利用 GitHub + hexo 搭建个人静态博客网站.md","hash":"04c4359f7d7e963997d9df993dade51e6a325258","modified":1609334290761},{"_id":"public/2020/12/29/MAC系统下快速利用 GitHub + hexo 搭建个人静态博客网站/index.html","hash":"328be4c937c205765a4d03f39884c9efd4efa394","modified":1609334416707},{"_id":"public/categories/教程/index.html","hash":"17608f3f52570504fad83971a0c0df7fa995bd7d","modified":1650725202430},{"_id":"public/tags/GitHub/index.html","hash":"475b6c5e383af221db7e16c88fa35f2536f42a91","modified":1650725202430},{"_id":"public/tags/Hexo/index.html","hash":"a36fd8f40216bd3260f7347457058d35a89f136a","modified":1650725202430},{"_id":"public/tags/Mac/index.html","hash":"6521c5fcb9ed5d35298513c14c1f533f5a5bb982","modified":1650725202430},{"_id":"source/_posts/MAC系统下快速利用 GitHub + hexo 搭建个人静态博客网站/new repository.png","hash":"f0f724a426999babe9558c8be89890518cdecc4c","modified":1609334153474},{"_id":"public/2020/12/29/MAC系统下快速利用 GitHub + hexo 搭建个人静态博客网站/new repository.png","hash":"f0f724a426999babe9558c8be89890518cdecc4c","modified":1609334416707},{"_id":"source/_posts/built-blog.md","hash":"56da96df7214d9c83296d2813b9a3153d1326239","modified":1609561140000},{"_id":"public/2020/12/29/built-blog/index.html","hash":"75381e6196c043575db9a983a1e0f5daeed01b14","modified":1650725202430},{"_id":"source/_posts/built-blog/new-repository.png","hash":"f0f724a426999babe9558c8be89890518cdecc4c","modified":1609381458906},{"_id":"source/_posts/built-blog/ssh2.png","hash":"7b4e8ae4a73a646fca5ced23548dc5c9f9bb5a1b","modified":1609479742580},{"_id":"source/_posts/built-blog/ssh3.png","hash":"5e81485ecd9f1b73c3dad2a0e24b0a2ed8bafa2d","modified":1609479742602},{"_id":"source/_posts/built-blog/initialize-blog.png","hash":"9e1c8788f7858062e21ba288d39c6e9152d849ab","modified":1609385848818},{"_id":"source/_posts/built-blog/modify-setting1.png","hash":"1dc6c861eba1e421ea3927d8aa8ee9444994ac87","modified":1609477842732},{"_id":"source/_posts/built-blog/new-repository2.png","hash":"1115a75b6093a2627c7da9062fd7764213d5860f","modified":1609478920281},{"_id":"source/_posts/built-blog/ssh1.png","hash":"29ef037d032597b1c1cba878d923b49eeac4d2e3","modified":1609479742545},{"_id":"source/_posts/built-blog/edit-blog-content.png","hash":"2272e16a4972f4540b952bfb7cde68f81ebdebc1","modified":1609483298406},{"_id":"public/2020/12/29/built-blog/new-repository.png","hash":"f0f724a426999babe9558c8be89890518cdecc4c","modified":1609483750993},{"_id":"public/2020/12/29/built-blog/ssh2.png","hash":"7b4e8ae4a73a646fca5ced23548dc5c9f9bb5a1b","modified":1609483750993},{"_id":"public/2020/12/29/built-blog/ssh3.png","hash":"5e81485ecd9f1b73c3dad2a0e24b0a2ed8bafa2d","modified":1609483750993},{"_id":"public/2020/12/29/built-blog/initialize-blog.png","hash":"9e1c8788f7858062e21ba288d39c6e9152d849ab","modified":1609483750993},{"_id":"public/2020/12/29/built-blog/modify-setting1.png","hash":"1dc6c861eba1e421ea3927d8aa8ee9444994ac87","modified":1609483750993},{"_id":"public/2020/12/29/built-blog/new-repository2.png","hash":"1115a75b6093a2627c7da9062fd7764213d5860f","modified":1609483750993},{"_id":"public/2020/12/29/built-blog/ssh1.png","hash":"29ef037d032597b1c1cba878d923b49eeac4d2e3","modified":1609483750993},{"_id":"public/2020/12/29/built-blog/edit-blog-content.png","hash":"2272e16a4972f4540b952bfb7cde68f81ebdebc1","modified":1609483750993},{"_id":"source/_posts/built-blog/modify-theme.png","hash":"ed45f9cc77e3b408b079bee04dc5e4a7a4d28e6b","modified":1609485837172},{"_id":"public/2020/12/29/built-blog/modify-theme.png","hash":"ed45f9cc77e3b408b079bee04dc5e4a7a4d28e6b","modified":1609486692661},{"_id":"source/_posts/data-science.md","hash":"c1b033c414456b7978042d8431a95c2b3ec55d32","modified":1650725160434},{"_id":"source/_posts/algorithm-sort.md","hash":"3571abc1cc21996b6ae1186fd0c7887cc39252e6","modified":1650724892833},{"_id":"public/categories/算法/index.html","hash":"8ca5d0e4209f5bf7b836215b468742882e0691fd","modified":1650725202430},{"_id":"public/categories/数据科学/index.html","hash":"4b57f569053ab29a1e0bf0f735453a358047ed1f","modified":1650725202430},{"_id":"public/archives/2021/index.html","hash":"f4c17a3d38826615b7fe1b47e33fdc9b1a8dc70d","modified":1650725202430},{"_id":"public/archives/2021/01/index.html","hash":"8062578ff4e8a19968964b3d6e1f7a734b5081f5","modified":1650725202430},{"_id":"public/archives/2022/index.html","hash":"55a0396b2e1ac8fff8e4765a3a57af8971e01206","modified":1650725202430},{"_id":"public/archives/2022/04/index.html","hash":"b3e5c6195c5ab785a09acd346913bdfbfdeb44f5","modified":1650725202430},{"_id":"public/2022/04/23/algorithm-sort/index.html","hash":"591e96b39eff1ff4c1efefa6957bbd7d0d58b768","modified":1650725202430},{"_id":"public/2021/01/02/data-science/index.html","hash":"979e5da5ab70470ac88eaf650daf9b25e0b0e7cb","modified":1650725202430},{"_id":"public/tags/sort-algorithm-python/index.html","hash":"1a052512bca023e32a4d00478f19f1265624c2ff","modified":1650725202430},{"_id":"public/tags/data-science/index.html","hash":"316a2eb6d4513a9ce13daa118346e6176ba7a850","modified":1650725202430}],"Category":[{"name":"未分类","_id":"ckj9tclpp0000enulenk9bjff"},{"name":"测试","_id":"ckj9tecfs0000g2ul4z2v4bhe"},{"name":"教程","_id":"ckj9znbx200015hul2ht3cdit"},{"name":"data science","_id":"cl2bubgcz0001wwul6rzehqf7"},{"name":"算法","_id":"cl2bxmehx000182ul599n5zjx"},{"name":"数据科学","_id":"cl2bxmehy000382ulal7i9t9u"}],"Data":[],"Page":[{"title":"categories","date":"2020-12-29T09:14:00.000Z","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2020-12-29 17:14:00\n---\n","updated":"2020-12-29T09:14:00.820Z","path":"categories/index.html","comments":1,"layout":"page","_id":"ckj9s6lx60000zqul91o28rdj","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2020-12-29T09:14:20.000Z","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2020-12-29 17:14:20\n---\n","updated":"2020-12-29T09:14:20.429Z","path":"tags/index.html","comments":1,"layout":"page","_id":"ckj9s6lx90001zqul9c2agahq","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"about","date":"2020-12-29T09:12:19.000Z","comment":true,"_content":"\n\n## About me\n\nDo something interesting while you still have dreams and hopes.","source":"about/index.md","raw":"---\ntitle: about\ndate: 2020-12-29 17:12:19\ncomment: true\n---\n\n\n## About me\n\nDo something interesting while you still have dreams and hopes.","updated":"2022-04-23T14:45:26.914Z","path":"about/index.html","_id":"ckj9s6lxc0002zquladdl4f1l","comments":1,"layout":"page","content":"<h2 id=\"About-me\"><a href=\"#About-me\" class=\"headerlink\" title=\"About me\"></a>About me</h2><p>Do something interesting while you still have dreams and hopes.</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"About-me\"><a href=\"#About-me\" class=\"headerlink\" title=\"About me\"></a>About me</h2><p>Do something interesting while you still have dreams and hopes.</p>\n"}],"Post":[{"title":"Hello World","date":"2020-12-29T07:29:15.000Z","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ndate: 2020-12-29 15:29:15\ntags: default\ncategories: 未分类\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"updated":"2020-12-29T09:56:23.847Z","_id":"ckj9nnqbb0000z7ulbe84czii","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Welcome to <a class=\"link\"   href=\"https://hexo.io/\" >Hexo<i class=\"fas fa-external-link-alt\"></i></a>! This is your very first post. Check <a class=\"link\"   href=\"https://hexo.io/docs/\" >documentation<i class=\"fas fa-external-link-alt\"></i></a> for more info. If you get any problems when using Hexo, you can find the answer in <a class=\"link\"   href=\"https://hexo.io/docs/troubleshooting.html\" >troubleshooting<i class=\"fas fa-external-link-alt\"></i></a> or you can ask me on <a class=\"link\"   href=\"https://github.com/hexojs/hexo/issues\" >GitHub<i class=\"fas fa-external-link-alt\"></i></a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a class=\"link\"   href=\"https://hexo.io/docs/writing.html\" >Writing<i class=\"fas fa-external-link-alt\"></i></a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a class=\"link\"   href=\"https://hexo.io/docs/server.html\" >Server<i class=\"fas fa-external-link-alt\"></i></a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a class=\"link\"   href=\"https://hexo.io/docs/generating.html\" >Generating<i class=\"fas fa-external-link-alt\"></i></a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a class=\"link\"   href=\"https://hexo.io/docs/one-command-deployment.html\" >Deployment<i class=\"fas fa-external-link-alt\"></i></a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a class=\"link\"   href=\"https://hexo.io/\" >Hexo<i class=\"fas fa-external-link-alt\"></i></a>! This is your very first post. Check <a class=\"link\"   href=\"https://hexo.io/docs/\" >documentation<i class=\"fas fa-external-link-alt\"></i></a> for more info. If you get any problems when using Hexo, you can find the answer in <a class=\"link\"   href=\"https://hexo.io/docs/troubleshooting.html\" >troubleshooting<i class=\"fas fa-external-link-alt\"></i></a> or you can ask me on <a class=\"link\"   href=\"https://github.com/hexojs/hexo/issues\" >GitHub<i class=\"fas fa-external-link-alt\"></i></a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a class=\"link\"   href=\"https://hexo.io/docs/writing.html\" >Writing<i class=\"fas fa-external-link-alt\"></i></a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a class=\"link\"   href=\"https://hexo.io/docs/server.html\" >Server<i class=\"fas fa-external-link-alt\"></i></a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a class=\"link\"   href=\"https://hexo.io/docs/generating.html\" >Generating<i class=\"fas fa-external-link-alt\"></i></a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a class=\"link\"   href=\"https://hexo.io/docs/one-command-deployment.html\" >Deployment<i class=\"fas fa-external-link-alt\"></i></a></p>\n"},{"title":"MAC系统下快速利用 GitHub + hexo 搭建个人精美静态博客网站","date":"2020-12-29T07:29:15.000Z","_content":"作者选择 hexo，主要是因为它简单易操作，有很多现存精美主题；选择keep模版主要是因为有搜索功能，万物皆可搜索，其主题简洁大方又五脏俱全。\n```\n必要工具：GitHub、Git、Node.js、hexo;\n非必要工具：Vusual Studio Code;\n环境：macOS Big Sur 11.1; git 2.29.2; node 14.4.0;\n```\n[这里有个实际效果](https://aishuohuadebenrenyi.github.io/)\n\n## 注册GitHub账号及新建仓库\n### 注册 GitHub 账号\n[点此进入官网进行注册](https://github.com)\n### 新建仓库\n![new-epository1](./built-blog/new-repository.png \"新建仓库\")\n![](./built-blog/new-repository2.png \"填写信息\")\n\n## 搭建博客框架\n### 安装Git、Node.js\n\n```\n# 首先检查是否已安装git和node.js\n# 终端输入一下命令\nnode -v #若出现版本信息，说明已安装\ngit --version #若出现版本信息，说明已安装\n# 如果没有安装，则进行安装,都可以通过直接下载安装测序进行安装，这里不演示，提供下载网址：\n[git]: https://sourceforge.net/projects/git-osx-installer/\n[node.js]: https://nodejs.org/en/\n```\n\n### 安装hexo\n```\nnpm install -g hexo-cli\n```\n\n### 搭建博客并初始化\n```\n# 在任意目录下创建一个blog文件夹\nmkdir blog\n# 进入目录\ncd blog\n# 初始化目录\nhexo init\n开启本地服务 \n# hexo s\n```\n若出现红框中的网址，说明本地启动成功，可以在浏览器中输入该网址查看效果\n![初始化](./built-blog/initialize-blog.png)\n\n\n### 博客关联到 GitHub 仓库\n\n1. 编辑博客的主要配置文件：config.yml\n只需要将红框中repo替换成你的username就可以\n![](./built-blog/modify-setting1.png)\n```\ndeploy:\n  type: git\n  repo: https://github.com/aishuohuadebenrenyi/aishuohuadebenrenyi.github.io.git\n  branch: master\n```\n\n2. 在目录中执行\n```\n# 产生静态网页\nhexo g\n# 部署到GitHub page上\nhexo d\n# 注：上面两个命令会经常使用，可简写为：\nhexo g -d\n# 注：如果执行 hexo d 后提示 ERROR Deployer not found: git，则执行下列命令\nnpm install --save hexo-deployer-git\nhexo d\n```\n然后需要输入你的GitHub账号和密码即可。\n\n3. 关联GitHub账号，免密提交（可选步骤）\n为了避免每次部署都需要输入账号密码，有一个一劳永逸的办法：生成密钥提交到\nGitHub,具体方法如下：\n```\n# 用你注册GitHub时的邮箱号，进行秘钥生成\ncd ~\nssh-keygen -t rsa -C \"xxxxxxx@qq.com\"\n# 系统就会生成一个隐藏文件夹.ssh\ncd .ssh\nls\n# 复制公钥\nvim id_rsa.pub\n```\n然后在GitHub中操作\n![](./built-blog/ssh1.png)\n![](./built-blog/ssh2.png)\n![](./built-blog/ssh3.png)\n\n注：如果提交失败，可能是vim打开文件复制的格式问题，就把id_rsa.pub中内容复制到其他地方（如text）,再复制粘贴提交。\n\n至此已搭建好一个简易博客。\n\n### 博客基本使用\n\n1. 创建新的文章\n```\n# 新建一个markdown文件\nhexo new \"built-blog\" # 双引号中是文件名，任意取\n```\n\n如果要在markdown中插入图片，我比较喜欢的方式如下\n\n```\n# 修改博客根目录下的_config.yml文件\npost_asset_folder: true\n# 安装一个可以上传本地图片的插件\nnpm install https://github.com/CodeFalling/hexo-asset-image --save\n# 修改后再执行 \nhexo new \"built-blog\"\n# 在source/_posts中会生成文章built-blog.md和同名文件夹built-blog。将图片资源放在built-blog中，文章就可以使用相对路径引用图片资源了。\n```\n你也可以将所有的图片放在一个文件下，或使用CDN通过URL地址引用，具体方法这里不再赘述。\n\n2. 编辑markdown文件\n可以使用markdown神器 typora， 也可以使用VS Code。\n这里使用 VS Code。推荐两个插件\n   + Markdown All in One\n有在Vscode中编写Markdown所需的一切（键盘快捷键，目录，自动预览等）\n   + Markdown Preview Github Styling Custom\n预览效果和 GitHub page 一样，博客就是GitHub page 搭建，真正的所见及所得。\n\n![编辑文](./built-blog/edit-blog-content.png)\n\n3. 发布文章至博客\n```\nhexo g -d\n```\n\n## 博客美化及功能完善\n\n### 主题选择\nHexo官网：https://hexo.io/themes/\n这里选择keep，项目地址 https://github.com/XPoet/hexo-theme-keep\n\n```\n# 按照提示操作\n$ cd blog\n$ git clone https://github.com/XPoet/hexo-theme-keep themes/keep\n# 修改配置文件 _config.yml\ntheme: keep\n```\n![](./built-blog/modify-theme.png)\n\n至此已搭建好keep风格的博客。里面没有搜索(search)、分类(categories)、标签(tags)等功能，按照[keep官方操作指南](https://keep.xpoet.cn/2020/11/Keep-%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97/)操作即可，简单易懂，此处不再赘述。\n\n若有什么问题，欢迎探讨。\n","source":"_posts/built-blog.md","raw":"---\ntitle: MAC系统下快速利用 GitHub + hexo 搭建个人精美静态博客网站\ndate: 2020-12-29 15:29:15\ntags: [GitHub, Hexo, Mac]\ncategories: 教程\n---\n作者选择 hexo，主要是因为它简单易操作，有很多现存精美主题；选择keep模版主要是因为有搜索功能，万物皆可搜索，其主题简洁大方又五脏俱全。\n```\n必要工具：GitHub、Git、Node.js、hexo;\n非必要工具：Vusual Studio Code;\n环境：macOS Big Sur 11.1; git 2.29.2; node 14.4.0;\n```\n[这里有个实际效果](https://aishuohuadebenrenyi.github.io/)\n\n## 注册GitHub账号及新建仓库\n### 注册 GitHub 账号\n[点此进入官网进行注册](https://github.com)\n### 新建仓库\n![new-epository1](./built-blog/new-repository.png \"新建仓库\")\n![](./built-blog/new-repository2.png \"填写信息\")\n\n## 搭建博客框架\n### 安装Git、Node.js\n\n```\n# 首先检查是否已安装git和node.js\n# 终端输入一下命令\nnode -v #若出现版本信息，说明已安装\ngit --version #若出现版本信息，说明已安装\n# 如果没有安装，则进行安装,都可以通过直接下载安装测序进行安装，这里不演示，提供下载网址：\n[git]: https://sourceforge.net/projects/git-osx-installer/\n[node.js]: https://nodejs.org/en/\n```\n\n### 安装hexo\n```\nnpm install -g hexo-cli\n```\n\n### 搭建博客并初始化\n```\n# 在任意目录下创建一个blog文件夹\nmkdir blog\n# 进入目录\ncd blog\n# 初始化目录\nhexo init\n开启本地服务 \n# hexo s\n```\n若出现红框中的网址，说明本地启动成功，可以在浏览器中输入该网址查看效果\n![初始化](./built-blog/initialize-blog.png)\n\n\n### 博客关联到 GitHub 仓库\n\n1. 编辑博客的主要配置文件：config.yml\n只需要将红框中repo替换成你的username就可以\n![](./built-blog/modify-setting1.png)\n```\ndeploy:\n  type: git\n  repo: https://github.com/aishuohuadebenrenyi/aishuohuadebenrenyi.github.io.git\n  branch: master\n```\n\n2. 在目录中执行\n```\n# 产生静态网页\nhexo g\n# 部署到GitHub page上\nhexo d\n# 注：上面两个命令会经常使用，可简写为：\nhexo g -d\n# 注：如果执行 hexo d 后提示 ERROR Deployer not found: git，则执行下列命令\nnpm install --save hexo-deployer-git\nhexo d\n```\n然后需要输入你的GitHub账号和密码即可。\n\n3. 关联GitHub账号，免密提交（可选步骤）\n为了避免每次部署都需要输入账号密码，有一个一劳永逸的办法：生成密钥提交到\nGitHub,具体方法如下：\n```\n# 用你注册GitHub时的邮箱号，进行秘钥生成\ncd ~\nssh-keygen -t rsa -C \"xxxxxxx@qq.com\"\n# 系统就会生成一个隐藏文件夹.ssh\ncd .ssh\nls\n# 复制公钥\nvim id_rsa.pub\n```\n然后在GitHub中操作\n![](./built-blog/ssh1.png)\n![](./built-blog/ssh2.png)\n![](./built-blog/ssh3.png)\n\n注：如果提交失败，可能是vim打开文件复制的格式问题，就把id_rsa.pub中内容复制到其他地方（如text）,再复制粘贴提交。\n\n至此已搭建好一个简易博客。\n\n### 博客基本使用\n\n1. 创建新的文章\n```\n# 新建一个markdown文件\nhexo new \"built-blog\" # 双引号中是文件名，任意取\n```\n\n如果要在markdown中插入图片，我比较喜欢的方式如下\n\n```\n# 修改博客根目录下的_config.yml文件\npost_asset_folder: true\n# 安装一个可以上传本地图片的插件\nnpm install https://github.com/CodeFalling/hexo-asset-image --save\n# 修改后再执行 \nhexo new \"built-blog\"\n# 在source/_posts中会生成文章built-blog.md和同名文件夹built-blog。将图片资源放在built-blog中，文章就可以使用相对路径引用图片资源了。\n```\n你也可以将所有的图片放在一个文件下，或使用CDN通过URL地址引用，具体方法这里不再赘述。\n\n2. 编辑markdown文件\n可以使用markdown神器 typora， 也可以使用VS Code。\n这里使用 VS Code。推荐两个插件\n   + Markdown All in One\n有在Vscode中编写Markdown所需的一切（键盘快捷键，目录，自动预览等）\n   + Markdown Preview Github Styling Custom\n预览效果和 GitHub page 一样，博客就是GitHub page 搭建，真正的所见及所得。\n\n![编辑文](./built-blog/edit-blog-content.png)\n\n3. 发布文章至博客\n```\nhexo g -d\n```\n\n## 博客美化及功能完善\n\n### 主题选择\nHexo官网：https://hexo.io/themes/\n这里选择keep，项目地址 https://github.com/XPoet/hexo-theme-keep\n\n```\n# 按照提示操作\n$ cd blog\n$ git clone https://github.com/XPoet/hexo-theme-keep themes/keep\n# 修改配置文件 _config.yml\ntheme: keep\n```\n![](./built-blog/modify-theme.png)\n\n至此已搭建好keep风格的博客。里面没有搜索(search)、分类(categories)、标签(tags)等功能，按照[keep官方操作指南](https://keep.xpoet.cn/2020/11/Keep-%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97/)操作即可，简单易懂，此处不再赘述。\n\n若有什么问题，欢迎探讨。\n","slug":"built-blog","published":1,"updated":"2021-01-02T04:19:00.000Z","_id":"ckjc7vn7v00002xul46abceyh","comments":1,"layout":"post","photos":[],"link":"","content":"<p>作者选择 hexo，主要是因为它简单易操作，有很多现存精美主题；选择keep模版主要是因为有搜索功能，万物皆可搜索，其主题简洁大方又五脏俱全。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">必要工具：GitHub、Git、Node.js、hexo;</span><br><span class=\"line\">非必要工具：Vusual Studio Code;</span><br><span class=\"line\">环境：macOS Big Sur 11.1; git 2.29.2; node 14.4.0;</span><br></pre></td></tr></table></figure>\n<p><a class=\"link\" href=\"https://aishuohuadebenrenyi.github.io/\">这里有个实际效果<i class=\"fas fa-external-link-alt\"></i></a></p>\n<h2 id=\"注册GitHub账号及新建仓库\"><a href=\"#注册GitHub账号及新建仓库\" class=\"headerlink\" title=\"注册GitHub账号及新建仓库\"></a>注册GitHub账号及新建仓库</h2><h3 id=\"注册-GitHub-账号\"><a href=\"#注册-GitHub-账号\" class=\"headerlink\" title=\"注册 GitHub 账号\"></a>注册 GitHub 账号</h3><p><a class=\"link\" href=\"https://github.com/\">点此进入官网进行注册<i class=\"fas fa-external-link-alt\"></i></a></p>\n<h3 id=\"新建仓库\"><a href=\"#新建仓库\" class=\"headerlink\" title=\"新建仓库\"></a>新建仓库</h3><p><img src=\"/2020/12/29/built-blog/new-repository.png\" alt=\"new-epository1\" title=\"新建仓库\"><br><img src=\"/2020/12/29/built-blog/new-repository2.png\" title=\"填写信息\"></p>\n<h2 id=\"搭建博客框架\"><a href=\"#搭建博客框架\" class=\"headerlink\" title=\"搭建博客框架\"></a>搭建博客框架</h2><h3 id=\"安装Git、Node-js\"><a href=\"#安装Git、Node-js\" class=\"headerlink\" title=\"安装Git、Node.js\"></a>安装Git、Node.js</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 首先检查是否已安装git和node.js</span><br><span class=\"line\"># 终端输入一下命令</span><br><span class=\"line\">node -v #若出现版本信息，说明已安装</span><br><span class=\"line\">git --version #若出现版本信息，说明已安装</span><br><span class=\"line\"># 如果没有安装，则进行安装,都可以通过直接下载安装测序进行安装，这里不演示，提供下载网址：</span><br><span class=\"line\">[git]: https:&#x2F;&#x2F;sourceforge.net&#x2F;projects&#x2F;git-osx-installer&#x2F;</span><br><span class=\"line\">[node.js]: https:&#x2F;&#x2F;nodejs.org&#x2F;en&#x2F;</span><br></pre></td></tr></table></figure>\n<h3 id=\"安装hexo\"><a href=\"#安装hexo\" class=\"headerlink\" title=\"安装hexo\"></a>安装hexo</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install -g hexo-cli</span><br></pre></td></tr></table></figure>\n<h3 id=\"搭建博客并初始化\"><a href=\"#搭建博客并初始化\" class=\"headerlink\" title=\"搭建博客并初始化\"></a>搭建博客并初始化</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 在任意目录下创建一个blog文件夹</span><br><span class=\"line\">mkdir blog</span><br><span class=\"line\"># 进入目录</span><br><span class=\"line\">cd blog</span><br><span class=\"line\"># 初始化目录</span><br><span class=\"line\">hexo init</span><br><span class=\"line\">开启本地服务 </span><br><span class=\"line\"># hexo s</span><br></pre></td></tr></table></figure>\n<p>若出现红框中的网址，说明本地启动成功，可以在浏览器中输入该网址查看效果<br><img src=\"/2020/12/29/built-blog/initialize-blog.png\" alt=\"初始化\"></p>\n<h3 id=\"博客关联到-GitHub-仓库\"><a href=\"#博客关联到-GitHub-仓库\" class=\"headerlink\" title=\"博客关联到 GitHub 仓库\"></a>博客关联到 GitHub 仓库</h3><ol>\n<li><p>编辑博客的主要配置文件：config.yml<br>只需要将红框中repo替换成你的username就可以<br><img src=\"/2020/12/29/built-blog/modify-setting1.png\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">deploy:</span><br><span class=\"line\">  type: git</span><br><span class=\"line\">  repo: https:&#x2F;&#x2F;github.com&#x2F;aishuohuadebenrenyi&#x2F;aishuohuadebenrenyi.github.io.git</span><br><span class=\"line\">  branch: master</span><br></pre></td></tr></table></figure></li>\n<li><p>在目录中执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 产生静态网页</span><br><span class=\"line\">hexo g</span><br><span class=\"line\"># 部署到GitHub page上</span><br><span class=\"line\">hexo d</span><br><span class=\"line\"># 注：上面两个命令会经常使用，可简写为：</span><br><span class=\"line\">hexo g -d</span><br><span class=\"line\"># 注：如果执行 hexo d 后提示 ERROR Deployer not found: git，则执行下列命令</span><br><span class=\"line\">npm install --save hexo-deployer-git</span><br><span class=\"line\">hexo d</span><br></pre></td></tr></table></figure>\n<p>然后需要输入你的GitHub账号和密码即可。</p>\n</li>\n<li><p>关联GitHub账号，免密提交（可选步骤）<br>为了避免每次部署都需要输入账号密码，有一个一劳永逸的办法：生成密钥提交到<br>GitHub,具体方法如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 用你注册GitHub时的邮箱号，进行秘钥生成</span><br><span class=\"line\">cd ~</span><br><span class=\"line\">ssh-keygen -t rsa -C &quot;xxxxxxx@qq.com&quot;</span><br><span class=\"line\"># 系统就会生成一个隐藏文件夹.ssh</span><br><span class=\"line\">cd .ssh</span><br><span class=\"line\">ls</span><br><span class=\"line\"># 复制公钥</span><br><span class=\"line\">vim id_rsa.pub</span><br></pre></td></tr></table></figure>\n<p>然后在GitHub中操作<br><img src=\"/2020/12/29/built-blog/ssh1.png\"><br><img src=\"/2020/12/29/built-blog/ssh2.png\"><br><img src=\"/2020/12/29/built-blog/ssh3.png\"></p>\n</li>\n</ol>\n<p>注：如果提交失败，可能是vim打开文件复制的格式问题，就把id_rsa.pub中内容复制到其他地方（如text）,再复制粘贴提交。</p>\n<p>至此已搭建好一个简易博客。</p>\n<h3 id=\"博客基本使用\"><a href=\"#博客基本使用\" class=\"headerlink\" title=\"博客基本使用\"></a>博客基本使用</h3><ol>\n<li>创建新的文章<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 新建一个markdown文件</span><br><span class=\"line\">hexo new &quot;built-blog&quot; # 双引号中是文件名，任意取</span><br></pre></td></tr></table></figure>\n如果要在markdown中插入图片，我比较喜欢的方式如下</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 修改博客根目录下的_config.yml文件</span><br><span class=\"line\">post_asset_folder: true</span><br><span class=\"line\"># 安装一个可以上传本地图片的插件</span><br><span class=\"line\">npm install https:&#x2F;&#x2F;github.com&#x2F;CodeFalling&#x2F;hexo-asset-image --save</span><br><span class=\"line\"># 修改后再执行 </span><br><span class=\"line\">hexo new &quot;built-blog&quot;</span><br><span class=\"line\"># 在source&#x2F;_posts中会生成文章built-blog.md和同名文件夹built-blog。将图片资源放在built-blog中，文章就可以使用相对路径引用图片资源了。</span><br></pre></td></tr></table></figure>\n<p>你也可以将所有的图片放在一个文件下，或使用CDN通过URL地址引用，具体方法这里不再赘述。</p>\n<ol start=\"2\">\n<li>编辑markdown文件<br>可以使用markdown神器 typora， 也可以使用VS Code。<br>这里使用 VS Code。推荐两个插件<ul>\n<li>Markdown All in One<br>有在Vscode中编写Markdown所需的一切（键盘快捷键，目录，自动预览等）</li>\n<li>Markdown Preview Github Styling Custom<br>预览效果和 GitHub page 一样，博客就是GitHub page 搭建，真正的所见及所得。</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"/2020/12/29/built-blog/edit-blog-content.png\" alt=\"编辑文\"></p>\n<ol start=\"3\">\n<li>发布文章至博客<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g -d</span><br></pre></td></tr></table></figure>\n<h2 id=\"博客美化及功能完善\"><a href=\"#博客美化及功能完善\" class=\"headerlink\" title=\"博客美化及功能完善\"></a>博客美化及功能完善</h2></li>\n</ol>\n<h3 id=\"主题选择\"><a href=\"#主题选择\" class=\"headerlink\" title=\"主题选择\"></a>主题选择</h3><p>Hexo官网：<a class=\"link\" href=\"https://hexo.io/themes/\">https://hexo.io/themes/<i class=\"fas fa-external-link-alt\"></i></a><br>这里选择keep，项目地址 <a class=\"link\" href=\"https://github.com/XPoet/hexo-theme-keep\">https://github.com/XPoet/hexo-theme-keep<i class=\"fas fa-external-link-alt\"></i></a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 按照提示操作</span><br><span class=\"line\">$ cd blog</span><br><span class=\"line\">$ git clone https:&#x2F;&#x2F;github.com&#x2F;XPoet&#x2F;hexo-theme-keep themes&#x2F;keep</span><br><span class=\"line\"># 修改配置文件 _config.yml</span><br><span class=\"line\">theme: keep</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/12/29/built-blog/modify-theme.png\"></p>\n<p>至此已搭建好keep风格的博客。里面没有搜索(search)、分类(categories)、标签(tags)等功能，按照<a class=\"link\" href=\"https://keep.xpoet.cn/2020/11/Keep-%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97/\">keep官方操作指南<i class=\"fas fa-external-link-alt\"></i></a>操作即可，简单易懂，此处不再赘述。</p>\n<p>若有什么问题，欢迎探讨。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>作者选择 hexo，主要是因为它简单易操作，有很多现存精美主题；选择keep模版主要是因为有搜索功能，万物皆可搜索，其主题简洁大方又五脏俱全。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">必要工具：GitHub、Git、Node.js、hexo;</span><br><span class=\"line\">非必要工具：Vusual Studio Code;</span><br><span class=\"line\">环境：macOS Big Sur 11.1; git 2.29.2; node 14.4.0;</span><br></pre></td></tr></table></figure>\n<p><a class=\"link\" href=\"https://aishuohuadebenrenyi.github.io/\">这里有个实际效果<i class=\"fas fa-external-link-alt\"></i></a></p>\n<h2 id=\"注册GitHub账号及新建仓库\"><a href=\"#注册GitHub账号及新建仓库\" class=\"headerlink\" title=\"注册GitHub账号及新建仓库\"></a>注册GitHub账号及新建仓库</h2><h3 id=\"注册-GitHub-账号\"><a href=\"#注册-GitHub-账号\" class=\"headerlink\" title=\"注册 GitHub 账号\"></a>注册 GitHub 账号</h3><p><a class=\"link\" href=\"https://github.com/\">点此进入官网进行注册<i class=\"fas fa-external-link-alt\"></i></a></p>\n<h3 id=\"新建仓库\"><a href=\"#新建仓库\" class=\"headerlink\" title=\"新建仓库\"></a>新建仓库</h3><p><img src=\"/2020/12/29/built-blog/new-repository.png\" alt=\"new-epository1\" title=\"新建仓库\"><br><img src=\"/2020/12/29/built-blog/new-repository2.png\" title=\"填写信息\"></p>\n<h2 id=\"搭建博客框架\"><a href=\"#搭建博客框架\" class=\"headerlink\" title=\"搭建博客框架\"></a>搭建博客框架</h2><h3 id=\"安装Git、Node-js\"><a href=\"#安装Git、Node-js\" class=\"headerlink\" title=\"安装Git、Node.js\"></a>安装Git、Node.js</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 首先检查是否已安装git和node.js</span><br><span class=\"line\"># 终端输入一下命令</span><br><span class=\"line\">node -v #若出现版本信息，说明已安装</span><br><span class=\"line\">git --version #若出现版本信息，说明已安装</span><br><span class=\"line\"># 如果没有安装，则进行安装,都可以通过直接下载安装测序进行安装，这里不演示，提供下载网址：</span><br><span class=\"line\">[git]: https:&#x2F;&#x2F;sourceforge.net&#x2F;projects&#x2F;git-osx-installer&#x2F;</span><br><span class=\"line\">[node.js]: https:&#x2F;&#x2F;nodejs.org&#x2F;en&#x2F;</span><br></pre></td></tr></table></figure>\n<h3 id=\"安装hexo\"><a href=\"#安装hexo\" class=\"headerlink\" title=\"安装hexo\"></a>安装hexo</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install -g hexo-cli</span><br></pre></td></tr></table></figure>\n<h3 id=\"搭建博客并初始化\"><a href=\"#搭建博客并初始化\" class=\"headerlink\" title=\"搭建博客并初始化\"></a>搭建博客并初始化</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 在任意目录下创建一个blog文件夹</span><br><span class=\"line\">mkdir blog</span><br><span class=\"line\"># 进入目录</span><br><span class=\"line\">cd blog</span><br><span class=\"line\"># 初始化目录</span><br><span class=\"line\">hexo init</span><br><span class=\"line\">开启本地服务 </span><br><span class=\"line\"># hexo s</span><br></pre></td></tr></table></figure>\n<p>若出现红框中的网址，说明本地启动成功，可以在浏览器中输入该网址查看效果<br><img src=\"/2020/12/29/built-blog/initialize-blog.png\" alt=\"初始化\"></p>\n<h3 id=\"博客关联到-GitHub-仓库\"><a href=\"#博客关联到-GitHub-仓库\" class=\"headerlink\" title=\"博客关联到 GitHub 仓库\"></a>博客关联到 GitHub 仓库</h3><ol>\n<li><p>编辑博客的主要配置文件：config.yml<br>只需要将红框中repo替换成你的username就可以<br><img src=\"/2020/12/29/built-blog/modify-setting1.png\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">deploy:</span><br><span class=\"line\">  type: git</span><br><span class=\"line\">  repo: https:&#x2F;&#x2F;github.com&#x2F;aishuohuadebenrenyi&#x2F;aishuohuadebenrenyi.github.io.git</span><br><span class=\"line\">  branch: master</span><br></pre></td></tr></table></figure></li>\n<li><p>在目录中执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 产生静态网页</span><br><span class=\"line\">hexo g</span><br><span class=\"line\"># 部署到GitHub page上</span><br><span class=\"line\">hexo d</span><br><span class=\"line\"># 注：上面两个命令会经常使用，可简写为：</span><br><span class=\"line\">hexo g -d</span><br><span class=\"line\"># 注：如果执行 hexo d 后提示 ERROR Deployer not found: git，则执行下列命令</span><br><span class=\"line\">npm install --save hexo-deployer-git</span><br><span class=\"line\">hexo d</span><br></pre></td></tr></table></figure>\n<p>然后需要输入你的GitHub账号和密码即可。</p>\n</li>\n<li><p>关联GitHub账号，免密提交（可选步骤）<br>为了避免每次部署都需要输入账号密码，有一个一劳永逸的办法：生成密钥提交到<br>GitHub,具体方法如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 用你注册GitHub时的邮箱号，进行秘钥生成</span><br><span class=\"line\">cd ~</span><br><span class=\"line\">ssh-keygen -t rsa -C &quot;xxxxxxx@qq.com&quot;</span><br><span class=\"line\"># 系统就会生成一个隐藏文件夹.ssh</span><br><span class=\"line\">cd .ssh</span><br><span class=\"line\">ls</span><br><span class=\"line\"># 复制公钥</span><br><span class=\"line\">vim id_rsa.pub</span><br></pre></td></tr></table></figure>\n<p>然后在GitHub中操作<br><img src=\"/2020/12/29/built-blog/ssh1.png\"><br><img src=\"/2020/12/29/built-blog/ssh2.png\"><br><img src=\"/2020/12/29/built-blog/ssh3.png\"></p>\n</li>\n</ol>\n<p>注：如果提交失败，可能是vim打开文件复制的格式问题，就把id_rsa.pub中内容复制到其他地方（如text）,再复制粘贴提交。</p>\n<p>至此已搭建好一个简易博客。</p>\n<h3 id=\"博客基本使用\"><a href=\"#博客基本使用\" class=\"headerlink\" title=\"博客基本使用\"></a>博客基本使用</h3><ol>\n<li>创建新的文章<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 新建一个markdown文件</span><br><span class=\"line\">hexo new &quot;built-blog&quot; # 双引号中是文件名，任意取</span><br></pre></td></tr></table></figure>\n如果要在markdown中插入图片，我比较喜欢的方式如下</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 修改博客根目录下的_config.yml文件</span><br><span class=\"line\">post_asset_folder: true</span><br><span class=\"line\"># 安装一个可以上传本地图片的插件</span><br><span class=\"line\">npm install https:&#x2F;&#x2F;github.com&#x2F;CodeFalling&#x2F;hexo-asset-image --save</span><br><span class=\"line\"># 修改后再执行 </span><br><span class=\"line\">hexo new &quot;built-blog&quot;</span><br><span class=\"line\"># 在source&#x2F;_posts中会生成文章built-blog.md和同名文件夹built-blog。将图片资源放在built-blog中，文章就可以使用相对路径引用图片资源了。</span><br></pre></td></tr></table></figure>\n<p>你也可以将所有的图片放在一个文件下，或使用CDN通过URL地址引用，具体方法这里不再赘述。</p>\n<ol start=\"2\">\n<li>编辑markdown文件<br>可以使用markdown神器 typora， 也可以使用VS Code。<br>这里使用 VS Code。推荐两个插件<ul>\n<li>Markdown All in One<br>有在Vscode中编写Markdown所需的一切（键盘快捷键，目录，自动预览等）</li>\n<li>Markdown Preview Github Styling Custom<br>预览效果和 GitHub page 一样，博客就是GitHub page 搭建，真正的所见及所得。</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"/2020/12/29/built-blog/edit-blog-content.png\" alt=\"编辑文\"></p>\n<ol start=\"3\">\n<li>发布文章至博客<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g -d</span><br></pre></td></tr></table></figure>\n<h2 id=\"博客美化及功能完善\"><a href=\"#博客美化及功能完善\" class=\"headerlink\" title=\"博客美化及功能完善\"></a>博客美化及功能完善</h2></li>\n</ol>\n<h3 id=\"主题选择\"><a href=\"#主题选择\" class=\"headerlink\" title=\"主题选择\"></a>主题选择</h3><p>Hexo官网：<a class=\"link\" href=\"https://hexo.io/themes/\">https://hexo.io/themes/<i class=\"fas fa-external-link-alt\"></i></a><br>这里选择keep，项目地址 <a class=\"link\" href=\"https://github.com/XPoet/hexo-theme-keep\">https://github.com/XPoet/hexo-theme-keep<i class=\"fas fa-external-link-alt\"></i></a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 按照提示操作</span><br><span class=\"line\">$ cd blog</span><br><span class=\"line\">$ git clone https:&#x2F;&#x2F;github.com&#x2F;XPoet&#x2F;hexo-theme-keep themes&#x2F;keep</span><br><span class=\"line\"># 修改配置文件 _config.yml</span><br><span class=\"line\">theme: keep</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2020/12/29/built-blog/modify-theme.png\"></p>\n<p>至此已搭建好keep风格的博客。里面没有搜索(search)、分类(categories)、标签(tags)等功能，按照<a class=\"link\" href=\"https://keep.xpoet.cn/2020/11/Keep-%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97/\">keep官方操作指南<i class=\"fas fa-external-link-alt\"></i></a>操作即可，简单易懂，此处不再赘述。</p>\n<p>若有什么问题，欢迎探讨。</p>\n"},{"title":"数据科学家养成计划手册(持续更新中)","date":"2021-01-02T05:58:55.000Z","_content":"\n\n本人是一名数据爱好者，梦想是成为一名数据科学家。这里记录我从零开始的学习过程。内容若有什么不对的，欢迎大家批评指正，也希望能和感兴趣的读者一起探讨。\n\n> 注1：数据科学家有两条发展路径，一条是偏向业务可视化的商业分析师（数据分析师），一条是偏向于建模算法的数据挖掘工程师。这里不予严格区分。本文旨在展示数据获取、处理、建模、分析、可视化等全过程。读者可根据自己的喜好选择一个方向深入。\n\n> 注2：本文长期维护\n\n## 数据分析全貌\n\n数据分析是一门利用统计学知识，从数据中提取有用信息，进行总结和概括的学科。\n\n一名优秀的数据分析师应该具备的能力：`好奇`、`谨慎`、`责任`。\n\n在实际的工作中，每个分析师应该有自己处理问题的工作流程，并在实际的工作中不断的完善和迭代。最基本的流程如下所示：  \n`数据工作流`：抛出问题 -> 获取数据 -> 数据研究 -> 问题结论 -> 解决办法\n\n`数据建模和挖掘具体流程`：数据获取 -> 探索性分析及可视化 -> 数据预处理 -> 数据挖掘建模 -> 模型评估\n\n前两个流程（数据获取、探索性分析与可视化）也是狭义的数据分析。\n\n`前修知识`：数学(高数、概率论与数理统计)、统计学、python(或者 R)；（sql、excel 本文暂不探讨）\n\n涉及的python第三方库：\n\n* 数据科学包：Numpy(数据基础包)、Pnadas(数据处理神器)、Scipy(科学计算包)、Matplotlib(可视化)、Seaborn(可视化)\n* 机器学习包：Scikit-learn(传统机器学习)、TensorFlow(深度学习)\n\n`工具`：Visual Studio Code(Jupyter插件)\n\n## 数据获取\n\n### 数据仓库\n\n`定义`：将所有的业务数据汇总处理，构成数据仓库(DW)\n\n* 全面、完备、尽可能详细的记录全部事实\n    * 如某人几分几秒浏览了什么页面\n* 方便对部分维度和数据的整理(数据集市-DM)\n\n`数据仓库与文件和日志的区别`：文件、日志只能顺序记录，不方便查找、比较、抽取特征等操作\n\n`数据仓库与数据库的区别`：\n\n* 数据仓库：\n    * 面向主题存储\n        * 主题：较高层次上对分析对象数据的一个完整并且一致的描述\n        * 如购买图书这个行为就是个主题，在几时几分几秒以什么样的价格购买了什么样的书就是这个购买主题的一个记录，记录里有时间有用户信息，有图书信息等各个维度的信息。主题就是各个数据相互联系的描述。\n    * 针对分析(OLAP)\n    * 可能冗余、相对变化较大、数据量大\n* 数据库\n    * 面向业务存储\n        * 高并发、快速读写、数据结构精简\n    * 针对应用(OLTP)\n        * 为用户提供数据上的支持和服务\n    * 组织规范\n\n### 检测与抓取(爬虫)\n\n`检测`：用检测设备和检测算法直接获取数据。如传感器网络\n\n`抓取(爬虫)`：直接通过获取网页内容进行解析和分析，直接解析网页、接口和文件的信息。\n\n* python常用库：\n    * 抓取：urllib、urllib2、requests、scrap\n    * 渲染：PhantomJS\n    * 解析：beautifulSoup、Xpath(lxml)\n\n### 填写、日志、埋点\n\n`用户填写`：用户填写的信息。如调查问卷、用户注册时填写的信息\n\n`操作日志`：以文件形式记录。\n\n* 前端日志：网页和APP中记录的日志\n* 后端日志：服务器的日志\n\n`埋点`：在APP或网页应用中针对特定的流程收集一定的信息，用来跟踪APP或网页被使用的情况，以便后继用来进一步优化产品或进行运营支持。\n\n* 比较常用的记录项：访问、访客、停留时间、页面查看和跳出率。\n* 分为页面统计和统计操作行为。\n* 可自己开发也可选择第三方工具，如友盟。\n\n### 计算\n\n`计算`：通过已有的数据计算生成衍生数据。如企业的投入产出比\n\n### 数据学习网站\n\n`数据竞赛网站`：kaggle & 天池  \n`图片数据集网站`：ImageNet、Open Images  \n`各领域统计数据`：统计局、政府机构、公司财报等\n\n## 探索性分析与数据可视化\n\n### 统计分析方法基础\n\n`集中趋势`：均值、中位数与分位数、众数  \n四分位数的计算方法：\n\n```\n# n为数据的个数  \nQ1位置 = (n+1)*25  \nQ2位置 = (n+1)*5  \nQ3位置 = (n+1)*75\n```\n\n`离中趋势`：标准差、方差\n\n`数据分布`：偏态与峰态、正态分布与三大分布\n\n* 偏态系数：数据平均值偏离状态的一种衡量\n* 峰态系数：数据分布集中强度的衡量\n* 分布概率\n    * 正态分布\n    * 卡方分布: 标准正态分布的平方和\n    * $T$分布：用于根据小样本来估计呈正态分布且方差未知的总体的均值\n    * $F$分布\n\n`抽样理论`\n\n* 抽样原因：\n    * 数据量异常大，全量计算的时间、成本都很大。(大数据发展，不再是问题)\n    * 全量检测不现实。比如测灯泡的寿命\n* 抽样类型：\n    * 重复抽样(有放回的抽样)\n    * 不重复抽样(无放回的抽样)\n* 抽样方式\n    * 完全随机抽样\n    * 等差距抽样：某个属性从低到高排列，等间距抽样\n    * 分类的分层抽样：根据各个类别的比例抽样\n* 抽样平均误差计算公式\n    * 重复抽样\n    * 不重复抽样\n* 估计总体时抽样数量的确定\n    * 重复抽样\n    * 不重复抽样\n\n`数据分类`\n\n* 定类(类别)：根据事物离散、无差别属性进行分类。如性别、名族\n* 定序(顺序)：可以界定数据的大小、但不能测定差值。如收入的高、中、低\n* 定距(间距)：可以界定数据大小同时可测定差距，但无绝对零点(乘除比率无意义)。如温度\n* 定比(比率)：可以界定数据大小同时可测定差距，有绝对零点。如身高、体重\n\n`假设检验与方差检验`  \n假设检验：做出一个假设，根据数据或已知的分布性质来推断这个假设成立的概率有多大。\n\n* 建立原假设 H0 (包含等号)，H1是H0的反命题，也称备择假设\n* 选择检验统计量\n* 根据显著性水平(一般为0.05)，确定拒绝域\n* 计算`P值`或`样本统计量`，做出判断\n\n检验统计量：\n\n* $\\\\mu$ 分布\n* 卡方分布：用于检测两个因素之间有无强联系\n* $T$检验：比较两组样本分布是否一致；两组值的均值有无较大差异\n* $F$检验（方差检验)：用于方差分析。多样本两两之间是否有差异。\n    * 总变差平方和\n    * 平均平方和/组内平方和\n    * 残差平方和/组内平方差\n    * 统计量\n\n> 注：也可以通过qq图来判断一个分布是否符合一个已知的分布，比如找到该分布的分位数做纵轴，正态分布的分位数做横轴，若连线接近角平分线，则符合\n\n`相关系数`：衡量两组数据或两组样本的分布趋势、变化趋势一致性程度的因子。\n\n* 皮尔逊(Pearson)相关系数\n* 斯皮尔(Spearman)曼相关系数\n\n`回归：线性回归`：确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法\n\n* 关键度量指标：\n    * 一元决定系数\n    * 多元决定系数\n* 残差不相关(DW检验)\n\n`PCA与奇异值分解`：尽可能少的失真情况下，线性降维，成分提取\n\n* 主成分分析法(PCA)\n\n    * 求特征协方差矩阵\n    * 求协方差矩阵的特征值和特征向量\n    * 将特征值按照从大到小的顺序排序，选取其中最大的k个\n    * 将样本点投影到选取的特征向量上\n* 奇异值分解(SVD)\n\n    * 特征矩阵 A 分解为 m_m 的酉阵，m_n 半正定矩阵(奇异矩阵)，n*n 酉阵转置 v\n\n### 探索性分析\n\n#### 单因子探索\n\n——展现数据全貌\n\n* 异常值分析(可用箱线图展示)\n    * 连续异常值\n        * 处理办法：舍去；异常值用边界值代替(四分位数)\n    * 离散异常值：离散属性定义范围外的所有值均为异常值。如空值；收入离散成高、中、低之外的值\n        * 处理办法：舍去；把所有异常值当作单独的一个值处理\n    * 常识异常值：在限定知识和常识范围外的所有值均为异常值。如身高20m\n* 对比分析\n    * 比什么：\n        * 绝对数比较：数值直接比较。如比较收入、身高\n        * 相对数比较：把几个有联系的指标联合成新的指标\n            * 结构相对数：部分与整体。如合格率、通过率\n            * 比例相对数：总体内用不同部分的数值进行比较。如三大产业相互比较\n            * 比较相对数：同一时空下相似或同质的指标进行比较。如不同时期同一产品的价格对比；不同互联网电商公司的待遇水平对比\n            * 动态相对数：一般包含时间概念。如用户增速\n            * 强度相对数：性质不同但又互相联系的属性进行联合。如人均、亩产、密度\n    * 怎么比：\n        * 时间\n            * 同比：今年2月同去年2月比\n            * 环比：今年2月同今年1月比\n        * 空间\n            * 现实方位：如不同国家、不同城市\n            * 逻辑上空间：如一家公司的不同部门；不同家公司之间的比较\n        * 经验和计划：如历史上失业率达到百分之几回发生暴乱，把国家的失业率与之比较；工作排期与实际进度之间的比较\n* 结构分析：各组成部分的分布与规律\n    * 静态：直接分析总体的组成。如十一五时间三大产业的比例\n    * 动态：时间为轴分析结构变化的趋势。如十一五期间三大产业比的变化\n* 分布分析：数据分布频率的显示分析\n    * 直接获取的概率分布\n    * 判断是否为正态分布\n        * 偏态系数\n        * 峰态系数\n    * 极大似然\n\n#### 多因子探索\n\n——探索属性与属性之间的联系\n\n* 交叉分析\n    * 热力图\n    * 透视表\n* 分组与钻取\n* 相关分析\n* 因子分析\n* 聚类分析(建模中也会用到)\n* 回归分析(建模中也会用到)\n\n### 数据可视化\n\n* 柱状图：横坐标表示离散值\n* 直方图：横坐标表示范围\n* 箱线图\n* 折线图\n* 饼图\n\n## 数据预处理\n\n### 缺失值、离群值\n\n`缺失值`：\n\n* 删除：缺失样本非常大(>75%)，则删除整条数据\n* 填充：缺失量<10%\n    * 若为正态分布，取均值\n    * 若为偏态，取中位数\n* 预测样本值：使用与缺失值相比相关性非常高的特征建立模型，预测缺失值\n\n`离群点`：远离数据主要部分的样本(极大值或极小值)\n\n* 同单因子探索分析的异常值处理相同：删除或填充\n\n### 标准化、纠偏\n\n`标准化`：去除数量纲(单位)的影响，提高模型的解释度，加快模型的收敛速度。具体方法如下：\n\n* 中心化：减去均再除以标准差(之后均值为0，标准差为1)\n* 01标准化：减去最小值再除以最大值和最小值的差\n\n`纠偏`\n\n* 正态分布：数据呈现对称的钟态分布\n* 右偏态：样本大量集中在均值的左边(均值偏到了右边)\n* 左偏态：样本大量集中在均值的右边(均值偏到了左边)\n* 处理方法：\n    * 右偏态：常用对数函数处理\n    * 左偏态：常用指数函数处理\n* 通用变换方法：以降低数据的偏态系数为目的，使得数据分布更加接近正态分布的变换方法。\n    * yeo-johnson 变换：可以处理包含正数、负数和零的变量\n    * box-cox 变换：只能处理数值皆为正的变量\n\n### 特征工程：共线性、将维、扩展\n\n`共线性`\n\n* 特征间共线性：两个或多个特征包含了相似的信息，相互之间存在强烈的相关关系。\n* 常用的判断标准：两个或两个以上的特征之间的相关系数高于0.8\n* 共线性的影响：降低运算效率；降低一些模型的稳定性；弱化一些模型的预测能力\n* 处理办法：\n    * 删除：一组相互共线的特征中只保留与因变量相关性最高的一个\n    * 变换：对共线的两列特征进行求比值、求差值等计算\n\n### 特征工程\n\n`数据降维和特征提取`  \n目的：降低不相关特征对模型准确性的干扰，降低模型的复杂度，提高模型的泛化能力，减少模型特征，提高模型训练和预测数据  \n处理办法：\n\n* 基于数据的理解，直接删除\n* 使用主成分分析法(PCA)对特征进行提取\n* 使用机器学习模型对特征进行筛选\n\n`特征扩展`  \n目的：解决模型欠拟合，捕捉自变量和因变量之间的非线性关系  \n常用方法：多项式扩展。举例如下：\n\n* 假设数据集中包含自变量a、b\n* 如果对自变量做多项式二次扩展\n* 自变量集从两个变量扩展为5个变量(a、b、a_a、b_b、a*b)\n\n## 数据挖掘建模\n\n### 数据集的划分方法\n\n训练集：用来训练和拟合模型  \n验证集：当通过训练集训练出多个模型后，使用验证集数据纠偏或比较预测  \n测试集：模型泛化能力的考量。  \n注：有时候数据集只划分两类，将验证集和测试机视为同一个数据集。\n\n`数据集划分的基本原则`\n\n* 保持训练集和验证集之间的互斥性\n* 测试样本尽量不在训练样本中出现，以保证验证集上的表现能代表模型的泛化能力\n\n`数据集划分方法`\n\n* `留出法`：直接将数据集划分成两个互斥的集合，一个做训练集，一个做验证集。常用的划分比例：7:3、7.5:2.5、8:2。若划分三类：6:2:2\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.model_selection import train_test_split\n```\n\n* `交叉验证法`：将数据集划分成k个大小相似的互斥子集，每次把k-1个子集做训练，1个子集做验证，训练k次，最终返回k在、次训练结果的均值。因此交叉验证法又被称为k次交叉法(k-fold)。\n\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.model_selection import KFold\n```\n\n### 传统机器学习算法\n\n传统机器学习算法根据样本集有无标注、是否部分有标注分为三类：监督学习、无监督学习、半监督学习。\n\n讲算法之前先说明几个概念：  \n`集成学习`：是指组合多个模型、有效提高模型泛化能力的学习策略。  \n\n| 基础概念 | 适用条件 |  \n| – | – |  \n| 弱可学习 | 多项式学习算法的效果不很明显 |  \n| 强可学习 | 多项式学习算法的效果较为明显 |  \n\n弱可学习可通过集成方法称为强可学习。  \n集成方法分类：\n\n* `袋装法`(bagging)：指将训练集分别用不同的模型进行训练，这些模型相互独立，然后将结果进行投票取均值的方法。如随机森林。\n* `提升法`(boost)：指训练集用一种模型训练出的结果作为另一个模型的输入，然后将其输出再作为其他模型的输入，如此反复。最后把这些模型进行加权叠加作为最终输出。如Adaboost、XGBoost。\n    * 注意这种方式中，子模型对最终结果的影响更大程度上取决于权值，而不是顺序。\n\n#### 监督学习\n\n适用于样本集有标注的情况。\n\n监督学习模型分为`分类`和`回归`两类。\n\n* 分类适用于标注(标签)是离散的情况\n* 回归适用于标注是连续数值的情况\n* 分类和回归没有绝对的划分界限，分类可视为一种有限的回归；回归可视为一种无限定序数据的分类。有些算法都可运用在这两类模型上。\n\n监督学习使用场景举例：图形识别、房价预测、银行信用评估等\n\n分类常用模型：KNN、朴素贝叶斯、Adaboost、随机森林  \n回归常用模型：线性回归、回归树和提升树  \n可同时用于分类和回归的模型：决策树、支持向量机(SVM)、Logistic模型、人工神经网络\n\n##### 分类\n\n具体的，分类模型也可以分为两种类型：生成模型、判别模型。\n\n* `生成模型`：通过求输入输出的联合概率分布，在求解类别归类的概率。如朴素贝叶斯\n* `判别模型`：不通过求联合概率分布，直接可以获得输出最大分类的概率。如KNN\n\n两者的区别：判别模型较生成模型，对数据的要求低一点，对数据的容忍度大一些，速度相对慢一些，适用范围更广一些。\n\n###### KNN\n\nKNN：K-Nearest Neighbors，最邻近结点算法。  \n算法思想：每个样本都可以用它最接近的K个邻近值来代表。  \n适用条件：用于标注在空间隔离性较好的情况。\n\n基础知识：\n\n* 欧式距离\n* 曼哈顿距离\n* 闵可夫斯基距离\n* KD-Tree：点作为叶子节点，线作为分枝节点\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.neighbors import KNeighborsClassifier  \nknn_clf= KNeighborsClassifier(n_neighbors=5) //最近5个点  \nknn_clf.fit(X_train,Y_train)  \nY_pred = knn_clf.predict(X_validation)\n```\n\n###### 朴素贝叶斯\n\n朴素：特征间相互独立。  \n算法思想：先通过已给定的训练集，以特征之间独立作为前提假设，学习从输入到输出的联合概率分布，再基于学习到的模型，输入 X 求出使得后验概率最大的输出 Y 。  \n适用条件：特征最好是离散的。\n\n基础知识：\n\n* 概率\n* 条件概率\n* 联合概率\n* 全概率公式\n* 贝叶斯公式\n* 拉普拉斯平滑：若条件概率为0，导致整个式子为0，则在所有值都加1.\n\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.naive_bayes import GaussianNB,BernoulliNB  \n//高斯模贝叶斯，假设特征是高斯分布  \nGaussianNB( ).fit( ).predict( )  \n//伯努利贝叶斯，适用于离散值是二值的情况  \nBernoulliNB( ).fit( ).predict( )\n```\n\n###### 决策树\n\n算法思想：  \n特征判别先后顺序的依据或评价手段：\n\n* 信息增益-ID3：适用于离散值较多的分类问题\n    * 值越大，该特征越先比较\n* 信息增益率-C4.5：适用于离散值较多的分类问题\n    * 考虑到了熵本身值大小的影响\n* Gini系数-CART：不纯度。适用于连续值分类问题\n    * 不纯度值最低的作当前区分\n\n注意事项：\n\n* 连续值切分方法同探索性分析的离散化方法\n* 规则用尽则投票，哪个样本多投哪个\n* 若过拟合，需要修建枝叶：\n    * 前剪枝：构造决策树之前规定每个叶子节点最多有多少个样本，或规定决策树的最大深度\n    * 后剪枝：先构造决策树，然后对样本值比较悬殊的枝叶进行修剪\n* 若想生成图示，需下载app：Graphviz\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.tree import DecisionTreeClassifier  \n//默认采用Gini系数(不纯度)  \nDecisionTreeClassifier( ).fit( ).predict( )  \n//使用信息增益(ID3)  \nDecisionTreeClassifier(criterion=\"entropy\").fit( ).predict( )\n```\n\n###### 支持向量机(SVM)\n\nSVM: Support Vector Machine\n\n基础概念：\n\n* 高维面\n* 分界面\n* 拉格朗日乘数法\n\n注意事项：\n\n* 若一些计算结果为无穷大，可容忍部分错误的分类，转换求 min(max(L)) ;也可利用 KKT 条件，求 max(min(L))\n* 如需扩维，有两种方式：\n    * 线映射，在计算。这样容易造成维度灾难。\n    * 先在低维空间计算，在利用核函数扩维\n        * 常见核函数：线性核函数、多项式核函数、高斯径向基(RBF)核函数\n* 若存在少部分异常，可松弛变量，即为了达到更宽的分界线，允许存在少量错分点\n* 若样本不平衡，根据实际业务场景定\n* 对于多分类问题：\n    * One-Other：有几个分类建几个 SVM ，分成一个分类和其他分类\n    * One-One：分类的两两之间分别建立 SVN\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.svm import SVC  \nSVC(c=100000).fit( ).predict( ) //c为分类精度，值越大运行时间越长\n```\n\n###### Adaboost\n\nAdaboost：集成方法中提升法的运用。\n\n特点：精度高，灵活可调，几乎不用担心过拟合，简化特征工程流程\n\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.ensemble import AdaboostClassifier  \nAdaboostClassifier().fit( ).predict( )\n```\n\n\n###### 随机森林\n\n随机森林：集成方法中袋装法的运用。由多个决策树集成。\n\n基本概念：\n\n* 树的个数\n    * 考虑到的样本的局部性的可能情况越多，越容易过拟合\n    * 树的数量与样本数量、特征数量都有关系，不断的尝试后确定\n* 树的特征数\n    * 特征少时每棵树用全部特征，特征多时每棵树用部分特征\n    * 可增加树的数量和并行计算的能力来平衡特征减少可能带来的损失\n* 树的训练集\n    * 每棵树的训练集都是模型训练集的一个子集\n    * 选取子集的方法有两种：\n        * 训练子集和模型训练集数量一样，采用有放回的抽样构成样本差异性\n        * 每棵树都用全部样本，通过缩减特征的规模构成样本的差异性\n\n注意事项：\n\n* 每个决策树可以不使用全部特征，减少规模和复杂度\n* 不需要剪枝，即可有效避免过拟合\n\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.ensemble import RandomForestClassifier  \nRandomForestClassifier().fit( ).predict( )\n```\n\n##### 回归\n\n###### 线性回归\n\n`一元线性回归`  \n适用条件：适用于线性可分的场景\n\n基本概念：\n\n* 损失函数\n* 参数优化目标\n* 最小二乘法\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.linear_model import LinearRegression  \n//线性回归  \nLinearRegression().fit( ).predict( )\n```\n\n`多元线性回归`\n\n基本概念：\n\n* 损失函数\n* 优化目标\n* 矩阵求解\n* 惩罚(正则化):通常在模型损失函数中增加一个正则项(惩罚项)来控制模型的复杂度。有两类惩罚项：\n    * L1正则系数：Lasso回归\n    * L2正则系数：ridge回归(岭回归)\n\n求解方法：`梯度下降法`  \n一种无约束多元函数极值求解方法，通过迭代得到最小化的损失函数所对应的模型参数。  \n基本思路：在求解目标函数 E(a) 的最小值时，a 沿梯度下降的方向不断变化求解最小值。\n\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.linear_model import Ridge,Lasso  \n//岭回归  \nRidge(alpha = 5).fit( ).predict( ) //alpha默认为0  \n//Lasso回归  \nLasso(alpha = ).fit( ).predict( )\n```\n\n\n###### Logistic回归\n\n基本概念：\n\n* 激活函数\n* 损失函数：对数似然损失函数\n* 梯度下降\n\n注意事项：同线性回归，也是求最小值，也可用梯度下降方法求解\n\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.linear_model import LogisticRegression  \nLogisticRegression( ).fit( ).predict( ) //alpha默认为0\n```\n\n\n###### 人工神经网络\n\n适用条件：适用于各种非线性映射。\n\n基本概念：\n\n* 感知器：处理线性映射关系\n* 感知器并联\n* 神经网络：\n    * 输入层：数据必须归一化；\n    * 隐含层；\n    * 输出层：必须是 one-hot 格式\n\n求解方法：求解所有参数\n\n* 梯度下降算法：参数多，很复杂\n* 反向传播算法(PyBrain)\n    * 前向计算\n    * 计算误差\n    * 反向单层调整\n    * 传播\n    * 不断迭代，直到输出收敛到误差范围内或迭代固定次数\n* 随机梯度下降算法(SGD,stochastic Gradient Decent)\n    * 每次调整权值时，选取部分样本进行梯度下降\n    * 优点是收敛更快，计算开销小；缺点是容易陷入局部最优解。\n    * 使用范围广\n\n注意事项：\n\n* 人工神经网络的深度加深就形成深度神经网络。\n* 算法易受离群点影响，易过拟合。解决办法有两种：\n    * 正则化\n    * dropout：每次随机选取部分节点，组成多个神经网络模型，将多个结果投票选出得票最多的模型取其值；对于回归模型取其均值。类似与集成方法。\n* 属性特征和结果要在0-1之间，且结果是 one-hot 形式\n* 输出结果进行 softmax 转化，确保其和为1\n\n\n```\n//python中没有神经网络的包，需要手写。  \n//步骤一：安装keras  \npip install tensorflow  \nconda install pip //仅window需要这一步，且已安装 Anconoda  \npip install keras  \n//步骤二：python中调用keras  \nfrom keras.models import Sequential // 类似容器  \nfrom keras.models import Dense,Activation //神经网络层，激活函数  \nfrom keras.models import SGD //随机梯度下降算法\n```\n\n###### 回归树和提升树\n\n回归树：\n\n* 与分类树(决策树)的区别：\n    * 分类树中只需叶子结点有分类的判断值\n    * 回归树中每个节点都有一个预测值，一般来说预测值是连续标注的平均值\n* 回归树的切分方法：\n    * 切分后两部分的方差和最小。其中一个特征可以使用多次，直到满足回归树的停止条件。\n* 回归树的停止条件有两种：\n    * 剪枝的限制：\n        * 树的最大深度\n        * 叶子的最大样本数量\n        * …\n    * 最小方差值\n* 回归树最终取叶子节点的平均值作为预测值。\n\n提升树：由多棵回归树集成。其中最佳的一种提升树是 `GBDT` (Gradient boosting Decision)梯度提升决策树\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.ensemble import GradientBoostingClassifier  \n//最佳提升树：梯度提升决策树  \nGradientBoostingClassifier(max_depth = 6,n_estimators = 100 ).fit( ).predict( ) //100棵树，每棵树深度为6\n```\n\n#### 非监督学习\n\n将集合分成有类似的对象组成的多个类的过程，适用于样本集无标注的情况。\n\n非监督学习模型分为`聚类`和`关联`。\n\n* 分类适用于标注(标签)是离散的情况\n* 回归适用于标注是连续数值的情况\n* 分类和回归没有绝对的划分界限，分类可视为一种有限的回归；回归可视为一种无限定序数据的分类。有些算法都可运用在这两类模型上。\n\n非监督学习使用场景举例：App客群分类、词向量转化等\n\n聚类常用模型：基于切割的k-means、基于层次的聚类、基于密度的DBSCAN、基于图的split  \n关联常用模型：Apriori、Apriori-All\n\n##### 聚类\n\n###### k-means\n\n算法思想：所有类都有一个中心，属于一个类的点到它的中心的距离相比于其他类的中心更近。中心是指质心，距离常用欧式距离。\n\n实现步骤：\n\n* 从n个样本中随机选取k个作为初始化的质心\n* 对每个样本测量其到每个质心的距离，并把它归到最近的质心的类\n* 重新计算已经得到的各个类的质心\n* 迭代第二、三步，直至新的质心与原质心相等或小于阈值，算法结束\n\n注意事项：\n\n* 初始质心位置可能回影响最终结果\n    * 多试几次，取最稳定的结果\n* 个别离群值会影响整体聚类效果\n    * 将取质心换成取中心(k-medoids)。k-medoids 中点与其他同类点的距离和最小\n* 必须指定k值\n    * 其他衡量因子辅助，如轮廓系数、最小误差…\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.cluster import KMeans  \nKMeans(n_clusters = 2).fit(X) //分成两类  \nKMeans.labels_.astype(np.int) //将聚类后得到的labels转换成int格式  \nplt.scatter( ) //画散点图\n```\n\n###### 层次聚类\n\n算法思想：相近的点尽可能接近。把相近的点视为一个簇，根据分类个数不断迭代。\n\n距离衡量指标：\n\n* ward距离\n* 平方残差和：值越小，两个簇越可以合成一个簇\n\n注意事项：\n\n* 层次聚类灵活，但是计算复杂度比较高，离群点影响比较大\n\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.cluster import AgglomerativeClustering  \nAgglomerativeClustering(n_clusters = 2,linkage=\"ward\") //分成两类\n```\n\n\n###### DBSCAN\n\n算法思想：一定区域内，密度达到一定程度才是一个类，否则是离群点。\n\n基本概念：\n\n* E邻域：给定对象半径为 E 内的区域称为该对象的 E邻域\n* 核心对象：如果给定对象 E邻域内的样本点大于或等于 MinPts，则称该对象为核心对象\n* 直接密度可达：对于样本集合 D，如果样本点 q在 p的E邻域内，并且 p为核心对象，那么对象 q从对象 p直接密度可达。\n* 密度可达：对于样本集合 D，给定一串样本点 p1、p2、…、pn, p = p1, q = pn,假设对象 pi从 pi-1直接密度可达，那么对象 q从对象 p密度可达\n* 密度相连：存在样本集合 D中的一点 o，如果对象 o到对象 p和对象 q都是密度可达的，那么 p和 q密度相连\n\n注意事项：\n\n* DBSCAN算法就是找到密度相连对象的最大集合\n* DBSCAN算法优点：对离群点不敏感\n* 缺点：\n    * 计算相邻两个点之间的点不容易\n        * 借助 KD-Tree等数据结构的辅助\n    * 需要指定两个参数：E 、 MinPts\n        * 多尝试\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.cluster import DBSCAN  \nDBSCAN(min_samples = 3,eps= 5) //min_samples：最小点数；eps:E邻域\n```\n\n\n###### 图分裂\n\n实现步骤：\n\n* 根据坐标点位置距离关系形成连通图(可采用DBSCAN等算法思路找到最大范围的点数，然后用边连接起来)\n* 将形成的多个连通图进行逐一分裂\n\n分类的依据：\n\n* 承受系数t\n* 分裂阈值$\\\\lambda$\n* 若 t > $\\\\lambda$,则将该组边切分\n\n注意事项：\n\n* 与基于层次的聚类思路相反，是从顶至下\n* 图建立方式、分裂方式非常灵活\n\n##### 关联\n\n基本概念：\n\n* 项目：一个字段，对交易来说一般是指一次交易中的一个物品，如：尿布\n* 事物：某个客户在一次交易中，发生的所有项目的集合，如：{尿布，啤酒}\n* 项集：包含若干个项目的集合(一次事务中)\n* 频繁项集：某个项集的支持度大于设定阈值(人为设定或者根据数据分布出经验来定)，即称这个项集为频繁项集\n* 支持度：项集{x, y}在总项集中出现的频率(support)\n* 置信度：在先决条件 x发生的条件下，有关联规则{x -> y}推出 y 的概率(Confidence)\n* 提升度：表示含有 x的条件下同时含有 y的概率，与无论含不含 x都含有 y的概率之比。(Confidence({x} -> {y}) / support({y}))。若提升度大于1，则表示购买 x对购买 y有提升作用。\n\n注意事项：\n\n* 两个频繁项集组成的项集不一定是频繁项集\n* 两个非频繁项集组成的项集一定不是频繁项集\n* 一个频繁项集和一个非频繁项集组成的项集一定不是频繁项集\n\n关联可分为两类：\n\n* 关联规则：反映一个事物与其他事物之间的相互依存性和关联性，如Apriori算法\n* 序列规则：与关联规则相似。不同的是将时间因素考虑进来，剔除关联规则中时间点靠后的项对时间点靠前的项的支持，如Apriori-All算法\n\n###### Apriori\n\n算法思想：先指定支持度的阈值，若一个项集的支持度大于这个阈值，则称其为频繁集，然后找出频繁项集。\n\n找出频繁集的方法：\n\n* 先找出一频繁项集，去掉一非频繁项集\n* 然后将一频繁项集组成二频繁项集，根据阈值去掉二非频繁项集\n* 再将一频繁项集和二频繁项集组成三频繁项集，根据阈值去掉三非频繁项集\n* 以此类推\n* 直至找出最高阶的频繁项集，所有组合遍历完毕，整理全部项集\n\n###### Apriori-All\n\n适用场景：预测用户在购买某种东西后，下次购买时还会买其他什么东西作为搭配\n\n实现步骤：\n\n* Forward: Apriori\n* Backward: 去掉时间序列之后的项对之前的项的支持\n\n注意事项：\n\n* sklearn中不支持序列规则，自己写\n\n#### 半监督学习\n\n适用于样本集部分有标注，部分无标注的情况。通常无标注样本数量远大于有标注样本的数量。\n\n原因：获取标注的成本较大；无标注样本可能很容易获得。\n\n算法思路：\n\n* 生成模型思路：先对所有有标注的样本计算出一个分布，然后判别无标注的样本如何标注。也可采用分批迭代的方式，如先将与有标注样本比较近的样本进行标注，然后调整分布，在标注接下来的样本。\n* 判别模型的思路。也就是指物以类聚，如标签传播算法。\n\n常用算法：标签传播算法\n\n##### 标签传播算法\n\n算法思想：根据没有标注的样本和周围有标注的样本进行相似度比较，相似度高的将其标注为临近的标注。  \n其中传播是指迭代由近及远的过程。相似度判别方法：KNN、RBF等\n\n```\n//python实现，参数自行查看sklearn官网  \nfrom sklearn.semi_supervised import LabelPropagation  \nLabelPropagation( ).fit( ).predict( )\n```\n\n\n#### 深度学习\n\n暂略\n\n* * *\n\n## 模型评估方法\n\npython中用法：\n\n```\nfrom sklearn.metrics import accuracy_score,recall_score,f1_score  \n//准确率  \nprint(\"ACC\",accuracy_score(Y_validation,Ypred))  \n//召回率  \nprint(\"REC\",recall_score(Y_validation,Ypred))  \n//F值  \nprint(\"F_score\",f1_score(Y_validation,Ypred))\n```\n\n### 分类模型的常用评价指标\n\n#### 二分类评价指标\n\n分正类(1)和负类(0)  \n`基本指标`\n\n* 误差率：错分类样本占总体样本的比例\n* 准确率(正确率)(Accuracy Rate)：正确分类样本占总体样本的比例\n\n`混淆矩阵`  \n| 真实情况 | 预测为正例 | 预测为负例  \n| —- | —- | —- |  \n| 正例 | TP(真正例) | FN(假反例) 漏  \n| 负例 | FP(假正例) 错 | TN(真反例)\n\n`衍生指标`\n\n* 查准率(precision):所有真正例占所有预测为正的样本的比例\n* 查全率(招回率,Recall,TPR):所有真正例占所有真实为正的样本的比例\n* F-measure(F-score):\n\n```\n2 * Recall * Accuracy / (Reacll + Accuracy)\n```\n\n\n* 错误接收率(FPR,False Postive Rate):\n\n```\nFP / (FP + TN)\n```\n\n* 错误拒绝率(FRR,False Rejction Rate):\n\n```\nFN / (TP + FN)\n```\n\n#### 多分类评价指标\n\n* 多元混淆矩阵\n* 准确率：同二分类\n* 召回率与F值：两种思路处理：\n    * 先计算所有的TP、FN等值，再以二值方法计算\n    * 分别把每个分类当作正类，各计算一个召回率或F值，然后取加权或不加权的平均值\n* ROC曲线与AUC值：衡量分类效果，并且可以限定阈值\n    * ROC曲线：以召回率(TPR)为纵轴，错误接收率(FPR)为横轴，采用不同的截断点，绘制ROC曲线。ROC曲线能够很容易地查出任意界限值对性能的识别能力。\n    * AUC值：ROC曲线与坐标轴构成的图形面积。AUC值越接近1，说明越准确。\n* 增益图和KS图：衡量分类效果\n    * 增益图：宏观上反映分类器的效果\n    * KS图：反映对正类样本份额例的\n\n### 回归模型的常用评价指标\n\n* 样本误差：衡量模型在一个样本上的预测准确度\n    * 样本误差 = 样本预测值 - 样本实际值\n* 平均误差方(MSE)：最常用的评价指标\n    * 所有样本的样本误差的平方和的均值。MSE越接近0，模型越准确。\n* 平均绝对误差(MAE)：较好解释的评价指标\n    * 所有样本的样本误差的绝对值的均值。MAE的单位与因变量的单位一致，其越接近0，模型越准确。\n* 平均绝对比例误差(MAPE)：平均绝对误差的衍生指标\n    * 所有样本的样本误差的绝对值占实际值的比例。指标越接近0，模型越准确。\n* 决定系数： R2-score\n    * 因变量的方差能被自变量解释的程度。指标越接近1，则代表自变量对于因变量的解释程度越高。通常 >0.5 ，就还不错。\n\n### 聚类模型的常用评价指标\n\n* RMS(Root Mean Square)：值越小，分类效果越好  \n    $$ RMS = \\\\frac{1}{n} \\\\sqrt{\\\\sum_{i=0}^{n} (x_i - \\\\bar{x})^{2}}$$\n* 轮廓系数\n    * a(i)为样本i与簇内其他样本的平均距离，也称为内聚度\n    * b(i)为样本i与其他某簇样本的平均距离，也称为分离度\n\n```\n//s(i)越接近1，分类效果越好；越接近-1，分类效果最差  \ns(i) = (b(i) - a(i)) / max{a(i),b(i)}\n```\n\n\n## 关联模型的常用评价指标\n\n* 支持度：项集{x, y}在总项集中出现的频率(support)\n* 置信度：在先决条件 x发生的条件下，有关联规则{x -> y}推出 y 的概率(Confidence)\n* 提升度：表示含有 x的条件下同时含有 y的概率，与无论含不含 x都含有 y的概率之比。(Confidence({x} -> {y}) / support({y}))。若提升度大于1，则表示购买 x对购买 y有提升作用。\n\n## 其他\n\npython中保存和加载模型的方式：\n\n```\nfrom sklearn.externals import joblib  \n//保存模型  \njoblib.dump(knn_clf,\"knn_clf\")  \n//加载模型  \njoblib.load(knn_clf,\"knn_clf\")\n```\n\n## 后续\n\n> 注：本文持续更新中","source":"_posts/data-science.md","raw":"---\ntitle: 数据科学家养成计划手册(持续更新中)\ndate: 2021-01-02 13:58:55\ntags: data science\ncategories: 数据科学\n---\n\n\n本人是一名数据爱好者，梦想是成为一名数据科学家。这里记录我从零开始的学习过程。内容若有什么不对的，欢迎大家批评指正，也希望能和感兴趣的读者一起探讨。\n\n> 注1：数据科学家有两条发展路径，一条是偏向业务可视化的商业分析师（数据分析师），一条是偏向于建模算法的数据挖掘工程师。这里不予严格区分。本文旨在展示数据获取、处理、建模、分析、可视化等全过程。读者可根据自己的喜好选择一个方向深入。\n\n> 注2：本文长期维护\n\n## 数据分析全貌\n\n数据分析是一门利用统计学知识，从数据中提取有用信息，进行总结和概括的学科。\n\n一名优秀的数据分析师应该具备的能力：`好奇`、`谨慎`、`责任`。\n\n在实际的工作中，每个分析师应该有自己处理问题的工作流程，并在实际的工作中不断的完善和迭代。最基本的流程如下所示：  \n`数据工作流`：抛出问题 -> 获取数据 -> 数据研究 -> 问题结论 -> 解决办法\n\n`数据建模和挖掘具体流程`：数据获取 -> 探索性分析及可视化 -> 数据预处理 -> 数据挖掘建模 -> 模型评估\n\n前两个流程（数据获取、探索性分析与可视化）也是狭义的数据分析。\n\n`前修知识`：数学(高数、概率论与数理统计)、统计学、python(或者 R)；（sql、excel 本文暂不探讨）\n\n涉及的python第三方库：\n\n* 数据科学包：Numpy(数据基础包)、Pnadas(数据处理神器)、Scipy(科学计算包)、Matplotlib(可视化)、Seaborn(可视化)\n* 机器学习包：Scikit-learn(传统机器学习)、TensorFlow(深度学习)\n\n`工具`：Visual Studio Code(Jupyter插件)\n\n## 数据获取\n\n### 数据仓库\n\n`定义`：将所有的业务数据汇总处理，构成数据仓库(DW)\n\n* 全面、完备、尽可能详细的记录全部事实\n    * 如某人几分几秒浏览了什么页面\n* 方便对部分维度和数据的整理(数据集市-DM)\n\n`数据仓库与文件和日志的区别`：文件、日志只能顺序记录，不方便查找、比较、抽取特征等操作\n\n`数据仓库与数据库的区别`：\n\n* 数据仓库：\n    * 面向主题存储\n        * 主题：较高层次上对分析对象数据的一个完整并且一致的描述\n        * 如购买图书这个行为就是个主题，在几时几分几秒以什么样的价格购买了什么样的书就是这个购买主题的一个记录，记录里有时间有用户信息，有图书信息等各个维度的信息。主题就是各个数据相互联系的描述。\n    * 针对分析(OLAP)\n    * 可能冗余、相对变化较大、数据量大\n* 数据库\n    * 面向业务存储\n        * 高并发、快速读写、数据结构精简\n    * 针对应用(OLTP)\n        * 为用户提供数据上的支持和服务\n    * 组织规范\n\n### 检测与抓取(爬虫)\n\n`检测`：用检测设备和检测算法直接获取数据。如传感器网络\n\n`抓取(爬虫)`：直接通过获取网页内容进行解析和分析，直接解析网页、接口和文件的信息。\n\n* python常用库：\n    * 抓取：urllib、urllib2、requests、scrap\n    * 渲染：PhantomJS\n    * 解析：beautifulSoup、Xpath(lxml)\n\n### 填写、日志、埋点\n\n`用户填写`：用户填写的信息。如调查问卷、用户注册时填写的信息\n\n`操作日志`：以文件形式记录。\n\n* 前端日志：网页和APP中记录的日志\n* 后端日志：服务器的日志\n\n`埋点`：在APP或网页应用中针对特定的流程收集一定的信息，用来跟踪APP或网页被使用的情况，以便后继用来进一步优化产品或进行运营支持。\n\n* 比较常用的记录项：访问、访客、停留时间、页面查看和跳出率。\n* 分为页面统计和统计操作行为。\n* 可自己开发也可选择第三方工具，如友盟。\n\n### 计算\n\n`计算`：通过已有的数据计算生成衍生数据。如企业的投入产出比\n\n### 数据学习网站\n\n`数据竞赛网站`：kaggle & 天池  \n`图片数据集网站`：ImageNet、Open Images  \n`各领域统计数据`：统计局、政府机构、公司财报等\n\n## 探索性分析与数据可视化\n\n### 统计分析方法基础\n\n`集中趋势`：均值、中位数与分位数、众数  \n四分位数的计算方法：\n\n```\n# n为数据的个数  \nQ1位置 = (n+1)*25  \nQ2位置 = (n+1)*5  \nQ3位置 = (n+1)*75\n```\n\n`离中趋势`：标准差、方差\n\n`数据分布`：偏态与峰态、正态分布与三大分布\n\n* 偏态系数：数据平均值偏离状态的一种衡量\n* 峰态系数：数据分布集中强度的衡量\n* 分布概率\n    * 正态分布\n    * 卡方分布: 标准正态分布的平方和\n    * $T$分布：用于根据小样本来估计呈正态分布且方差未知的总体的均值\n    * $F$分布\n\n`抽样理论`\n\n* 抽样原因：\n    * 数据量异常大，全量计算的时间、成本都很大。(大数据发展，不再是问题)\n    * 全量检测不现实。比如测灯泡的寿命\n* 抽样类型：\n    * 重复抽样(有放回的抽样)\n    * 不重复抽样(无放回的抽样)\n* 抽样方式\n    * 完全随机抽样\n    * 等差距抽样：某个属性从低到高排列，等间距抽样\n    * 分类的分层抽样：根据各个类别的比例抽样\n* 抽样平均误差计算公式\n    * 重复抽样\n    * 不重复抽样\n* 估计总体时抽样数量的确定\n    * 重复抽样\n    * 不重复抽样\n\n`数据分类`\n\n* 定类(类别)：根据事物离散、无差别属性进行分类。如性别、名族\n* 定序(顺序)：可以界定数据的大小、但不能测定差值。如收入的高、中、低\n* 定距(间距)：可以界定数据大小同时可测定差距，但无绝对零点(乘除比率无意义)。如温度\n* 定比(比率)：可以界定数据大小同时可测定差距，有绝对零点。如身高、体重\n\n`假设检验与方差检验`  \n假设检验：做出一个假设，根据数据或已知的分布性质来推断这个假设成立的概率有多大。\n\n* 建立原假设 H0 (包含等号)，H1是H0的反命题，也称备择假设\n* 选择检验统计量\n* 根据显著性水平(一般为0.05)，确定拒绝域\n* 计算`P值`或`样本统计量`，做出判断\n\n检验统计量：\n\n* $\\\\mu$ 分布\n* 卡方分布：用于检测两个因素之间有无强联系\n* $T$检验：比较两组样本分布是否一致；两组值的均值有无较大差异\n* $F$检验（方差检验)：用于方差分析。多样本两两之间是否有差异。\n    * 总变差平方和\n    * 平均平方和/组内平方和\n    * 残差平方和/组内平方差\n    * 统计量\n\n> 注：也可以通过qq图来判断一个分布是否符合一个已知的分布，比如找到该分布的分位数做纵轴，正态分布的分位数做横轴，若连线接近角平分线，则符合\n\n`相关系数`：衡量两组数据或两组样本的分布趋势、变化趋势一致性程度的因子。\n\n* 皮尔逊(Pearson)相关系数\n* 斯皮尔(Spearman)曼相关系数\n\n`回归：线性回归`：确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法\n\n* 关键度量指标：\n    * 一元决定系数\n    * 多元决定系数\n* 残差不相关(DW检验)\n\n`PCA与奇异值分解`：尽可能少的失真情况下，线性降维，成分提取\n\n* 主成分分析法(PCA)\n\n    * 求特征协方差矩阵\n    * 求协方差矩阵的特征值和特征向量\n    * 将特征值按照从大到小的顺序排序，选取其中最大的k个\n    * 将样本点投影到选取的特征向量上\n* 奇异值分解(SVD)\n\n    * 特征矩阵 A 分解为 m_m 的酉阵，m_n 半正定矩阵(奇异矩阵)，n*n 酉阵转置 v\n\n### 探索性分析\n\n#### 单因子探索\n\n——展现数据全貌\n\n* 异常值分析(可用箱线图展示)\n    * 连续异常值\n        * 处理办法：舍去；异常值用边界值代替(四分位数)\n    * 离散异常值：离散属性定义范围外的所有值均为异常值。如空值；收入离散成高、中、低之外的值\n        * 处理办法：舍去；把所有异常值当作单独的一个值处理\n    * 常识异常值：在限定知识和常识范围外的所有值均为异常值。如身高20m\n* 对比分析\n    * 比什么：\n        * 绝对数比较：数值直接比较。如比较收入、身高\n        * 相对数比较：把几个有联系的指标联合成新的指标\n            * 结构相对数：部分与整体。如合格率、通过率\n            * 比例相对数：总体内用不同部分的数值进行比较。如三大产业相互比较\n            * 比较相对数：同一时空下相似或同质的指标进行比较。如不同时期同一产品的价格对比；不同互联网电商公司的待遇水平对比\n            * 动态相对数：一般包含时间概念。如用户增速\n            * 强度相对数：性质不同但又互相联系的属性进行联合。如人均、亩产、密度\n    * 怎么比：\n        * 时间\n            * 同比：今年2月同去年2月比\n            * 环比：今年2月同今年1月比\n        * 空间\n            * 现实方位：如不同国家、不同城市\n            * 逻辑上空间：如一家公司的不同部门；不同家公司之间的比较\n        * 经验和计划：如历史上失业率达到百分之几回发生暴乱，把国家的失业率与之比较；工作排期与实际进度之间的比较\n* 结构分析：各组成部分的分布与规律\n    * 静态：直接分析总体的组成。如十一五时间三大产业的比例\n    * 动态：时间为轴分析结构变化的趋势。如十一五期间三大产业比的变化\n* 分布分析：数据分布频率的显示分析\n    * 直接获取的概率分布\n    * 判断是否为正态分布\n        * 偏态系数\n        * 峰态系数\n    * 极大似然\n\n#### 多因子探索\n\n——探索属性与属性之间的联系\n\n* 交叉分析\n    * 热力图\n    * 透视表\n* 分组与钻取\n* 相关分析\n* 因子分析\n* 聚类分析(建模中也会用到)\n* 回归分析(建模中也会用到)\n\n### 数据可视化\n\n* 柱状图：横坐标表示离散值\n* 直方图：横坐标表示范围\n* 箱线图\n* 折线图\n* 饼图\n\n## 数据预处理\n\n### 缺失值、离群值\n\n`缺失值`：\n\n* 删除：缺失样本非常大(>75%)，则删除整条数据\n* 填充：缺失量<10%\n    * 若为正态分布，取均值\n    * 若为偏态，取中位数\n* 预测样本值：使用与缺失值相比相关性非常高的特征建立模型，预测缺失值\n\n`离群点`：远离数据主要部分的样本(极大值或极小值)\n\n* 同单因子探索分析的异常值处理相同：删除或填充\n\n### 标准化、纠偏\n\n`标准化`：去除数量纲(单位)的影响，提高模型的解释度，加快模型的收敛速度。具体方法如下：\n\n* 中心化：减去均再除以标准差(之后均值为0，标准差为1)\n* 01标准化：减去最小值再除以最大值和最小值的差\n\n`纠偏`\n\n* 正态分布：数据呈现对称的钟态分布\n* 右偏态：样本大量集中在均值的左边(均值偏到了右边)\n* 左偏态：样本大量集中在均值的右边(均值偏到了左边)\n* 处理方法：\n    * 右偏态：常用对数函数处理\n    * 左偏态：常用指数函数处理\n* 通用变换方法：以降低数据的偏态系数为目的，使得数据分布更加接近正态分布的变换方法。\n    * yeo-johnson 变换：可以处理包含正数、负数和零的变量\n    * box-cox 变换：只能处理数值皆为正的变量\n\n### 特征工程：共线性、将维、扩展\n\n`共线性`\n\n* 特征间共线性：两个或多个特征包含了相似的信息，相互之间存在强烈的相关关系。\n* 常用的判断标准：两个或两个以上的特征之间的相关系数高于0.8\n* 共线性的影响：降低运算效率；降低一些模型的稳定性；弱化一些模型的预测能力\n* 处理办法：\n    * 删除：一组相互共线的特征中只保留与因变量相关性最高的一个\n    * 变换：对共线的两列特征进行求比值、求差值等计算\n\n### 特征工程\n\n`数据降维和特征提取`  \n目的：降低不相关特征对模型准确性的干扰，降低模型的复杂度，提高模型的泛化能力，减少模型特征，提高模型训练和预测数据  \n处理办法：\n\n* 基于数据的理解，直接删除\n* 使用主成分分析法(PCA)对特征进行提取\n* 使用机器学习模型对特征进行筛选\n\n`特征扩展`  \n目的：解决模型欠拟合，捕捉自变量和因变量之间的非线性关系  \n常用方法：多项式扩展。举例如下：\n\n* 假设数据集中包含自变量a、b\n* 如果对自变量做多项式二次扩展\n* 自变量集从两个变量扩展为5个变量(a、b、a_a、b_b、a*b)\n\n## 数据挖掘建模\n\n### 数据集的划分方法\n\n训练集：用来训练和拟合模型  \n验证集：当通过训练集训练出多个模型后，使用验证集数据纠偏或比较预测  \n测试集：模型泛化能力的考量。  \n注：有时候数据集只划分两类，将验证集和测试机视为同一个数据集。\n\n`数据集划分的基本原则`\n\n* 保持训练集和验证集之间的互斥性\n* 测试样本尽量不在训练样本中出现，以保证验证集上的表现能代表模型的泛化能力\n\n`数据集划分方法`\n\n* `留出法`：直接将数据集划分成两个互斥的集合，一个做训练集，一个做验证集。常用的划分比例：7:3、7.5:2.5、8:2。若划分三类：6:2:2\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.model_selection import train_test_split\n```\n\n* `交叉验证法`：将数据集划分成k个大小相似的互斥子集，每次把k-1个子集做训练，1个子集做验证，训练k次，最终返回k在、次训练结果的均值。因此交叉验证法又被称为k次交叉法(k-fold)。\n\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.model_selection import KFold\n```\n\n### 传统机器学习算法\n\n传统机器学习算法根据样本集有无标注、是否部分有标注分为三类：监督学习、无监督学习、半监督学习。\n\n讲算法之前先说明几个概念：  \n`集成学习`：是指组合多个模型、有效提高模型泛化能力的学习策略。  \n\n| 基础概念 | 适用条件 |  \n| – | – |  \n| 弱可学习 | 多项式学习算法的效果不很明显 |  \n| 强可学习 | 多项式学习算法的效果较为明显 |  \n\n弱可学习可通过集成方法称为强可学习。  \n集成方法分类：\n\n* `袋装法`(bagging)：指将训练集分别用不同的模型进行训练，这些模型相互独立，然后将结果进行投票取均值的方法。如随机森林。\n* `提升法`(boost)：指训练集用一种模型训练出的结果作为另一个模型的输入，然后将其输出再作为其他模型的输入，如此反复。最后把这些模型进行加权叠加作为最终输出。如Adaboost、XGBoost。\n    * 注意这种方式中，子模型对最终结果的影响更大程度上取决于权值，而不是顺序。\n\n#### 监督学习\n\n适用于样本集有标注的情况。\n\n监督学习模型分为`分类`和`回归`两类。\n\n* 分类适用于标注(标签)是离散的情况\n* 回归适用于标注是连续数值的情况\n* 分类和回归没有绝对的划分界限，分类可视为一种有限的回归；回归可视为一种无限定序数据的分类。有些算法都可运用在这两类模型上。\n\n监督学习使用场景举例：图形识别、房价预测、银行信用评估等\n\n分类常用模型：KNN、朴素贝叶斯、Adaboost、随机森林  \n回归常用模型：线性回归、回归树和提升树  \n可同时用于分类和回归的模型：决策树、支持向量机(SVM)、Logistic模型、人工神经网络\n\n##### 分类\n\n具体的，分类模型也可以分为两种类型：生成模型、判别模型。\n\n* `生成模型`：通过求输入输出的联合概率分布，在求解类别归类的概率。如朴素贝叶斯\n* `判别模型`：不通过求联合概率分布，直接可以获得输出最大分类的概率。如KNN\n\n两者的区别：判别模型较生成模型，对数据的要求低一点，对数据的容忍度大一些，速度相对慢一些，适用范围更广一些。\n\n###### KNN\n\nKNN：K-Nearest Neighbors，最邻近结点算法。  \n算法思想：每个样本都可以用它最接近的K个邻近值来代表。  \n适用条件：用于标注在空间隔离性较好的情况。\n\n基础知识：\n\n* 欧式距离\n* 曼哈顿距离\n* 闵可夫斯基距离\n* KD-Tree：点作为叶子节点，线作为分枝节点\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.neighbors import KNeighborsClassifier  \nknn_clf= KNeighborsClassifier(n_neighbors=5) //最近5个点  \nknn_clf.fit(X_train,Y_train)  \nY_pred = knn_clf.predict(X_validation)\n```\n\n###### 朴素贝叶斯\n\n朴素：特征间相互独立。  \n算法思想：先通过已给定的训练集，以特征之间独立作为前提假设，学习从输入到输出的联合概率分布，再基于学习到的模型，输入 X 求出使得后验概率最大的输出 Y 。  \n适用条件：特征最好是离散的。\n\n基础知识：\n\n* 概率\n* 条件概率\n* 联合概率\n* 全概率公式\n* 贝叶斯公式\n* 拉普拉斯平滑：若条件概率为0，导致整个式子为0，则在所有值都加1.\n\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.naive_bayes import GaussianNB,BernoulliNB  \n//高斯模贝叶斯，假设特征是高斯分布  \nGaussianNB( ).fit( ).predict( )  \n//伯努利贝叶斯，适用于离散值是二值的情况  \nBernoulliNB( ).fit( ).predict( )\n```\n\n###### 决策树\n\n算法思想：  \n特征判别先后顺序的依据或评价手段：\n\n* 信息增益-ID3：适用于离散值较多的分类问题\n    * 值越大，该特征越先比较\n* 信息增益率-C4.5：适用于离散值较多的分类问题\n    * 考虑到了熵本身值大小的影响\n* Gini系数-CART：不纯度。适用于连续值分类问题\n    * 不纯度值最低的作当前区分\n\n注意事项：\n\n* 连续值切分方法同探索性分析的离散化方法\n* 规则用尽则投票，哪个样本多投哪个\n* 若过拟合，需要修建枝叶：\n    * 前剪枝：构造决策树之前规定每个叶子节点最多有多少个样本，或规定决策树的最大深度\n    * 后剪枝：先构造决策树，然后对样本值比较悬殊的枝叶进行修剪\n* 若想生成图示，需下载app：Graphviz\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.tree import DecisionTreeClassifier  \n//默认采用Gini系数(不纯度)  \nDecisionTreeClassifier( ).fit( ).predict( )  \n//使用信息增益(ID3)  \nDecisionTreeClassifier(criterion=\"entropy\").fit( ).predict( )\n```\n\n###### 支持向量机(SVM)\n\nSVM: Support Vector Machine\n\n基础概念：\n\n* 高维面\n* 分界面\n* 拉格朗日乘数法\n\n注意事项：\n\n* 若一些计算结果为无穷大，可容忍部分错误的分类，转换求 min(max(L)) ;也可利用 KKT 条件，求 max(min(L))\n* 如需扩维，有两种方式：\n    * 线映射，在计算。这样容易造成维度灾难。\n    * 先在低维空间计算，在利用核函数扩维\n        * 常见核函数：线性核函数、多项式核函数、高斯径向基(RBF)核函数\n* 若存在少部分异常，可松弛变量，即为了达到更宽的分界线，允许存在少量错分点\n* 若样本不平衡，根据实际业务场景定\n* 对于多分类问题：\n    * One-Other：有几个分类建几个 SVM ，分成一个分类和其他分类\n    * One-One：分类的两两之间分别建立 SVN\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.svm import SVC  \nSVC(c=100000).fit( ).predict( ) //c为分类精度，值越大运行时间越长\n```\n\n###### Adaboost\n\nAdaboost：集成方法中提升法的运用。\n\n特点：精度高，灵活可调，几乎不用担心过拟合，简化特征工程流程\n\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.ensemble import AdaboostClassifier  \nAdaboostClassifier().fit( ).predict( )\n```\n\n\n###### 随机森林\n\n随机森林：集成方法中袋装法的运用。由多个决策树集成。\n\n基本概念：\n\n* 树的个数\n    * 考虑到的样本的局部性的可能情况越多，越容易过拟合\n    * 树的数量与样本数量、特征数量都有关系，不断的尝试后确定\n* 树的特征数\n    * 特征少时每棵树用全部特征，特征多时每棵树用部分特征\n    * 可增加树的数量和并行计算的能力来平衡特征减少可能带来的损失\n* 树的训练集\n    * 每棵树的训练集都是模型训练集的一个子集\n    * 选取子集的方法有两种：\n        * 训练子集和模型训练集数量一样，采用有放回的抽样构成样本差异性\n        * 每棵树都用全部样本，通过缩减特征的规模构成样本的差异性\n\n注意事项：\n\n* 每个决策树可以不使用全部特征，减少规模和复杂度\n* 不需要剪枝，即可有效避免过拟合\n\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.ensemble import RandomForestClassifier  \nRandomForestClassifier().fit( ).predict( )\n```\n\n##### 回归\n\n###### 线性回归\n\n`一元线性回归`  \n适用条件：适用于线性可分的场景\n\n基本概念：\n\n* 损失函数\n* 参数优化目标\n* 最小二乘法\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.linear_model import LinearRegression  \n//线性回归  \nLinearRegression().fit( ).predict( )\n```\n\n`多元线性回归`\n\n基本概念：\n\n* 损失函数\n* 优化目标\n* 矩阵求解\n* 惩罚(正则化):通常在模型损失函数中增加一个正则项(惩罚项)来控制模型的复杂度。有两类惩罚项：\n    * L1正则系数：Lasso回归\n    * L2正则系数：ridge回归(岭回归)\n\n求解方法：`梯度下降法`  \n一种无约束多元函数极值求解方法，通过迭代得到最小化的损失函数所对应的模型参数。  \n基本思路：在求解目标函数 E(a) 的最小值时，a 沿梯度下降的方向不断变化求解最小值。\n\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.linear_model import Ridge,Lasso  \n//岭回归  \nRidge(alpha = 5).fit( ).predict( ) //alpha默认为0  \n//Lasso回归  \nLasso(alpha = ).fit( ).predict( )\n```\n\n\n###### Logistic回归\n\n基本概念：\n\n* 激活函数\n* 损失函数：对数似然损失函数\n* 梯度下降\n\n注意事项：同线性回归，也是求最小值，也可用梯度下降方法求解\n\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.linear_model import LogisticRegression  \nLogisticRegression( ).fit( ).predict( ) //alpha默认为0\n```\n\n\n###### 人工神经网络\n\n适用条件：适用于各种非线性映射。\n\n基本概念：\n\n* 感知器：处理线性映射关系\n* 感知器并联\n* 神经网络：\n    * 输入层：数据必须归一化；\n    * 隐含层；\n    * 输出层：必须是 one-hot 格式\n\n求解方法：求解所有参数\n\n* 梯度下降算法：参数多，很复杂\n* 反向传播算法(PyBrain)\n    * 前向计算\n    * 计算误差\n    * 反向单层调整\n    * 传播\n    * 不断迭代，直到输出收敛到误差范围内或迭代固定次数\n* 随机梯度下降算法(SGD,stochastic Gradient Decent)\n    * 每次调整权值时，选取部分样本进行梯度下降\n    * 优点是收敛更快，计算开销小；缺点是容易陷入局部最优解。\n    * 使用范围广\n\n注意事项：\n\n* 人工神经网络的深度加深就形成深度神经网络。\n* 算法易受离群点影响，易过拟合。解决办法有两种：\n    * 正则化\n    * dropout：每次随机选取部分节点，组成多个神经网络模型，将多个结果投票选出得票最多的模型取其值；对于回归模型取其均值。类似与集成方法。\n* 属性特征和结果要在0-1之间，且结果是 one-hot 形式\n* 输出结果进行 softmax 转化，确保其和为1\n\n\n```\n//python中没有神经网络的包，需要手写。  \n//步骤一：安装keras  \npip install tensorflow  \nconda install pip //仅window需要这一步，且已安装 Anconoda  \npip install keras  \n//步骤二：python中调用keras  \nfrom keras.models import Sequential // 类似容器  \nfrom keras.models import Dense,Activation //神经网络层，激活函数  \nfrom keras.models import SGD //随机梯度下降算法\n```\n\n###### 回归树和提升树\n\n回归树：\n\n* 与分类树(决策树)的区别：\n    * 分类树中只需叶子结点有分类的判断值\n    * 回归树中每个节点都有一个预测值，一般来说预测值是连续标注的平均值\n* 回归树的切分方法：\n    * 切分后两部分的方差和最小。其中一个特征可以使用多次，直到满足回归树的停止条件。\n* 回归树的停止条件有两种：\n    * 剪枝的限制：\n        * 树的最大深度\n        * 叶子的最大样本数量\n        * …\n    * 最小方差值\n* 回归树最终取叶子节点的平均值作为预测值。\n\n提升树：由多棵回归树集成。其中最佳的一种提升树是 `GBDT` (Gradient boosting Decision)梯度提升决策树\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.ensemble import GradientBoostingClassifier  \n//最佳提升树：梯度提升决策树  \nGradientBoostingClassifier(max_depth = 6,n_estimators = 100 ).fit( ).predict( ) //100棵树，每棵树深度为6\n```\n\n#### 非监督学习\n\n将集合分成有类似的对象组成的多个类的过程，适用于样本集无标注的情况。\n\n非监督学习模型分为`聚类`和`关联`。\n\n* 分类适用于标注(标签)是离散的情况\n* 回归适用于标注是连续数值的情况\n* 分类和回归没有绝对的划分界限，分类可视为一种有限的回归；回归可视为一种无限定序数据的分类。有些算法都可运用在这两类模型上。\n\n非监督学习使用场景举例：App客群分类、词向量转化等\n\n聚类常用模型：基于切割的k-means、基于层次的聚类、基于密度的DBSCAN、基于图的split  \n关联常用模型：Apriori、Apriori-All\n\n##### 聚类\n\n###### k-means\n\n算法思想：所有类都有一个中心，属于一个类的点到它的中心的距离相比于其他类的中心更近。中心是指质心，距离常用欧式距离。\n\n实现步骤：\n\n* 从n个样本中随机选取k个作为初始化的质心\n* 对每个样本测量其到每个质心的距离，并把它归到最近的质心的类\n* 重新计算已经得到的各个类的质心\n* 迭代第二、三步，直至新的质心与原质心相等或小于阈值，算法结束\n\n注意事项：\n\n* 初始质心位置可能回影响最终结果\n    * 多试几次，取最稳定的结果\n* 个别离群值会影响整体聚类效果\n    * 将取质心换成取中心(k-medoids)。k-medoids 中点与其他同类点的距离和最小\n* 必须指定k值\n    * 其他衡量因子辅助，如轮廓系数、最小误差…\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.cluster import KMeans  \nKMeans(n_clusters = 2).fit(X) //分成两类  \nKMeans.labels_.astype(np.int) //将聚类后得到的labels转换成int格式  \nplt.scatter( ) //画散点图\n```\n\n###### 层次聚类\n\n算法思想：相近的点尽可能接近。把相近的点视为一个簇，根据分类个数不断迭代。\n\n距离衡量指标：\n\n* ward距离\n* 平方残差和：值越小，两个簇越可以合成一个簇\n\n注意事项：\n\n* 层次聚类灵活，但是计算复杂度比较高，离群点影响比较大\n\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.cluster import AgglomerativeClustering  \nAgglomerativeClustering(n_clusters = 2,linkage=\"ward\") //分成两类\n```\n\n\n###### DBSCAN\n\n算法思想：一定区域内，密度达到一定程度才是一个类，否则是离群点。\n\n基本概念：\n\n* E邻域：给定对象半径为 E 内的区域称为该对象的 E邻域\n* 核心对象：如果给定对象 E邻域内的样本点大于或等于 MinPts，则称该对象为核心对象\n* 直接密度可达：对于样本集合 D，如果样本点 q在 p的E邻域内，并且 p为核心对象，那么对象 q从对象 p直接密度可达。\n* 密度可达：对于样本集合 D，给定一串样本点 p1、p2、…、pn, p = p1, q = pn,假设对象 pi从 pi-1直接密度可达，那么对象 q从对象 p密度可达\n* 密度相连：存在样本集合 D中的一点 o，如果对象 o到对象 p和对象 q都是密度可达的，那么 p和 q密度相连\n\n注意事项：\n\n* DBSCAN算法就是找到密度相连对象的最大集合\n* DBSCAN算法优点：对离群点不敏感\n* 缺点：\n    * 计算相邻两个点之间的点不容易\n        * 借助 KD-Tree等数据结构的辅助\n    * 需要指定两个参数：E 、 MinPts\n        * 多尝试\n\n```\n//python实现，供参考，具体见官网  \nfrom sklearn.cluster import DBSCAN  \nDBSCAN(min_samples = 3,eps= 5) //min_samples：最小点数；eps:E邻域\n```\n\n\n###### 图分裂\n\n实现步骤：\n\n* 根据坐标点位置距离关系形成连通图(可采用DBSCAN等算法思路找到最大范围的点数，然后用边连接起来)\n* 将形成的多个连通图进行逐一分裂\n\n分类的依据：\n\n* 承受系数t\n* 分裂阈值$\\\\lambda$\n* 若 t > $\\\\lambda$,则将该组边切分\n\n注意事项：\n\n* 与基于层次的聚类思路相反，是从顶至下\n* 图建立方式、分裂方式非常灵活\n\n##### 关联\n\n基本概念：\n\n* 项目：一个字段，对交易来说一般是指一次交易中的一个物品，如：尿布\n* 事物：某个客户在一次交易中，发生的所有项目的集合，如：{尿布，啤酒}\n* 项集：包含若干个项目的集合(一次事务中)\n* 频繁项集：某个项集的支持度大于设定阈值(人为设定或者根据数据分布出经验来定)，即称这个项集为频繁项集\n* 支持度：项集{x, y}在总项集中出现的频率(support)\n* 置信度：在先决条件 x发生的条件下，有关联规则{x -> y}推出 y 的概率(Confidence)\n* 提升度：表示含有 x的条件下同时含有 y的概率，与无论含不含 x都含有 y的概率之比。(Confidence({x} -> {y}) / support({y}))。若提升度大于1，则表示购买 x对购买 y有提升作用。\n\n注意事项：\n\n* 两个频繁项集组成的项集不一定是频繁项集\n* 两个非频繁项集组成的项集一定不是频繁项集\n* 一个频繁项集和一个非频繁项集组成的项集一定不是频繁项集\n\n关联可分为两类：\n\n* 关联规则：反映一个事物与其他事物之间的相互依存性和关联性，如Apriori算法\n* 序列规则：与关联规则相似。不同的是将时间因素考虑进来，剔除关联规则中时间点靠后的项对时间点靠前的项的支持，如Apriori-All算法\n\n###### Apriori\n\n算法思想：先指定支持度的阈值，若一个项集的支持度大于这个阈值，则称其为频繁集，然后找出频繁项集。\n\n找出频繁集的方法：\n\n* 先找出一频繁项集，去掉一非频繁项集\n* 然后将一频繁项集组成二频繁项集，根据阈值去掉二非频繁项集\n* 再将一频繁项集和二频繁项集组成三频繁项集，根据阈值去掉三非频繁项集\n* 以此类推\n* 直至找出最高阶的频繁项集，所有组合遍历完毕，整理全部项集\n\n###### Apriori-All\n\n适用场景：预测用户在购买某种东西后，下次购买时还会买其他什么东西作为搭配\n\n实现步骤：\n\n* Forward: Apriori\n* Backward: 去掉时间序列之后的项对之前的项的支持\n\n注意事项：\n\n* sklearn中不支持序列规则，自己写\n\n#### 半监督学习\n\n适用于样本集部分有标注，部分无标注的情况。通常无标注样本数量远大于有标注样本的数量。\n\n原因：获取标注的成本较大；无标注样本可能很容易获得。\n\n算法思路：\n\n* 生成模型思路：先对所有有标注的样本计算出一个分布，然后判别无标注的样本如何标注。也可采用分批迭代的方式，如先将与有标注样本比较近的样本进行标注，然后调整分布，在标注接下来的样本。\n* 判别模型的思路。也就是指物以类聚，如标签传播算法。\n\n常用算法：标签传播算法\n\n##### 标签传播算法\n\n算法思想：根据没有标注的样本和周围有标注的样本进行相似度比较，相似度高的将其标注为临近的标注。  \n其中传播是指迭代由近及远的过程。相似度判别方法：KNN、RBF等\n\n```\n//python实现，参数自行查看sklearn官网  \nfrom sklearn.semi_supervised import LabelPropagation  \nLabelPropagation( ).fit( ).predict( )\n```\n\n\n#### 深度学习\n\n暂略\n\n* * *\n\n## 模型评估方法\n\npython中用法：\n\n```\nfrom sklearn.metrics import accuracy_score,recall_score,f1_score  \n//准确率  \nprint(\"ACC\",accuracy_score(Y_validation,Ypred))  \n//召回率  \nprint(\"REC\",recall_score(Y_validation,Ypred))  \n//F值  \nprint(\"F_score\",f1_score(Y_validation,Ypred))\n```\n\n### 分类模型的常用评价指标\n\n#### 二分类评价指标\n\n分正类(1)和负类(0)  \n`基本指标`\n\n* 误差率：错分类样本占总体样本的比例\n* 准确率(正确率)(Accuracy Rate)：正确分类样本占总体样本的比例\n\n`混淆矩阵`  \n| 真实情况 | 预测为正例 | 预测为负例  \n| —- | —- | —- |  \n| 正例 | TP(真正例) | FN(假反例) 漏  \n| 负例 | FP(假正例) 错 | TN(真反例)\n\n`衍生指标`\n\n* 查准率(precision):所有真正例占所有预测为正的样本的比例\n* 查全率(招回率,Recall,TPR):所有真正例占所有真实为正的样本的比例\n* F-measure(F-score):\n\n```\n2 * Recall * Accuracy / (Reacll + Accuracy)\n```\n\n\n* 错误接收率(FPR,False Postive Rate):\n\n```\nFP / (FP + TN)\n```\n\n* 错误拒绝率(FRR,False Rejction Rate):\n\n```\nFN / (TP + FN)\n```\n\n#### 多分类评价指标\n\n* 多元混淆矩阵\n* 准确率：同二分类\n* 召回率与F值：两种思路处理：\n    * 先计算所有的TP、FN等值，再以二值方法计算\n    * 分别把每个分类当作正类，各计算一个召回率或F值，然后取加权或不加权的平均值\n* ROC曲线与AUC值：衡量分类效果，并且可以限定阈值\n    * ROC曲线：以召回率(TPR)为纵轴，错误接收率(FPR)为横轴，采用不同的截断点，绘制ROC曲线。ROC曲线能够很容易地查出任意界限值对性能的识别能力。\n    * AUC值：ROC曲线与坐标轴构成的图形面积。AUC值越接近1，说明越准确。\n* 增益图和KS图：衡量分类效果\n    * 增益图：宏观上反映分类器的效果\n    * KS图：反映对正类样本份额例的\n\n### 回归模型的常用评价指标\n\n* 样本误差：衡量模型在一个样本上的预测准确度\n    * 样本误差 = 样本预测值 - 样本实际值\n* 平均误差方(MSE)：最常用的评价指标\n    * 所有样本的样本误差的平方和的均值。MSE越接近0，模型越准确。\n* 平均绝对误差(MAE)：较好解释的评价指标\n    * 所有样本的样本误差的绝对值的均值。MAE的单位与因变量的单位一致，其越接近0，模型越准确。\n* 平均绝对比例误差(MAPE)：平均绝对误差的衍生指标\n    * 所有样本的样本误差的绝对值占实际值的比例。指标越接近0，模型越准确。\n* 决定系数： R2-score\n    * 因变量的方差能被自变量解释的程度。指标越接近1，则代表自变量对于因变量的解释程度越高。通常 >0.5 ，就还不错。\n\n### 聚类模型的常用评价指标\n\n* RMS(Root Mean Square)：值越小，分类效果越好  \n    $$ RMS = \\\\frac{1}{n} \\\\sqrt{\\\\sum_{i=0}^{n} (x_i - \\\\bar{x})^{2}}$$\n* 轮廓系数\n    * a(i)为样本i与簇内其他样本的平均距离，也称为内聚度\n    * b(i)为样本i与其他某簇样本的平均距离，也称为分离度\n\n```\n//s(i)越接近1，分类效果越好；越接近-1，分类效果最差  \ns(i) = (b(i) - a(i)) / max{a(i),b(i)}\n```\n\n\n## 关联模型的常用评价指标\n\n* 支持度：项集{x, y}在总项集中出现的频率(support)\n* 置信度：在先决条件 x发生的条件下，有关联规则{x -> y}推出 y 的概率(Confidence)\n* 提升度：表示含有 x的条件下同时含有 y的概率，与无论含不含 x都含有 y的概率之比。(Confidence({x} -> {y}) / support({y}))。若提升度大于1，则表示购买 x对购买 y有提升作用。\n\n## 其他\n\npython中保存和加载模型的方式：\n\n```\nfrom sklearn.externals import joblib  \n//保存模型  \njoblib.dump(knn_clf,\"knn_clf\")  \n//加载模型  \njoblib.load(knn_clf,\"knn_clf\")\n```\n\n## 后续\n\n> 注：本文持续更新中","slug":"data-science","published":1,"updated":"2022-04-23T14:46:00.434Z","_id":"cl2bubgcy0000wwulcc3nargo","comments":1,"layout":"post","photos":[],"link":"","content":"<p>本人是一名数据爱好者，梦想是成为一名数据科学家。这里记录我从零开始的学习过程。内容若有什么不对的，欢迎大家批评指正，也希望能和感兴趣的读者一起探讨。</p>\n<blockquote>\n<p>注1：数据科学家有两条发展路径，一条是偏向业务可视化的商业分析师（数据分析师），一条是偏向于建模算法的数据挖掘工程师。这里不予严格区分。本文旨在展示数据获取、处理、建模、分析、可视化等全过程。读者可根据自己的喜好选择一个方向深入。</p>\n</blockquote>\n<blockquote>\n<p>注2：本文长期维护</p>\n</blockquote>\n<h2 id=\"数据分析全貌\"><a href=\"#数据分析全貌\" class=\"headerlink\" title=\"数据分析全貌\"></a>数据分析全貌</h2><p>数据分析是一门利用统计学知识，从数据中提取有用信息，进行总结和概括的学科。</p>\n<p>一名优秀的数据分析师应该具备的能力：<code>好奇</code>、<code>谨慎</code>、<code>责任</code>。</p>\n<p>在实际的工作中，每个分析师应该有自己处理问题的工作流程，并在实际的工作中不断的完善和迭代。最基本的流程如下所示：<br><code>数据工作流</code>：抛出问题 -&gt; 获取数据 -&gt; 数据研究 -&gt; 问题结论 -&gt; 解决办法</p>\n<p><code>数据建模和挖掘具体流程</code>：数据获取 -&gt; 探索性分析及可视化 -&gt; 数据预处理 -&gt; 数据挖掘建模 -&gt; 模型评估</p>\n<p>前两个流程（数据获取、探索性分析与可视化）也是狭义的数据分析。</p>\n<p><code>前修知识</code>：数学(高数、概率论与数理统计)、统计学、python(或者 R)；（sql、excel 本文暂不探讨）</p>\n<p>涉及的python第三方库：</p>\n<ul>\n<li>数据科学包：Numpy(数据基础包)、Pnadas(数据处理神器)、Scipy(科学计算包)、Matplotlib(可视化)、Seaborn(可视化)</li>\n<li>机器学习包：Scikit-learn(传统机器学习)、TensorFlow(深度学习)</li>\n</ul>\n<p><code>工具</code>：Visual Studio Code(Jupyter插件)</p>\n<h2 id=\"数据获取\"><a href=\"#数据获取\" class=\"headerlink\" title=\"数据获取\"></a>数据获取</h2><h3 id=\"数据仓库\"><a href=\"#数据仓库\" class=\"headerlink\" title=\"数据仓库\"></a>数据仓库</h3><p><code>定义</code>：将所有的业务数据汇总处理，构成数据仓库(DW)</p>\n<ul>\n<li>全面、完备、尽可能详细的记录全部事实<ul>\n<li>如某人几分几秒浏览了什么页面</li>\n</ul>\n</li>\n<li>方便对部分维度和数据的整理(数据集市-DM)</li>\n</ul>\n<p><code>数据仓库与文件和日志的区别</code>：文件、日志只能顺序记录，不方便查找、比较、抽取特征等操作</p>\n<p><code>数据仓库与数据库的区别</code>：</p>\n<ul>\n<li>数据仓库：<ul>\n<li>面向主题存储<ul>\n<li>主题：较高层次上对分析对象数据的一个完整并且一致的描述</li>\n<li>如购买图书这个行为就是个主题，在几时几分几秒以什么样的价格购买了什么样的书就是这个购买主题的一个记录，记录里有时间有用户信息，有图书信息等各个维度的信息。主题就是各个数据相互联系的描述。</li>\n</ul>\n</li>\n<li>针对分析(OLAP)</li>\n<li>可能冗余、相对变化较大、数据量大</li>\n</ul>\n</li>\n<li>数据库<ul>\n<li>面向业务存储<ul>\n<li>高并发、快速读写、数据结构精简</li>\n</ul>\n</li>\n<li>针对应用(OLTP)<ul>\n<li>为用户提供数据上的支持和服务</li>\n</ul>\n</li>\n<li>组织规范</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"检测与抓取-爬虫\"><a href=\"#检测与抓取-爬虫\" class=\"headerlink\" title=\"检测与抓取(爬虫)\"></a>检测与抓取(爬虫)</h3><p><code>检测</code>：用检测设备和检测算法直接获取数据。如传感器网络</p>\n<p><code>抓取(爬虫)</code>：直接通过获取网页内容进行解析和分析，直接解析网页、接口和文件的信息。</p>\n<ul>\n<li>python常用库：<ul>\n<li>抓取：urllib、urllib2、requests、scrap</li>\n<li>渲染：PhantomJS</li>\n<li>解析：beautifulSoup、Xpath(lxml)</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"填写、日志、埋点\"><a href=\"#填写、日志、埋点\" class=\"headerlink\" title=\"填写、日志、埋点\"></a>填写、日志、埋点</h3><p><code>用户填写</code>：用户填写的信息。如调查问卷、用户注册时填写的信息</p>\n<p><code>操作日志</code>：以文件形式记录。</p>\n<ul>\n<li>前端日志：网页和APP中记录的日志</li>\n<li>后端日志：服务器的日志</li>\n</ul>\n<p><code>埋点</code>：在APP或网页应用中针对特定的流程收集一定的信息，用来跟踪APP或网页被使用的情况，以便后继用来进一步优化产品或进行运营支持。</p>\n<ul>\n<li>比较常用的记录项：访问、访客、停留时间、页面查看和跳出率。</li>\n<li>分为页面统计和统计操作行为。</li>\n<li>可自己开发也可选择第三方工具，如友盟。</li>\n</ul>\n<h3 id=\"计算\"><a href=\"#计算\" class=\"headerlink\" title=\"计算\"></a>计算</h3><p><code>计算</code>：通过已有的数据计算生成衍生数据。如企业的投入产出比</p>\n<h3 id=\"数据学习网站\"><a href=\"#数据学习网站\" class=\"headerlink\" title=\"数据学习网站\"></a>数据学习网站</h3><p><code>数据竞赛网站</code>：kaggle &amp; 天池<br><code>图片数据集网站</code>：ImageNet、Open Images<br><code>各领域统计数据</code>：统计局、政府机构、公司财报等</p>\n<h2 id=\"探索性分析与数据可视化\"><a href=\"#探索性分析与数据可视化\" class=\"headerlink\" title=\"探索性分析与数据可视化\"></a>探索性分析与数据可视化</h2><h3 id=\"统计分析方法基础\"><a href=\"#统计分析方法基础\" class=\"headerlink\" title=\"统计分析方法基础\"></a>统计分析方法基础</h3><p><code>集中趋势</code>：均值、中位数与分位数、众数<br>四分位数的计算方法：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># n为数据的个数  </span><br><span class=\"line\">Q1位置 &#x3D; (n+1)*25  </span><br><span class=\"line\">Q2位置 &#x3D; (n+1)*5  </span><br><span class=\"line\">Q3位置 &#x3D; (n+1)*75</span><br></pre></td></tr></table></figure>\n<p><code>离中趋势</code>：标准差、方差</p>\n<p><code>数据分布</code>：偏态与峰态、正态分布与三大分布</p>\n<ul>\n<li>偏态系数：数据平均值偏离状态的一种衡量</li>\n<li>峰态系数：数据分布集中强度的衡量</li>\n<li>分布概率<ul>\n<li>正态分布</li>\n<li>卡方分布: 标准正态分布的平方和</li>\n<li>$T$分布：用于根据小样本来估计呈正态分布且方差未知的总体的均值</li>\n<li>$F$分布</li>\n</ul>\n</li>\n</ul>\n<p><code>抽样理论</code></p>\n<ul>\n<li>抽样原因：<ul>\n<li>数据量异常大，全量计算的时间、成本都很大。(大数据发展，不再是问题)</li>\n<li>全量检测不现实。比如测灯泡的寿命</li>\n</ul>\n</li>\n<li>抽样类型：<ul>\n<li>重复抽样(有放回的抽样)</li>\n<li>不重复抽样(无放回的抽样)</li>\n</ul>\n</li>\n<li>抽样方式<ul>\n<li>完全随机抽样</li>\n<li>等差距抽样：某个属性从低到高排列，等间距抽样</li>\n<li>分类的分层抽样：根据各个类别的比例抽样</li>\n</ul>\n</li>\n<li>抽样平均误差计算公式<ul>\n<li>重复抽样</li>\n<li>不重复抽样</li>\n</ul>\n</li>\n<li>估计总体时抽样数量的确定<ul>\n<li>重复抽样</li>\n<li>不重复抽样</li>\n</ul>\n</li>\n</ul>\n<p><code>数据分类</code></p>\n<ul>\n<li>定类(类别)：根据事物离散、无差别属性进行分类。如性别、名族</li>\n<li>定序(顺序)：可以界定数据的大小、但不能测定差值。如收入的高、中、低</li>\n<li>定距(间距)：可以界定数据大小同时可测定差距，但无绝对零点(乘除比率无意义)。如温度</li>\n<li>定比(比率)：可以界定数据大小同时可测定差距，有绝对零点。如身高、体重</li>\n</ul>\n<p><code>假设检验与方差检验</code><br>假设检验：做出一个假设，根据数据或已知的分布性质来推断这个假设成立的概率有多大。</p>\n<ul>\n<li>建立原假设 H0 (包含等号)，H1是H0的反命题，也称备择假设</li>\n<li>选择检验统计量</li>\n<li>根据显著性水平(一般为0.05)，确定拒绝域</li>\n<li>计算<code>P值</code>或<code>样本统计量</code>，做出判断</li>\n</ul>\n<p>检验统计量：</p>\n<ul>\n<li>$\\mu$ 分布</li>\n<li>卡方分布：用于检测两个因素之间有无强联系</li>\n<li>$T$检验：比较两组样本分布是否一致；两组值的均值有无较大差异</li>\n<li>$F$检验（方差检验)：用于方差分析。多样本两两之间是否有差异。<ul>\n<li>总变差平方和</li>\n<li>平均平方和/组内平方和</li>\n<li>残差平方和/组内平方差</li>\n<li>统计量</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>注：也可以通过qq图来判断一个分布是否符合一个已知的分布，比如找到该分布的分位数做纵轴，正态分布的分位数做横轴，若连线接近角平分线，则符合</p>\n</blockquote>\n<p><code>相关系数</code>：衡量两组数据或两组样本的分布趋势、变化趋势一致性程度的因子。</p>\n<ul>\n<li>皮尔逊(Pearson)相关系数</li>\n<li>斯皮尔(Spearman)曼相关系数</li>\n</ul>\n<p><code>回归：线性回归</code>：确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法</p>\n<ul>\n<li>关键度量指标：<ul>\n<li>一元决定系数</li>\n<li>多元决定系数</li>\n</ul>\n</li>\n<li>残差不相关(DW检验)</li>\n</ul>\n<p><code>PCA与奇异值分解</code>：尽可能少的失真情况下，线性降维，成分提取</p>\n<ul>\n<li><p>主成分分析法(PCA)</p>\n<ul>\n<li>求特征协方差矩阵</li>\n<li>求协方差矩阵的特征值和特征向量</li>\n<li>将特征值按照从大到小的顺序排序，选取其中最大的k个</li>\n<li>将样本点投影到选取的特征向量上</li>\n</ul>\n</li>\n<li><p>奇异值分解(SVD)</p>\n<ul>\n<li>特征矩阵 A 分解为 m_m 的酉阵，m_n 半正定矩阵(奇异矩阵)，n*n 酉阵转置 v</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"探索性分析\"><a href=\"#探索性分析\" class=\"headerlink\" title=\"探索性分析\"></a>探索性分析</h3><h4 id=\"单因子探索\"><a href=\"#单因子探索\" class=\"headerlink\" title=\"单因子探索\"></a>单因子探索</h4><p>——展现数据全貌</p>\n<ul>\n<li>异常值分析(可用箱线图展示)<ul>\n<li>连续异常值<ul>\n<li>处理办法：舍去；异常值用边界值代替(四分位数)</li>\n</ul>\n</li>\n<li>离散异常值：离散属性定义范围外的所有值均为异常值。如空值；收入离散成高、中、低之外的值<ul>\n<li>处理办法：舍去；把所有异常值当作单独的一个值处理</li>\n</ul>\n</li>\n<li>常识异常值：在限定知识和常识范围外的所有值均为异常值。如身高20m</li>\n</ul>\n</li>\n<li>对比分析<ul>\n<li>比什么：<ul>\n<li>绝对数比较：数值直接比较。如比较收入、身高</li>\n<li>相对数比较：把几个有联系的指标联合成新的指标<ul>\n<li>结构相对数：部分与整体。如合格率、通过率</li>\n<li>比例相对数：总体内用不同部分的数值进行比较。如三大产业相互比较</li>\n<li>比较相对数：同一时空下相似或同质的指标进行比较。如不同时期同一产品的价格对比；不同互联网电商公司的待遇水平对比</li>\n<li>动态相对数：一般包含时间概念。如用户增速</li>\n<li>强度相对数：性质不同但又互相联系的属性进行联合。如人均、亩产、密度</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>怎么比：<ul>\n<li>时间<ul>\n<li>同比：今年2月同去年2月比</li>\n<li>环比：今年2月同今年1月比</li>\n</ul>\n</li>\n<li>空间<ul>\n<li>现实方位：如不同国家、不同城市</li>\n<li>逻辑上空间：如一家公司的不同部门；不同家公司之间的比较</li>\n</ul>\n</li>\n<li>经验和计划：如历史上失业率达到百分之几回发生暴乱，把国家的失业率与之比较；工作排期与实际进度之间的比较</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>结构分析：各组成部分的分布与规律<ul>\n<li>静态：直接分析总体的组成。如十一五时间三大产业的比例</li>\n<li>动态：时间为轴分析结构变化的趋势。如十一五期间三大产业比的变化</li>\n</ul>\n</li>\n<li>分布分析：数据分布频率的显示分析<ul>\n<li>直接获取的概率分布</li>\n<li>判断是否为正态分布<ul>\n<li>偏态系数</li>\n<li>峰态系数</li>\n</ul>\n</li>\n<li>极大似然</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"多因子探索\"><a href=\"#多因子探索\" class=\"headerlink\" title=\"多因子探索\"></a>多因子探索</h4><p>——探索属性与属性之间的联系</p>\n<ul>\n<li>交叉分析<ul>\n<li>热力图</li>\n<li>透视表</li>\n</ul>\n</li>\n<li>分组与钻取</li>\n<li>相关分析</li>\n<li>因子分析</li>\n<li>聚类分析(建模中也会用到)</li>\n<li>回归分析(建模中也会用到)</li>\n</ul>\n<h3 id=\"数据可视化\"><a href=\"#数据可视化\" class=\"headerlink\" title=\"数据可视化\"></a>数据可视化</h3><ul>\n<li>柱状图：横坐标表示离散值</li>\n<li>直方图：横坐标表示范围</li>\n<li>箱线图</li>\n<li>折线图</li>\n<li>饼图</li>\n</ul>\n<h2 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h2><h3 id=\"缺失值、离群值\"><a href=\"#缺失值、离群值\" class=\"headerlink\" title=\"缺失值、离群值\"></a>缺失值、离群值</h3><p><code>缺失值</code>：</p>\n<ul>\n<li>删除：缺失样本非常大(&gt;75%)，则删除整条数据</li>\n<li>填充：缺失量&lt;10%<ul>\n<li>若为正态分布，取均值</li>\n<li>若为偏态，取中位数</li>\n</ul>\n</li>\n<li>预测样本值：使用与缺失值相比相关性非常高的特征建立模型，预测缺失值</li>\n</ul>\n<p><code>离群点</code>：远离数据主要部分的样本(极大值或极小值)</p>\n<ul>\n<li>同单因子探索分析的异常值处理相同：删除或填充</li>\n</ul>\n<h3 id=\"标准化、纠偏\"><a href=\"#标准化、纠偏\" class=\"headerlink\" title=\"标准化、纠偏\"></a>标准化、纠偏</h3><p><code>标准化</code>：去除数量纲(单位)的影响，提高模型的解释度，加快模型的收敛速度。具体方法如下：</p>\n<ul>\n<li>中心化：减去均再除以标准差(之后均值为0，标准差为1)</li>\n<li>01标准化：减去最小值再除以最大值和最小值的差</li>\n</ul>\n<p><code>纠偏</code></p>\n<ul>\n<li>正态分布：数据呈现对称的钟态分布</li>\n<li>右偏态：样本大量集中在均值的左边(均值偏到了右边)</li>\n<li>左偏态：样本大量集中在均值的右边(均值偏到了左边)</li>\n<li>处理方法：<ul>\n<li>右偏态：常用对数函数处理</li>\n<li>左偏态：常用指数函数处理</li>\n</ul>\n</li>\n<li>通用变换方法：以降低数据的偏态系数为目的，使得数据分布更加接近正态分布的变换方法。<ul>\n<li>yeo-johnson 变换：可以处理包含正数、负数和零的变量</li>\n<li>box-cox 变换：只能处理数值皆为正的变量</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"特征工程：共线性、将维、扩展\"><a href=\"#特征工程：共线性、将维、扩展\" class=\"headerlink\" title=\"特征工程：共线性、将维、扩展\"></a>特征工程：共线性、将维、扩展</h3><p><code>共线性</code></p>\n<ul>\n<li>特征间共线性：两个或多个特征包含了相似的信息，相互之间存在强烈的相关关系。</li>\n<li>常用的判断标准：两个或两个以上的特征之间的相关系数高于0.8</li>\n<li>共线性的影响：降低运算效率；降低一些模型的稳定性；弱化一些模型的预测能力</li>\n<li>处理办法：<ul>\n<li>删除：一组相互共线的特征中只保留与因变量相关性最高的一个</li>\n<li>变换：对共线的两列特征进行求比值、求差值等计算</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"特征工程\"><a href=\"#特征工程\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h3><p><code>数据降维和特征提取</code><br>目的：降低不相关特征对模型准确性的干扰，降低模型的复杂度，提高模型的泛化能力，减少模型特征，提高模型训练和预测数据<br>处理办法：</p>\n<ul>\n<li>基于数据的理解，直接删除</li>\n<li>使用主成分分析法(PCA)对特征进行提取</li>\n<li>使用机器学习模型对特征进行筛选</li>\n</ul>\n<p><code>特征扩展</code><br>目的：解决模型欠拟合，捕捉自变量和因变量之间的非线性关系<br>常用方法：多项式扩展。举例如下：</p>\n<ul>\n<li>假设数据集中包含自变量a、b</li>\n<li>如果对自变量做多项式二次扩展</li>\n<li>自变量集从两个变量扩展为5个变量(a、b、a_a、b_b、a*b)</li>\n</ul>\n<h2 id=\"数据挖掘建模\"><a href=\"#数据挖掘建模\" class=\"headerlink\" title=\"数据挖掘建模\"></a>数据挖掘建模</h2><h3 id=\"数据集的划分方法\"><a href=\"#数据集的划分方法\" class=\"headerlink\" title=\"数据集的划分方法\"></a>数据集的划分方法</h3><p>训练集：用来训练和拟合模型<br>验证集：当通过训练集训练出多个模型后，使用验证集数据纠偏或比较预测<br>测试集：模型泛化能力的考量。<br>注：有时候数据集只划分两类，将验证集和测试机视为同一个数据集。</p>\n<p><code>数据集划分的基本原则</code></p>\n<ul>\n<li>保持训练集和验证集之间的互斥性</li>\n<li>测试样本尽量不在训练样本中出现，以保证验证集上的表现能代表模型的泛化能力</li>\n</ul>\n<p><code>数据集划分方法</code></p>\n<ul>\n<li><code>留出法</code>：直接将数据集划分成两个互斥的集合，一个做训练集，一个做验证集。常用的划分比例：7:3、7.5:2.5、8:2。若划分三类：6:2:2</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.model_selection import train_test_split</span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>交叉验证法</code>：将数据集划分成k个大小相似的互斥子集，每次把k-1个子集做训练，1个子集做验证，训练k次，最终返回k在、次训练结果的均值。因此交叉验证法又被称为k次交叉法(k-fold)。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.model_selection import KFold</span><br></pre></td></tr></table></figure>\n<h3 id=\"传统机器学习算法\"><a href=\"#传统机器学习算法\" class=\"headerlink\" title=\"传统机器学习算法\"></a>传统机器学习算法</h3><p>传统机器学习算法根据样本集有无标注、是否部分有标注分为三类：监督学习、无监督学习、半监督学习。</p>\n<p>讲算法之前先说明几个概念：<br><code>集成学习</code>：是指组合多个模型、有效提高模型泛化能力的学习策略。  </p>\n<p>| 基础概念 | 适用条件 |<br>| – | – |<br>| 弱可学习 | 多项式学习算法的效果不很明显 |<br>| 强可学习 | 多项式学习算法的效果较为明显 |  </p>\n<p>弱可学习可通过集成方法称为强可学习。<br>集成方法分类：</p>\n<ul>\n<li><code>袋装法</code>(bagging)：指将训练集分别用不同的模型进行训练，这些模型相互独立，然后将结果进行投票取均值的方法。如随机森林。</li>\n<li><code>提升法</code>(boost)：指训练集用一种模型训练出的结果作为另一个模型的输入，然后将其输出再作为其他模型的输入，如此反复。最后把这些模型进行加权叠加作为最终输出。如Adaboost、XGBoost。<ul>\n<li>注意这种方式中，子模型对最终结果的影响更大程度上取决于权值，而不是顺序。</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"监督学习\"><a href=\"#监督学习\" class=\"headerlink\" title=\"监督学习\"></a>监督学习</h4><p>适用于样本集有标注的情况。</p>\n<p>监督学习模型分为<code>分类</code>和<code>回归</code>两类。</p>\n<ul>\n<li>分类适用于标注(标签)是离散的情况</li>\n<li>回归适用于标注是连续数值的情况</li>\n<li>分类和回归没有绝对的划分界限，分类可视为一种有限的回归；回归可视为一种无限定序数据的分类。有些算法都可运用在这两类模型上。</li>\n</ul>\n<p>监督学习使用场景举例：图形识别、房价预测、银行信用评估等</p>\n<p>分类常用模型：KNN、朴素贝叶斯、Adaboost、随机森林<br>回归常用模型：线性回归、回归树和提升树<br>可同时用于分类和回归的模型：决策树、支持向量机(SVM)、Logistic模型、人工神经网络</p>\n<h5 id=\"分类\"><a href=\"#分类\" class=\"headerlink\" title=\"分类\"></a>分类</h5><p>具体的，分类模型也可以分为两种类型：生成模型、判别模型。</p>\n<ul>\n<li><code>生成模型</code>：通过求输入输出的联合概率分布，在求解类别归类的概率。如朴素贝叶斯</li>\n<li><code>判别模型</code>：不通过求联合概率分布，直接可以获得输出最大分类的概率。如KNN</li>\n</ul>\n<p>两者的区别：判别模型较生成模型，对数据的要求低一点，对数据的容忍度大一些，速度相对慢一些，适用范围更广一些。</p>\n<h6 id=\"KNN\"><a href=\"#KNN\" class=\"headerlink\" title=\"KNN\"></a>KNN</h6><p>KNN：K-Nearest Neighbors，最邻近结点算法。<br>算法思想：每个样本都可以用它最接近的K个邻近值来代表。<br>适用条件：用于标注在空间隔离性较好的情况。</p>\n<p>基础知识：</p>\n<ul>\n<li>欧式距离</li>\n<li>曼哈顿距离</li>\n<li>闵可夫斯基距离</li>\n<li>KD-Tree：点作为叶子节点，线作为分枝节点</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.neighbors import KNeighborsClassifier  </span><br><span class=\"line\">knn_clf&#x3D; KNeighborsClassifier(n_neighbors&#x3D;5) &#x2F;&#x2F;最近5个点  </span><br><span class=\"line\">knn_clf.fit(X_train,Y_train)  </span><br><span class=\"line\">Y_pred &#x3D; knn_clf.predict(X_validation)</span><br></pre></td></tr></table></figure>\n<h6 id=\"朴素贝叶斯\"><a href=\"#朴素贝叶斯\" class=\"headerlink\" title=\"朴素贝叶斯\"></a>朴素贝叶斯</h6><p>朴素：特征间相互独立。<br>算法思想：先通过已给定的训练集，以特征之间独立作为前提假设，学习从输入到输出的联合概率分布，再基于学习到的模型，输入 X 求出使得后验概率最大的输出 Y 。<br>适用条件：特征最好是离散的。</p>\n<p>基础知识：</p>\n<ul>\n<li>概率</li>\n<li>条件概率</li>\n<li>联合概率</li>\n<li>全概率公式</li>\n<li>贝叶斯公式</li>\n<li>拉普拉斯平滑：若条件概率为0，导致整个式子为0，则在所有值都加1.</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.naive_bayes import GaussianNB,BernoulliNB  </span><br><span class=\"line\">&#x2F;&#x2F;高斯模贝叶斯，假设特征是高斯分布  </span><br><span class=\"line\">GaussianNB( ).fit( ).predict( )  </span><br><span class=\"line\">&#x2F;&#x2F;伯努利贝叶斯，适用于离散值是二值的情况  </span><br><span class=\"line\">BernoulliNB( ).fit( ).predict( )</span><br></pre></td></tr></table></figure>\n<h6 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h6><p>算法思想：<br>特征判别先后顺序的依据或评价手段：</p>\n<ul>\n<li>信息增益-ID3：适用于离散值较多的分类问题<ul>\n<li>值越大，该特征越先比较</li>\n</ul>\n</li>\n<li>信息增益率-C4.5：适用于离散值较多的分类问题<ul>\n<li>考虑到了熵本身值大小的影响</li>\n</ul>\n</li>\n<li>Gini系数-CART：不纯度。适用于连续值分类问题<ul>\n<li>不纯度值最低的作当前区分</li>\n</ul>\n</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>连续值切分方法同探索性分析的离散化方法</li>\n<li>规则用尽则投票，哪个样本多投哪个</li>\n<li>若过拟合，需要修建枝叶：<ul>\n<li>前剪枝：构造决策树之前规定每个叶子节点最多有多少个样本，或规定决策树的最大深度</li>\n<li>后剪枝：先构造决策树，然后对样本值比较悬殊的枝叶进行修剪</li>\n</ul>\n</li>\n<li>若想生成图示，需下载app：Graphviz</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.tree import DecisionTreeClassifier  </span><br><span class=\"line\">&#x2F;&#x2F;默认采用Gini系数(不纯度)  </span><br><span class=\"line\">DecisionTreeClassifier( ).fit( ).predict( )  </span><br><span class=\"line\">&#x2F;&#x2F;使用信息增益(ID3)  </span><br><span class=\"line\">DecisionTreeClassifier(criterion&#x3D;&quot;entropy&quot;).fit( ).predict( )</span><br></pre></td></tr></table></figure>\n<h6 id=\"支持向量机-SVM\"><a href=\"#支持向量机-SVM\" class=\"headerlink\" title=\"支持向量机(SVM)\"></a>支持向量机(SVM)</h6><p>SVM: Support Vector Machine</p>\n<p>基础概念：</p>\n<ul>\n<li>高维面</li>\n<li>分界面</li>\n<li>拉格朗日乘数法</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>若一些计算结果为无穷大，可容忍部分错误的分类，转换求 min(max(L)) ;也可利用 KKT 条件，求 max(min(L))</li>\n<li>如需扩维，有两种方式：<ul>\n<li>线映射，在计算。这样容易造成维度灾难。</li>\n<li>先在低维空间计算，在利用核函数扩维<ul>\n<li>常见核函数：线性核函数、多项式核函数、高斯径向基(RBF)核函数</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>若存在少部分异常，可松弛变量，即为了达到更宽的分界线，允许存在少量错分点</li>\n<li>若样本不平衡，根据实际业务场景定</li>\n<li>对于多分类问题：<ul>\n<li>One-Other：有几个分类建几个 SVM ，分成一个分类和其他分类</li>\n<li>One-One：分类的两两之间分别建立 SVN</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.svm import SVC  </span><br><span class=\"line\">SVC(c&#x3D;100000).fit( ).predict( ) &#x2F;&#x2F;c为分类精度，值越大运行时间越长</span><br></pre></td></tr></table></figure>\n<h6 id=\"Adaboost\"><a href=\"#Adaboost\" class=\"headerlink\" title=\"Adaboost\"></a>Adaboost</h6><p>Adaboost：集成方法中提升法的运用。</p>\n<p>特点：精度高，灵活可调，几乎不用担心过拟合，简化特征工程流程</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.ensemble import AdaboostClassifier  </span><br><span class=\"line\">AdaboostClassifier().fit( ).predict( )</span><br></pre></td></tr></table></figure>\n\n<h6 id=\"随机森林\"><a href=\"#随机森林\" class=\"headerlink\" title=\"随机森林\"></a>随机森林</h6><p>随机森林：集成方法中袋装法的运用。由多个决策树集成。</p>\n<p>基本概念：</p>\n<ul>\n<li>树的个数<ul>\n<li>考虑到的样本的局部性的可能情况越多，越容易过拟合</li>\n<li>树的数量与样本数量、特征数量都有关系，不断的尝试后确定</li>\n</ul>\n</li>\n<li>树的特征数<ul>\n<li>特征少时每棵树用全部特征，特征多时每棵树用部分特征</li>\n<li>可增加树的数量和并行计算的能力来平衡特征减少可能带来的损失</li>\n</ul>\n</li>\n<li>树的训练集<ul>\n<li>每棵树的训练集都是模型训练集的一个子集</li>\n<li>选取子集的方法有两种：<ul>\n<li>训练子集和模型训练集数量一样，采用有放回的抽样构成样本差异性</li>\n<li>每棵树都用全部样本，通过缩减特征的规模构成样本的差异性</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>每个决策树可以不使用全部特征，减少规模和复杂度</li>\n<li>不需要剪枝，即可有效避免过拟合</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.ensemble import RandomForestClassifier  </span><br><span class=\"line\">RandomForestClassifier().fit( ).predict( )</span><br></pre></td></tr></table></figure>\n<h5 id=\"回归\"><a href=\"#回归\" class=\"headerlink\" title=\"回归\"></a>回归</h5><h6 id=\"线性回归\"><a href=\"#线性回归\" class=\"headerlink\" title=\"线性回归\"></a>线性回归</h6><p><code>一元线性回归</code><br>适用条件：适用于线性可分的场景</p>\n<p>基本概念：</p>\n<ul>\n<li>损失函数</li>\n<li>参数优化目标</li>\n<li>最小二乘法</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.linear_model import LinearRegression  </span><br><span class=\"line\">&#x2F;&#x2F;线性回归  </span><br><span class=\"line\">LinearRegression().fit( ).predict( )</span><br></pre></td></tr></table></figure>\n<p><code>多元线性回归</code></p>\n<p>基本概念：</p>\n<ul>\n<li>损失函数</li>\n<li>优化目标</li>\n<li>矩阵求解</li>\n<li>惩罚(正则化):通常在模型损失函数中增加一个正则项(惩罚项)来控制模型的复杂度。有两类惩罚项：<ul>\n<li>L1正则系数：Lasso回归</li>\n<li>L2正则系数：ridge回归(岭回归)</li>\n</ul>\n</li>\n</ul>\n<p>求解方法：<code>梯度下降法</code><br>一种无约束多元函数极值求解方法，通过迭代得到最小化的损失函数所对应的模型参数。<br>基本思路：在求解目标函数 E(a) 的最小值时，a 沿梯度下降的方向不断变化求解最小值。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.linear_model import Ridge,Lasso  </span><br><span class=\"line\">&#x2F;&#x2F;岭回归  </span><br><span class=\"line\">Ridge(alpha &#x3D; 5).fit( ).predict( ) &#x2F;&#x2F;alpha默认为0  </span><br><span class=\"line\">&#x2F;&#x2F;Lasso回归  </span><br><span class=\"line\">Lasso(alpha &#x3D; ).fit( ).predict( )</span><br></pre></td></tr></table></figure>\n\n<h6 id=\"Logistic回归\"><a href=\"#Logistic回归\" class=\"headerlink\" title=\"Logistic回归\"></a>Logistic回归</h6><p>基本概念：</p>\n<ul>\n<li>激活函数</li>\n<li>损失函数：对数似然损失函数</li>\n<li>梯度下降</li>\n</ul>\n<p>注意事项：同线性回归，也是求最小值，也可用梯度下降方法求解</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.linear_model import LogisticRegression  </span><br><span class=\"line\">LogisticRegression( ).fit( ).predict( ) &#x2F;&#x2F;alpha默认为0</span><br></pre></td></tr></table></figure>\n\n<h6 id=\"人工神经网络\"><a href=\"#人工神经网络\" class=\"headerlink\" title=\"人工神经网络\"></a>人工神经网络</h6><p>适用条件：适用于各种非线性映射。</p>\n<p>基本概念：</p>\n<ul>\n<li>感知器：处理线性映射关系</li>\n<li>感知器并联</li>\n<li>神经网络：<ul>\n<li>输入层：数据必须归一化；</li>\n<li>隐含层；</li>\n<li>输出层：必须是 one-hot 格式</li>\n</ul>\n</li>\n</ul>\n<p>求解方法：求解所有参数</p>\n<ul>\n<li>梯度下降算法：参数多，很复杂</li>\n<li>反向传播算法(PyBrain)<ul>\n<li>前向计算</li>\n<li>计算误差</li>\n<li>反向单层调整</li>\n<li>传播</li>\n<li>不断迭代，直到输出收敛到误差范围内或迭代固定次数</li>\n</ul>\n</li>\n<li>随机梯度下降算法(SGD,stochastic Gradient Decent)<ul>\n<li>每次调整权值时，选取部分样本进行梯度下降</li>\n<li>优点是收敛更快，计算开销小；缺点是容易陷入局部最优解。</li>\n<li>使用范围广</li>\n</ul>\n</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>人工神经网络的深度加深就形成深度神经网络。</li>\n<li>算法易受离群点影响，易过拟合。解决办法有两种：<ul>\n<li>正则化</li>\n<li>dropout：每次随机选取部分节点，组成多个神经网络模型，将多个结果投票选出得票最多的模型取其值；对于回归模型取其均值。类似与集成方法。</li>\n</ul>\n</li>\n<li>属性特征和结果要在0-1之间，且结果是 one-hot 形式</li>\n<li>输出结果进行 softmax 转化，确保其和为1</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python中没有神经网络的包，需要手写。  </span><br><span class=\"line\">&#x2F;&#x2F;步骤一：安装keras  </span><br><span class=\"line\">pip install tensorflow  </span><br><span class=\"line\">conda install pip &#x2F;&#x2F;仅window需要这一步，且已安装 Anconoda  </span><br><span class=\"line\">pip install keras  </span><br><span class=\"line\">&#x2F;&#x2F;步骤二：python中调用keras  </span><br><span class=\"line\">from keras.models import Sequential &#x2F;&#x2F; 类似容器  </span><br><span class=\"line\">from keras.models import Dense,Activation &#x2F;&#x2F;神经网络层，激活函数  </span><br><span class=\"line\">from keras.models import SGD &#x2F;&#x2F;随机梯度下降算法</span><br></pre></td></tr></table></figure>\n<h6 id=\"回归树和提升树\"><a href=\"#回归树和提升树\" class=\"headerlink\" title=\"回归树和提升树\"></a>回归树和提升树</h6><p>回归树：</p>\n<ul>\n<li>与分类树(决策树)的区别：<ul>\n<li>分类树中只需叶子结点有分类的判断值</li>\n<li>回归树中每个节点都有一个预测值，一般来说预测值是连续标注的平均值</li>\n</ul>\n</li>\n<li>回归树的切分方法：<ul>\n<li>切分后两部分的方差和最小。其中一个特征可以使用多次，直到满足回归树的停止条件。</li>\n</ul>\n</li>\n<li>回归树的停止条件有两种：<ul>\n<li>剪枝的限制：<ul>\n<li>树的最大深度</li>\n<li>叶子的最大样本数量</li>\n<li>…</li>\n</ul>\n</li>\n<li>最小方差值</li>\n</ul>\n</li>\n<li>回归树最终取叶子节点的平均值作为预测值。</li>\n</ul>\n<p>提升树：由多棵回归树集成。其中最佳的一种提升树是 <code>GBDT</code> (Gradient boosting Decision)梯度提升决策树</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.ensemble import GradientBoostingClassifier  </span><br><span class=\"line\">&#x2F;&#x2F;最佳提升树：梯度提升决策树  </span><br><span class=\"line\">GradientBoostingClassifier(max_depth &#x3D; 6,n_estimators &#x3D; 100 ).fit( ).predict( ) &#x2F;&#x2F;100棵树，每棵树深度为6</span><br></pre></td></tr></table></figure>\n<h4 id=\"非监督学习\"><a href=\"#非监督学习\" class=\"headerlink\" title=\"非监督学习\"></a>非监督学习</h4><p>将集合分成有类似的对象组成的多个类的过程，适用于样本集无标注的情况。</p>\n<p>非监督学习模型分为<code>聚类</code>和<code>关联</code>。</p>\n<ul>\n<li>分类适用于标注(标签)是离散的情况</li>\n<li>回归适用于标注是连续数值的情况</li>\n<li>分类和回归没有绝对的划分界限，分类可视为一种有限的回归；回归可视为一种无限定序数据的分类。有些算法都可运用在这两类模型上。</li>\n</ul>\n<p>非监督学习使用场景举例：App客群分类、词向量转化等</p>\n<p>聚类常用模型：基于切割的k-means、基于层次的聚类、基于密度的DBSCAN、基于图的split<br>关联常用模型：Apriori、Apriori-All</p>\n<h5 id=\"聚类\"><a href=\"#聚类\" class=\"headerlink\" title=\"聚类\"></a>聚类</h5><h6 id=\"k-means\"><a href=\"#k-means\" class=\"headerlink\" title=\"k-means\"></a>k-means</h6><p>算法思想：所有类都有一个中心，属于一个类的点到它的中心的距离相比于其他类的中心更近。中心是指质心，距离常用欧式距离。</p>\n<p>实现步骤：</p>\n<ul>\n<li>从n个样本中随机选取k个作为初始化的质心</li>\n<li>对每个样本测量其到每个质心的距离，并把它归到最近的质心的类</li>\n<li>重新计算已经得到的各个类的质心</li>\n<li>迭代第二、三步，直至新的质心与原质心相等或小于阈值，算法结束</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>初始质心位置可能回影响最终结果<ul>\n<li>多试几次，取最稳定的结果</li>\n</ul>\n</li>\n<li>个别离群值会影响整体聚类效果<ul>\n<li>将取质心换成取中心(k-medoids)。k-medoids 中点与其他同类点的距离和最小</li>\n</ul>\n</li>\n<li>必须指定k值<ul>\n<li>其他衡量因子辅助，如轮廓系数、最小误差…</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.cluster import KMeans  </span><br><span class=\"line\">KMeans(n_clusters &#x3D; 2).fit(X) &#x2F;&#x2F;分成两类  </span><br><span class=\"line\">KMeans.labels_.astype(np.int) &#x2F;&#x2F;将聚类后得到的labels转换成int格式  </span><br><span class=\"line\">plt.scatter( ) &#x2F;&#x2F;画散点图</span><br></pre></td></tr></table></figure>\n<h6 id=\"层次聚类\"><a href=\"#层次聚类\" class=\"headerlink\" title=\"层次聚类\"></a>层次聚类</h6><p>算法思想：相近的点尽可能接近。把相近的点视为一个簇，根据分类个数不断迭代。</p>\n<p>距离衡量指标：</p>\n<ul>\n<li>ward距离</li>\n<li>平方残差和：值越小，两个簇越可以合成一个簇</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>层次聚类灵活，但是计算复杂度比较高，离群点影响比较大</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.cluster import AgglomerativeClustering  </span><br><span class=\"line\">AgglomerativeClustering(n_clusters &#x3D; 2,linkage&#x3D;&quot;ward&quot;) &#x2F;&#x2F;分成两类</span><br></pre></td></tr></table></figure>\n\n<h6 id=\"DBSCAN\"><a href=\"#DBSCAN\" class=\"headerlink\" title=\"DBSCAN\"></a>DBSCAN</h6><p>算法思想：一定区域内，密度达到一定程度才是一个类，否则是离群点。</p>\n<p>基本概念：</p>\n<ul>\n<li>E邻域：给定对象半径为 E 内的区域称为该对象的 E邻域</li>\n<li>核心对象：如果给定对象 E邻域内的样本点大于或等于 MinPts，则称该对象为核心对象</li>\n<li>直接密度可达：对于样本集合 D，如果样本点 q在 p的E邻域内，并且 p为核心对象，那么对象 q从对象 p直接密度可达。</li>\n<li>密度可达：对于样本集合 D，给定一串样本点 p1、p2、…、pn, p = p1, q = pn,假设对象 pi从 pi-1直接密度可达，那么对象 q从对象 p密度可达</li>\n<li>密度相连：存在样本集合 D中的一点 o，如果对象 o到对象 p和对象 q都是密度可达的，那么 p和 q密度相连</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>DBSCAN算法就是找到密度相连对象的最大集合</li>\n<li>DBSCAN算法优点：对离群点不敏感</li>\n<li>缺点：<ul>\n<li>计算相邻两个点之间的点不容易<ul>\n<li>借助 KD-Tree等数据结构的辅助</li>\n</ul>\n</li>\n<li>需要指定两个参数：E 、 MinPts<ul>\n<li>多尝试</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.cluster import DBSCAN  </span><br><span class=\"line\">DBSCAN(min_samples &#x3D; 3,eps&#x3D; 5) &#x2F;&#x2F;min_samples：最小点数；eps:E邻域</span><br></pre></td></tr></table></figure>\n\n<h6 id=\"图分裂\"><a href=\"#图分裂\" class=\"headerlink\" title=\"图分裂\"></a>图分裂</h6><p>实现步骤：</p>\n<ul>\n<li>根据坐标点位置距离关系形成连通图(可采用DBSCAN等算法思路找到最大范围的点数，然后用边连接起来)</li>\n<li>将形成的多个连通图进行逐一分裂</li>\n</ul>\n<p>分类的依据：</p>\n<ul>\n<li>承受系数t</li>\n<li>分裂阈值$\\lambda$</li>\n<li>若 t &gt; $\\lambda$,则将该组边切分</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>与基于层次的聚类思路相反，是从顶至下</li>\n<li>图建立方式、分裂方式非常灵活</li>\n</ul>\n<h5 id=\"关联\"><a href=\"#关联\" class=\"headerlink\" title=\"关联\"></a>关联</h5><p>基本概念：</p>\n<ul>\n<li>项目：一个字段，对交易来说一般是指一次交易中的一个物品，如：尿布</li>\n<li>事物：某个客户在一次交易中，发生的所有项目的集合，如：{尿布，啤酒}</li>\n<li>项集：包含若干个项目的集合(一次事务中)</li>\n<li>频繁项集：某个项集的支持度大于设定阈值(人为设定或者根据数据分布出经验来定)，即称这个项集为频繁项集</li>\n<li>支持度：项集{x, y}在总项集中出现的频率(support)</li>\n<li>置信度：在先决条件 x发生的条件下，有关联规则{x -&gt; y}推出 y 的概率(Confidence)</li>\n<li>提升度：表示含有 x的条件下同时含有 y的概率，与无论含不含 x都含有 y的概率之比。(Confidence({x} -&gt; {y}) / support({y}))。若提升度大于1，则表示购买 x对购买 y有提升作用。</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>两个频繁项集组成的项集不一定是频繁项集</li>\n<li>两个非频繁项集组成的项集一定不是频繁项集</li>\n<li>一个频繁项集和一个非频繁项集组成的项集一定不是频繁项集</li>\n</ul>\n<p>关联可分为两类：</p>\n<ul>\n<li>关联规则：反映一个事物与其他事物之间的相互依存性和关联性，如Apriori算法</li>\n<li>序列规则：与关联规则相似。不同的是将时间因素考虑进来，剔除关联规则中时间点靠后的项对时间点靠前的项的支持，如Apriori-All算法</li>\n</ul>\n<h6 id=\"Apriori\"><a href=\"#Apriori\" class=\"headerlink\" title=\"Apriori\"></a>Apriori</h6><p>算法思想：先指定支持度的阈值，若一个项集的支持度大于这个阈值，则称其为频繁集，然后找出频繁项集。</p>\n<p>找出频繁集的方法：</p>\n<ul>\n<li>先找出一频繁项集，去掉一非频繁项集</li>\n<li>然后将一频繁项集组成二频繁项集，根据阈值去掉二非频繁项集</li>\n<li>再将一频繁项集和二频繁项集组成三频繁项集，根据阈值去掉三非频繁项集</li>\n<li>以此类推</li>\n<li>直至找出最高阶的频繁项集，所有组合遍历完毕，整理全部项集</li>\n</ul>\n<h6 id=\"Apriori-All\"><a href=\"#Apriori-All\" class=\"headerlink\" title=\"Apriori-All\"></a>Apriori-All</h6><p>适用场景：预测用户在购买某种东西后，下次购买时还会买其他什么东西作为搭配</p>\n<p>实现步骤：</p>\n<ul>\n<li>Forward: Apriori</li>\n<li>Backward: 去掉时间序列之后的项对之前的项的支持</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>sklearn中不支持序列规则，自己写</li>\n</ul>\n<h4 id=\"半监督学习\"><a href=\"#半监督学习\" class=\"headerlink\" title=\"半监督学习\"></a>半监督学习</h4><p>适用于样本集部分有标注，部分无标注的情况。通常无标注样本数量远大于有标注样本的数量。</p>\n<p>原因：获取标注的成本较大；无标注样本可能很容易获得。</p>\n<p>算法思路：</p>\n<ul>\n<li>生成模型思路：先对所有有标注的样本计算出一个分布，然后判别无标注的样本如何标注。也可采用分批迭代的方式，如先将与有标注样本比较近的样本进行标注，然后调整分布，在标注接下来的样本。</li>\n<li>判别模型的思路。也就是指物以类聚，如标签传播算法。</li>\n</ul>\n<p>常用算法：标签传播算法</p>\n<h5 id=\"标签传播算法\"><a href=\"#标签传播算法\" class=\"headerlink\" title=\"标签传播算法\"></a>标签传播算法</h5><p>算法思想：根据没有标注的样本和周围有标注的样本进行相似度比较，相似度高的将其标注为临近的标注。<br>其中传播是指迭代由近及远的过程。相似度判别方法：KNN、RBF等</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，参数自行查看sklearn官网  </span><br><span class=\"line\">from sklearn.semi_supervised import LabelPropagation  </span><br><span class=\"line\">LabelPropagation( ).fit( ).predict( )</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"深度学习\"><a href=\"#深度学习\" class=\"headerlink\" title=\"深度学习\"></a>深度学习</h4><p>暂略</p>\n<hr>\n<h2 id=\"模型评估方法\"><a href=\"#模型评估方法\" class=\"headerlink\" title=\"模型评估方法\"></a>模型评估方法</h2><p>python中用法：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.metrics import accuracy_score,recall_score,f1_score  </span><br><span class=\"line\">&#x2F;&#x2F;准确率  </span><br><span class=\"line\">print(&quot;ACC&quot;,accuracy_score(Y_validation,Ypred))  </span><br><span class=\"line\">&#x2F;&#x2F;召回率  </span><br><span class=\"line\">print(&quot;REC&quot;,recall_score(Y_validation,Ypred))  </span><br><span class=\"line\">&#x2F;&#x2F;F值  </span><br><span class=\"line\">print(&quot;F_score&quot;,f1_score(Y_validation,Ypred))</span><br></pre></td></tr></table></figure>\n<h3 id=\"分类模型的常用评价指标\"><a href=\"#分类模型的常用评价指标\" class=\"headerlink\" title=\"分类模型的常用评价指标\"></a>分类模型的常用评价指标</h3><h4 id=\"二分类评价指标\"><a href=\"#二分类评价指标\" class=\"headerlink\" title=\"二分类评价指标\"></a>二分类评价指标</h4><p>分正类(1)和负类(0)<br><code>基本指标</code></p>\n<ul>\n<li>误差率：错分类样本占总体样本的比例</li>\n<li>准确率(正确率)(Accuracy Rate)：正确分类样本占总体样本的比例</li>\n</ul>\n<p><code>混淆矩阵</code><br>| 真实情况 | 预测为正例 | 预测为负例<br>| —- | —- | —- |<br>| 正例 | TP(真正例) | FN(假反例) 漏<br>| 负例 | FP(假正例) 错 | TN(真反例)</p>\n<p><code>衍生指标</code></p>\n<ul>\n<li>查准率(precision):所有真正例占所有预测为正的样本的比例</li>\n<li>查全率(招回率,Recall,TPR):所有真正例占所有真实为正的样本的比例</li>\n<li>F-measure(F-score):</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2 * Recall * Accuracy &#x2F; (Reacll + Accuracy)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>错误接收率(FPR,False Postive Rate):</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FP &#x2F; (FP + TN)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>错误拒绝率(FRR,False Rejction Rate):</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FN &#x2F; (TP + FN)</span><br></pre></td></tr></table></figure>\n<h4 id=\"多分类评价指标\"><a href=\"#多分类评价指标\" class=\"headerlink\" title=\"多分类评价指标\"></a>多分类评价指标</h4><ul>\n<li>多元混淆矩阵</li>\n<li>准确率：同二分类</li>\n<li>召回率与F值：两种思路处理：<ul>\n<li>先计算所有的TP、FN等值，再以二值方法计算</li>\n<li>分别把每个分类当作正类，各计算一个召回率或F值，然后取加权或不加权的平均值</li>\n</ul>\n</li>\n<li>ROC曲线与AUC值：衡量分类效果，并且可以限定阈值<ul>\n<li>ROC曲线：以召回率(TPR)为纵轴，错误接收率(FPR)为横轴，采用不同的截断点，绘制ROC曲线。ROC曲线能够很容易地查出任意界限值对性能的识别能力。</li>\n<li>AUC值：ROC曲线与坐标轴构成的图形面积。AUC值越接近1，说明越准确。</li>\n</ul>\n</li>\n<li>增益图和KS图：衡量分类效果<ul>\n<li>增益图：宏观上反映分类器的效果</li>\n<li>KS图：反映对正类样本份额例的</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"回归模型的常用评价指标\"><a href=\"#回归模型的常用评价指标\" class=\"headerlink\" title=\"回归模型的常用评价指标\"></a>回归模型的常用评价指标</h3><ul>\n<li>样本误差：衡量模型在一个样本上的预测准确度<ul>\n<li>样本误差 = 样本预测值 - 样本实际值</li>\n</ul>\n</li>\n<li>平均误差方(MSE)：最常用的评价指标<ul>\n<li>所有样本的样本误差的平方和的均值。MSE越接近0，模型越准确。</li>\n</ul>\n</li>\n<li>平均绝对误差(MAE)：较好解释的评价指标<ul>\n<li>所有样本的样本误差的绝对值的均值。MAE的单位与因变量的单位一致，其越接近0，模型越准确。</li>\n</ul>\n</li>\n<li>平均绝对比例误差(MAPE)：平均绝对误差的衍生指标<ul>\n<li>所有样本的样本误差的绝对值占实际值的比例。指标越接近0，模型越准确。</li>\n</ul>\n</li>\n<li>决定系数： R2-score<ul>\n<li>因变量的方差能被自变量解释的程度。指标越接近1，则代表自变量对于因变量的解释程度越高。通常 &gt;0.5 ，就还不错。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"聚类模型的常用评价指标\"><a href=\"#聚类模型的常用评价指标\" class=\"headerlink\" title=\"聚类模型的常用评价指标\"></a>聚类模型的常用评价指标</h3><ul>\n<li>RMS(Root Mean Square)：值越小，分类效果越好<br>  $$ RMS = \\frac{1}{n} \\sqrt{\\sum_{i=0}^{n} (x_i - \\bar{x})^{2}}$$</li>\n<li>轮廓系数<ul>\n<li>a(i)为样本i与簇内其他样本的平均距离，也称为内聚度</li>\n<li>b(i)为样本i与其他某簇样本的平均距离，也称为分离度</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;s(i)越接近1，分类效果越好；越接近-1，分类效果最差  </span><br><span class=\"line\">s(i) &#x3D; (b(i) - a(i)) &#x2F; max&#123;a(i),b(i)&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"关联模型的常用评价指标\"><a href=\"#关联模型的常用评价指标\" class=\"headerlink\" title=\"关联模型的常用评价指标\"></a>关联模型的常用评价指标</h2><ul>\n<li>支持度：项集{x, y}在总项集中出现的频率(support)</li>\n<li>置信度：在先决条件 x发生的条件下，有关联规则{x -&gt; y}推出 y 的概率(Confidence)</li>\n<li>提升度：表示含有 x的条件下同时含有 y的概率，与无论含不含 x都含有 y的概率之比。(Confidence({x} -&gt; {y}) / support({y}))。若提升度大于1，则表示购买 x对购买 y有提升作用。</li>\n</ul>\n<h2 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h2><p>python中保存和加载模型的方式：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.externals import joblib  </span><br><span class=\"line\">&#x2F;&#x2F;保存模型  </span><br><span class=\"line\">joblib.dump(knn_clf,&quot;knn_clf&quot;)  </span><br><span class=\"line\">&#x2F;&#x2F;加载模型  </span><br><span class=\"line\">joblib.load(knn_clf,&quot;knn_clf&quot;)</span><br></pre></td></tr></table></figure>\n<h2 id=\"后续\"><a href=\"#后续\" class=\"headerlink\" title=\"后续\"></a>后续</h2><blockquote>\n<p>注：本文持续更新中</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<p>本人是一名数据爱好者，梦想是成为一名数据科学家。这里记录我从零开始的学习过程。内容若有什么不对的，欢迎大家批评指正，也希望能和感兴趣的读者一起探讨。</p>\n<blockquote>\n<p>注1：数据科学家有两条发展路径，一条是偏向业务可视化的商业分析师（数据分析师），一条是偏向于建模算法的数据挖掘工程师。这里不予严格区分。本文旨在展示数据获取、处理、建模、分析、可视化等全过程。读者可根据自己的喜好选择一个方向深入。</p>\n</blockquote>\n<blockquote>\n<p>注2：本文长期维护</p>\n</blockquote>\n<h2 id=\"数据分析全貌\"><a href=\"#数据分析全貌\" class=\"headerlink\" title=\"数据分析全貌\"></a>数据分析全貌</h2><p>数据分析是一门利用统计学知识，从数据中提取有用信息，进行总结和概括的学科。</p>\n<p>一名优秀的数据分析师应该具备的能力：<code>好奇</code>、<code>谨慎</code>、<code>责任</code>。</p>\n<p>在实际的工作中，每个分析师应该有自己处理问题的工作流程，并在实际的工作中不断的完善和迭代。最基本的流程如下所示：<br><code>数据工作流</code>：抛出问题 -&gt; 获取数据 -&gt; 数据研究 -&gt; 问题结论 -&gt; 解决办法</p>\n<p><code>数据建模和挖掘具体流程</code>：数据获取 -&gt; 探索性分析及可视化 -&gt; 数据预处理 -&gt; 数据挖掘建模 -&gt; 模型评估</p>\n<p>前两个流程（数据获取、探索性分析与可视化）也是狭义的数据分析。</p>\n<p><code>前修知识</code>：数学(高数、概率论与数理统计)、统计学、python(或者 R)；（sql、excel 本文暂不探讨）</p>\n<p>涉及的python第三方库：</p>\n<ul>\n<li>数据科学包：Numpy(数据基础包)、Pnadas(数据处理神器)、Scipy(科学计算包)、Matplotlib(可视化)、Seaborn(可视化)</li>\n<li>机器学习包：Scikit-learn(传统机器学习)、TensorFlow(深度学习)</li>\n</ul>\n<p><code>工具</code>：Visual Studio Code(Jupyter插件)</p>\n<h2 id=\"数据获取\"><a href=\"#数据获取\" class=\"headerlink\" title=\"数据获取\"></a>数据获取</h2><h3 id=\"数据仓库\"><a href=\"#数据仓库\" class=\"headerlink\" title=\"数据仓库\"></a>数据仓库</h3><p><code>定义</code>：将所有的业务数据汇总处理，构成数据仓库(DW)</p>\n<ul>\n<li>全面、完备、尽可能详细的记录全部事实<ul>\n<li>如某人几分几秒浏览了什么页面</li>\n</ul>\n</li>\n<li>方便对部分维度和数据的整理(数据集市-DM)</li>\n</ul>\n<p><code>数据仓库与文件和日志的区别</code>：文件、日志只能顺序记录，不方便查找、比较、抽取特征等操作</p>\n<p><code>数据仓库与数据库的区别</code>：</p>\n<ul>\n<li>数据仓库：<ul>\n<li>面向主题存储<ul>\n<li>主题：较高层次上对分析对象数据的一个完整并且一致的描述</li>\n<li>如购买图书这个行为就是个主题，在几时几分几秒以什么样的价格购买了什么样的书就是这个购买主题的一个记录，记录里有时间有用户信息，有图书信息等各个维度的信息。主题就是各个数据相互联系的描述。</li>\n</ul>\n</li>\n<li>针对分析(OLAP)</li>\n<li>可能冗余、相对变化较大、数据量大</li>\n</ul>\n</li>\n<li>数据库<ul>\n<li>面向业务存储<ul>\n<li>高并发、快速读写、数据结构精简</li>\n</ul>\n</li>\n<li>针对应用(OLTP)<ul>\n<li>为用户提供数据上的支持和服务</li>\n</ul>\n</li>\n<li>组织规范</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"检测与抓取-爬虫\"><a href=\"#检测与抓取-爬虫\" class=\"headerlink\" title=\"检测与抓取(爬虫)\"></a>检测与抓取(爬虫)</h3><p><code>检测</code>：用检测设备和检测算法直接获取数据。如传感器网络</p>\n<p><code>抓取(爬虫)</code>：直接通过获取网页内容进行解析和分析，直接解析网页、接口和文件的信息。</p>\n<ul>\n<li>python常用库：<ul>\n<li>抓取：urllib、urllib2、requests、scrap</li>\n<li>渲染：PhantomJS</li>\n<li>解析：beautifulSoup、Xpath(lxml)</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"填写、日志、埋点\"><a href=\"#填写、日志、埋点\" class=\"headerlink\" title=\"填写、日志、埋点\"></a>填写、日志、埋点</h3><p><code>用户填写</code>：用户填写的信息。如调查问卷、用户注册时填写的信息</p>\n<p><code>操作日志</code>：以文件形式记录。</p>\n<ul>\n<li>前端日志：网页和APP中记录的日志</li>\n<li>后端日志：服务器的日志</li>\n</ul>\n<p><code>埋点</code>：在APP或网页应用中针对特定的流程收集一定的信息，用来跟踪APP或网页被使用的情况，以便后继用来进一步优化产品或进行运营支持。</p>\n<ul>\n<li>比较常用的记录项：访问、访客、停留时间、页面查看和跳出率。</li>\n<li>分为页面统计和统计操作行为。</li>\n<li>可自己开发也可选择第三方工具，如友盟。</li>\n</ul>\n<h3 id=\"计算\"><a href=\"#计算\" class=\"headerlink\" title=\"计算\"></a>计算</h3><p><code>计算</code>：通过已有的数据计算生成衍生数据。如企业的投入产出比</p>\n<h3 id=\"数据学习网站\"><a href=\"#数据学习网站\" class=\"headerlink\" title=\"数据学习网站\"></a>数据学习网站</h3><p><code>数据竞赛网站</code>：kaggle &amp; 天池<br><code>图片数据集网站</code>：ImageNet、Open Images<br><code>各领域统计数据</code>：统计局、政府机构、公司财报等</p>\n<h2 id=\"探索性分析与数据可视化\"><a href=\"#探索性分析与数据可视化\" class=\"headerlink\" title=\"探索性分析与数据可视化\"></a>探索性分析与数据可视化</h2><h3 id=\"统计分析方法基础\"><a href=\"#统计分析方法基础\" class=\"headerlink\" title=\"统计分析方法基础\"></a>统计分析方法基础</h3><p><code>集中趋势</code>：均值、中位数与分位数、众数<br>四分位数的计算方法：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># n为数据的个数  </span><br><span class=\"line\">Q1位置 &#x3D; (n+1)*25  </span><br><span class=\"line\">Q2位置 &#x3D; (n+1)*5  </span><br><span class=\"line\">Q3位置 &#x3D; (n+1)*75</span><br></pre></td></tr></table></figure>\n<p><code>离中趋势</code>：标准差、方差</p>\n<p><code>数据分布</code>：偏态与峰态、正态分布与三大分布</p>\n<ul>\n<li>偏态系数：数据平均值偏离状态的一种衡量</li>\n<li>峰态系数：数据分布集中强度的衡量</li>\n<li>分布概率<ul>\n<li>正态分布</li>\n<li>卡方分布: 标准正态分布的平方和</li>\n<li>$T$分布：用于根据小样本来估计呈正态分布且方差未知的总体的均值</li>\n<li>$F$分布</li>\n</ul>\n</li>\n</ul>\n<p><code>抽样理论</code></p>\n<ul>\n<li>抽样原因：<ul>\n<li>数据量异常大，全量计算的时间、成本都很大。(大数据发展，不再是问题)</li>\n<li>全量检测不现实。比如测灯泡的寿命</li>\n</ul>\n</li>\n<li>抽样类型：<ul>\n<li>重复抽样(有放回的抽样)</li>\n<li>不重复抽样(无放回的抽样)</li>\n</ul>\n</li>\n<li>抽样方式<ul>\n<li>完全随机抽样</li>\n<li>等差距抽样：某个属性从低到高排列，等间距抽样</li>\n<li>分类的分层抽样：根据各个类别的比例抽样</li>\n</ul>\n</li>\n<li>抽样平均误差计算公式<ul>\n<li>重复抽样</li>\n<li>不重复抽样</li>\n</ul>\n</li>\n<li>估计总体时抽样数量的确定<ul>\n<li>重复抽样</li>\n<li>不重复抽样</li>\n</ul>\n</li>\n</ul>\n<p><code>数据分类</code></p>\n<ul>\n<li>定类(类别)：根据事物离散、无差别属性进行分类。如性别、名族</li>\n<li>定序(顺序)：可以界定数据的大小、但不能测定差值。如收入的高、中、低</li>\n<li>定距(间距)：可以界定数据大小同时可测定差距，但无绝对零点(乘除比率无意义)。如温度</li>\n<li>定比(比率)：可以界定数据大小同时可测定差距，有绝对零点。如身高、体重</li>\n</ul>\n<p><code>假设检验与方差检验</code><br>假设检验：做出一个假设，根据数据或已知的分布性质来推断这个假设成立的概率有多大。</p>\n<ul>\n<li>建立原假设 H0 (包含等号)，H1是H0的反命题，也称备择假设</li>\n<li>选择检验统计量</li>\n<li>根据显著性水平(一般为0.05)，确定拒绝域</li>\n<li>计算<code>P值</code>或<code>样本统计量</code>，做出判断</li>\n</ul>\n<p>检验统计量：</p>\n<ul>\n<li>$\\mu$ 分布</li>\n<li>卡方分布：用于检测两个因素之间有无强联系</li>\n<li>$T$检验：比较两组样本分布是否一致；两组值的均值有无较大差异</li>\n<li>$F$检验（方差检验)：用于方差分析。多样本两两之间是否有差异。<ul>\n<li>总变差平方和</li>\n<li>平均平方和/组内平方和</li>\n<li>残差平方和/组内平方差</li>\n<li>统计量</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>注：也可以通过qq图来判断一个分布是否符合一个已知的分布，比如找到该分布的分位数做纵轴，正态分布的分位数做横轴，若连线接近角平分线，则符合</p>\n</blockquote>\n<p><code>相关系数</code>：衡量两组数据或两组样本的分布趋势、变化趋势一致性程度的因子。</p>\n<ul>\n<li>皮尔逊(Pearson)相关系数</li>\n<li>斯皮尔(Spearman)曼相关系数</li>\n</ul>\n<p><code>回归：线性回归</code>：确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法</p>\n<ul>\n<li>关键度量指标：<ul>\n<li>一元决定系数</li>\n<li>多元决定系数</li>\n</ul>\n</li>\n<li>残差不相关(DW检验)</li>\n</ul>\n<p><code>PCA与奇异值分解</code>：尽可能少的失真情况下，线性降维，成分提取</p>\n<ul>\n<li><p>主成分分析法(PCA)</p>\n<ul>\n<li>求特征协方差矩阵</li>\n<li>求协方差矩阵的特征值和特征向量</li>\n<li>将特征值按照从大到小的顺序排序，选取其中最大的k个</li>\n<li>将样本点投影到选取的特征向量上</li>\n</ul>\n</li>\n<li><p>奇异值分解(SVD)</p>\n<ul>\n<li>特征矩阵 A 分解为 m_m 的酉阵，m_n 半正定矩阵(奇异矩阵)，n*n 酉阵转置 v</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"探索性分析\"><a href=\"#探索性分析\" class=\"headerlink\" title=\"探索性分析\"></a>探索性分析</h3><h4 id=\"单因子探索\"><a href=\"#单因子探索\" class=\"headerlink\" title=\"单因子探索\"></a>单因子探索</h4><p>——展现数据全貌</p>\n<ul>\n<li>异常值分析(可用箱线图展示)<ul>\n<li>连续异常值<ul>\n<li>处理办法：舍去；异常值用边界值代替(四分位数)</li>\n</ul>\n</li>\n<li>离散异常值：离散属性定义范围外的所有值均为异常值。如空值；收入离散成高、中、低之外的值<ul>\n<li>处理办法：舍去；把所有异常值当作单独的一个值处理</li>\n</ul>\n</li>\n<li>常识异常值：在限定知识和常识范围外的所有值均为异常值。如身高20m</li>\n</ul>\n</li>\n<li>对比分析<ul>\n<li>比什么：<ul>\n<li>绝对数比较：数值直接比较。如比较收入、身高</li>\n<li>相对数比较：把几个有联系的指标联合成新的指标<ul>\n<li>结构相对数：部分与整体。如合格率、通过率</li>\n<li>比例相对数：总体内用不同部分的数值进行比较。如三大产业相互比较</li>\n<li>比较相对数：同一时空下相似或同质的指标进行比较。如不同时期同一产品的价格对比；不同互联网电商公司的待遇水平对比</li>\n<li>动态相对数：一般包含时间概念。如用户增速</li>\n<li>强度相对数：性质不同但又互相联系的属性进行联合。如人均、亩产、密度</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>怎么比：<ul>\n<li>时间<ul>\n<li>同比：今年2月同去年2月比</li>\n<li>环比：今年2月同今年1月比</li>\n</ul>\n</li>\n<li>空间<ul>\n<li>现实方位：如不同国家、不同城市</li>\n<li>逻辑上空间：如一家公司的不同部门；不同家公司之间的比较</li>\n</ul>\n</li>\n<li>经验和计划：如历史上失业率达到百分之几回发生暴乱，把国家的失业率与之比较；工作排期与实际进度之间的比较</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>结构分析：各组成部分的分布与规律<ul>\n<li>静态：直接分析总体的组成。如十一五时间三大产业的比例</li>\n<li>动态：时间为轴分析结构变化的趋势。如十一五期间三大产业比的变化</li>\n</ul>\n</li>\n<li>分布分析：数据分布频率的显示分析<ul>\n<li>直接获取的概率分布</li>\n<li>判断是否为正态分布<ul>\n<li>偏态系数</li>\n<li>峰态系数</li>\n</ul>\n</li>\n<li>极大似然</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"多因子探索\"><a href=\"#多因子探索\" class=\"headerlink\" title=\"多因子探索\"></a>多因子探索</h4><p>——探索属性与属性之间的联系</p>\n<ul>\n<li>交叉分析<ul>\n<li>热力图</li>\n<li>透视表</li>\n</ul>\n</li>\n<li>分组与钻取</li>\n<li>相关分析</li>\n<li>因子分析</li>\n<li>聚类分析(建模中也会用到)</li>\n<li>回归分析(建模中也会用到)</li>\n</ul>\n<h3 id=\"数据可视化\"><a href=\"#数据可视化\" class=\"headerlink\" title=\"数据可视化\"></a>数据可视化</h3><ul>\n<li>柱状图：横坐标表示离散值</li>\n<li>直方图：横坐标表示范围</li>\n<li>箱线图</li>\n<li>折线图</li>\n<li>饼图</li>\n</ul>\n<h2 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h2><h3 id=\"缺失值、离群值\"><a href=\"#缺失值、离群值\" class=\"headerlink\" title=\"缺失值、离群值\"></a>缺失值、离群值</h3><p><code>缺失值</code>：</p>\n<ul>\n<li>删除：缺失样本非常大(&gt;75%)，则删除整条数据</li>\n<li>填充：缺失量&lt;10%<ul>\n<li>若为正态分布，取均值</li>\n<li>若为偏态，取中位数</li>\n</ul>\n</li>\n<li>预测样本值：使用与缺失值相比相关性非常高的特征建立模型，预测缺失值</li>\n</ul>\n<p><code>离群点</code>：远离数据主要部分的样本(极大值或极小值)</p>\n<ul>\n<li>同单因子探索分析的异常值处理相同：删除或填充</li>\n</ul>\n<h3 id=\"标准化、纠偏\"><a href=\"#标准化、纠偏\" class=\"headerlink\" title=\"标准化、纠偏\"></a>标准化、纠偏</h3><p><code>标准化</code>：去除数量纲(单位)的影响，提高模型的解释度，加快模型的收敛速度。具体方法如下：</p>\n<ul>\n<li>中心化：减去均再除以标准差(之后均值为0，标准差为1)</li>\n<li>01标准化：减去最小值再除以最大值和最小值的差</li>\n</ul>\n<p><code>纠偏</code></p>\n<ul>\n<li>正态分布：数据呈现对称的钟态分布</li>\n<li>右偏态：样本大量集中在均值的左边(均值偏到了右边)</li>\n<li>左偏态：样本大量集中在均值的右边(均值偏到了左边)</li>\n<li>处理方法：<ul>\n<li>右偏态：常用对数函数处理</li>\n<li>左偏态：常用指数函数处理</li>\n</ul>\n</li>\n<li>通用变换方法：以降低数据的偏态系数为目的，使得数据分布更加接近正态分布的变换方法。<ul>\n<li>yeo-johnson 变换：可以处理包含正数、负数和零的变量</li>\n<li>box-cox 变换：只能处理数值皆为正的变量</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"特征工程：共线性、将维、扩展\"><a href=\"#特征工程：共线性、将维、扩展\" class=\"headerlink\" title=\"特征工程：共线性、将维、扩展\"></a>特征工程：共线性、将维、扩展</h3><p><code>共线性</code></p>\n<ul>\n<li>特征间共线性：两个或多个特征包含了相似的信息，相互之间存在强烈的相关关系。</li>\n<li>常用的判断标准：两个或两个以上的特征之间的相关系数高于0.8</li>\n<li>共线性的影响：降低运算效率；降低一些模型的稳定性；弱化一些模型的预测能力</li>\n<li>处理办法：<ul>\n<li>删除：一组相互共线的特征中只保留与因变量相关性最高的一个</li>\n<li>变换：对共线的两列特征进行求比值、求差值等计算</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"特征工程\"><a href=\"#特征工程\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h3><p><code>数据降维和特征提取</code><br>目的：降低不相关特征对模型准确性的干扰，降低模型的复杂度，提高模型的泛化能力，减少模型特征，提高模型训练和预测数据<br>处理办法：</p>\n<ul>\n<li>基于数据的理解，直接删除</li>\n<li>使用主成分分析法(PCA)对特征进行提取</li>\n<li>使用机器学习模型对特征进行筛选</li>\n</ul>\n<p><code>特征扩展</code><br>目的：解决模型欠拟合，捕捉自变量和因变量之间的非线性关系<br>常用方法：多项式扩展。举例如下：</p>\n<ul>\n<li>假设数据集中包含自变量a、b</li>\n<li>如果对自变量做多项式二次扩展</li>\n<li>自变量集从两个变量扩展为5个变量(a、b、a_a、b_b、a*b)</li>\n</ul>\n<h2 id=\"数据挖掘建模\"><a href=\"#数据挖掘建模\" class=\"headerlink\" title=\"数据挖掘建模\"></a>数据挖掘建模</h2><h3 id=\"数据集的划分方法\"><a href=\"#数据集的划分方法\" class=\"headerlink\" title=\"数据集的划分方法\"></a>数据集的划分方法</h3><p>训练集：用来训练和拟合模型<br>验证集：当通过训练集训练出多个模型后，使用验证集数据纠偏或比较预测<br>测试集：模型泛化能力的考量。<br>注：有时候数据集只划分两类，将验证集和测试机视为同一个数据集。</p>\n<p><code>数据集划分的基本原则</code></p>\n<ul>\n<li>保持训练集和验证集之间的互斥性</li>\n<li>测试样本尽量不在训练样本中出现，以保证验证集上的表现能代表模型的泛化能力</li>\n</ul>\n<p><code>数据集划分方法</code></p>\n<ul>\n<li><code>留出法</code>：直接将数据集划分成两个互斥的集合，一个做训练集，一个做验证集。常用的划分比例：7:3、7.5:2.5、8:2。若划分三类：6:2:2</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.model_selection import train_test_split</span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>交叉验证法</code>：将数据集划分成k个大小相似的互斥子集，每次把k-1个子集做训练，1个子集做验证，训练k次，最终返回k在、次训练结果的均值。因此交叉验证法又被称为k次交叉法(k-fold)。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.model_selection import KFold</span><br></pre></td></tr></table></figure>\n<h3 id=\"传统机器学习算法\"><a href=\"#传统机器学习算法\" class=\"headerlink\" title=\"传统机器学习算法\"></a>传统机器学习算法</h3><p>传统机器学习算法根据样本集有无标注、是否部分有标注分为三类：监督学习、无监督学习、半监督学习。</p>\n<p>讲算法之前先说明几个概念：<br><code>集成学习</code>：是指组合多个模型、有效提高模型泛化能力的学习策略。  </p>\n<p>| 基础概念 | 适用条件 |<br>| – | – |<br>| 弱可学习 | 多项式学习算法的效果不很明显 |<br>| 强可学习 | 多项式学习算法的效果较为明显 |  </p>\n<p>弱可学习可通过集成方法称为强可学习。<br>集成方法分类：</p>\n<ul>\n<li><code>袋装法</code>(bagging)：指将训练集分别用不同的模型进行训练，这些模型相互独立，然后将结果进行投票取均值的方法。如随机森林。</li>\n<li><code>提升法</code>(boost)：指训练集用一种模型训练出的结果作为另一个模型的输入，然后将其输出再作为其他模型的输入，如此反复。最后把这些模型进行加权叠加作为最终输出。如Adaboost、XGBoost。<ul>\n<li>注意这种方式中，子模型对最终结果的影响更大程度上取决于权值，而不是顺序。</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"监督学习\"><a href=\"#监督学习\" class=\"headerlink\" title=\"监督学习\"></a>监督学习</h4><p>适用于样本集有标注的情况。</p>\n<p>监督学习模型分为<code>分类</code>和<code>回归</code>两类。</p>\n<ul>\n<li>分类适用于标注(标签)是离散的情况</li>\n<li>回归适用于标注是连续数值的情况</li>\n<li>分类和回归没有绝对的划分界限，分类可视为一种有限的回归；回归可视为一种无限定序数据的分类。有些算法都可运用在这两类模型上。</li>\n</ul>\n<p>监督学习使用场景举例：图形识别、房价预测、银行信用评估等</p>\n<p>分类常用模型：KNN、朴素贝叶斯、Adaboost、随机森林<br>回归常用模型：线性回归、回归树和提升树<br>可同时用于分类和回归的模型：决策树、支持向量机(SVM)、Logistic模型、人工神经网络</p>\n<h5 id=\"分类\"><a href=\"#分类\" class=\"headerlink\" title=\"分类\"></a>分类</h5><p>具体的，分类模型也可以分为两种类型：生成模型、判别模型。</p>\n<ul>\n<li><code>生成模型</code>：通过求输入输出的联合概率分布，在求解类别归类的概率。如朴素贝叶斯</li>\n<li><code>判别模型</code>：不通过求联合概率分布，直接可以获得输出最大分类的概率。如KNN</li>\n</ul>\n<p>两者的区别：判别模型较生成模型，对数据的要求低一点，对数据的容忍度大一些，速度相对慢一些，适用范围更广一些。</p>\n<h6 id=\"KNN\"><a href=\"#KNN\" class=\"headerlink\" title=\"KNN\"></a>KNN</h6><p>KNN：K-Nearest Neighbors，最邻近结点算法。<br>算法思想：每个样本都可以用它最接近的K个邻近值来代表。<br>适用条件：用于标注在空间隔离性较好的情况。</p>\n<p>基础知识：</p>\n<ul>\n<li>欧式距离</li>\n<li>曼哈顿距离</li>\n<li>闵可夫斯基距离</li>\n<li>KD-Tree：点作为叶子节点，线作为分枝节点</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.neighbors import KNeighborsClassifier  </span><br><span class=\"line\">knn_clf&#x3D; KNeighborsClassifier(n_neighbors&#x3D;5) &#x2F;&#x2F;最近5个点  </span><br><span class=\"line\">knn_clf.fit(X_train,Y_train)  </span><br><span class=\"line\">Y_pred &#x3D; knn_clf.predict(X_validation)</span><br></pre></td></tr></table></figure>\n<h6 id=\"朴素贝叶斯\"><a href=\"#朴素贝叶斯\" class=\"headerlink\" title=\"朴素贝叶斯\"></a>朴素贝叶斯</h6><p>朴素：特征间相互独立。<br>算法思想：先通过已给定的训练集，以特征之间独立作为前提假设，学习从输入到输出的联合概率分布，再基于学习到的模型，输入 X 求出使得后验概率最大的输出 Y 。<br>适用条件：特征最好是离散的。</p>\n<p>基础知识：</p>\n<ul>\n<li>概率</li>\n<li>条件概率</li>\n<li>联合概率</li>\n<li>全概率公式</li>\n<li>贝叶斯公式</li>\n<li>拉普拉斯平滑：若条件概率为0，导致整个式子为0，则在所有值都加1.</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.naive_bayes import GaussianNB,BernoulliNB  </span><br><span class=\"line\">&#x2F;&#x2F;高斯模贝叶斯，假设特征是高斯分布  </span><br><span class=\"line\">GaussianNB( ).fit( ).predict( )  </span><br><span class=\"line\">&#x2F;&#x2F;伯努利贝叶斯，适用于离散值是二值的情况  </span><br><span class=\"line\">BernoulliNB( ).fit( ).predict( )</span><br></pre></td></tr></table></figure>\n<h6 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h6><p>算法思想：<br>特征判别先后顺序的依据或评价手段：</p>\n<ul>\n<li>信息增益-ID3：适用于离散值较多的分类问题<ul>\n<li>值越大，该特征越先比较</li>\n</ul>\n</li>\n<li>信息增益率-C4.5：适用于离散值较多的分类问题<ul>\n<li>考虑到了熵本身值大小的影响</li>\n</ul>\n</li>\n<li>Gini系数-CART：不纯度。适用于连续值分类问题<ul>\n<li>不纯度值最低的作当前区分</li>\n</ul>\n</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>连续值切分方法同探索性分析的离散化方法</li>\n<li>规则用尽则投票，哪个样本多投哪个</li>\n<li>若过拟合，需要修建枝叶：<ul>\n<li>前剪枝：构造决策树之前规定每个叶子节点最多有多少个样本，或规定决策树的最大深度</li>\n<li>后剪枝：先构造决策树，然后对样本值比较悬殊的枝叶进行修剪</li>\n</ul>\n</li>\n<li>若想生成图示，需下载app：Graphviz</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.tree import DecisionTreeClassifier  </span><br><span class=\"line\">&#x2F;&#x2F;默认采用Gini系数(不纯度)  </span><br><span class=\"line\">DecisionTreeClassifier( ).fit( ).predict( )  </span><br><span class=\"line\">&#x2F;&#x2F;使用信息增益(ID3)  </span><br><span class=\"line\">DecisionTreeClassifier(criterion&#x3D;&quot;entropy&quot;).fit( ).predict( )</span><br></pre></td></tr></table></figure>\n<h6 id=\"支持向量机-SVM\"><a href=\"#支持向量机-SVM\" class=\"headerlink\" title=\"支持向量机(SVM)\"></a>支持向量机(SVM)</h6><p>SVM: Support Vector Machine</p>\n<p>基础概念：</p>\n<ul>\n<li>高维面</li>\n<li>分界面</li>\n<li>拉格朗日乘数法</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>若一些计算结果为无穷大，可容忍部分错误的分类，转换求 min(max(L)) ;也可利用 KKT 条件，求 max(min(L))</li>\n<li>如需扩维，有两种方式：<ul>\n<li>线映射，在计算。这样容易造成维度灾难。</li>\n<li>先在低维空间计算，在利用核函数扩维<ul>\n<li>常见核函数：线性核函数、多项式核函数、高斯径向基(RBF)核函数</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>若存在少部分异常，可松弛变量，即为了达到更宽的分界线，允许存在少量错分点</li>\n<li>若样本不平衡，根据实际业务场景定</li>\n<li>对于多分类问题：<ul>\n<li>One-Other：有几个分类建几个 SVM ，分成一个分类和其他分类</li>\n<li>One-One：分类的两两之间分别建立 SVN</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.svm import SVC  </span><br><span class=\"line\">SVC(c&#x3D;100000).fit( ).predict( ) &#x2F;&#x2F;c为分类精度，值越大运行时间越长</span><br></pre></td></tr></table></figure>\n<h6 id=\"Adaboost\"><a href=\"#Adaboost\" class=\"headerlink\" title=\"Adaboost\"></a>Adaboost</h6><p>Adaboost：集成方法中提升法的运用。</p>\n<p>特点：精度高，灵活可调，几乎不用担心过拟合，简化特征工程流程</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.ensemble import AdaboostClassifier  </span><br><span class=\"line\">AdaboostClassifier().fit( ).predict( )</span><br></pre></td></tr></table></figure>\n\n<h6 id=\"随机森林\"><a href=\"#随机森林\" class=\"headerlink\" title=\"随机森林\"></a>随机森林</h6><p>随机森林：集成方法中袋装法的运用。由多个决策树集成。</p>\n<p>基本概念：</p>\n<ul>\n<li>树的个数<ul>\n<li>考虑到的样本的局部性的可能情况越多，越容易过拟合</li>\n<li>树的数量与样本数量、特征数量都有关系，不断的尝试后确定</li>\n</ul>\n</li>\n<li>树的特征数<ul>\n<li>特征少时每棵树用全部特征，特征多时每棵树用部分特征</li>\n<li>可增加树的数量和并行计算的能力来平衡特征减少可能带来的损失</li>\n</ul>\n</li>\n<li>树的训练集<ul>\n<li>每棵树的训练集都是模型训练集的一个子集</li>\n<li>选取子集的方法有两种：<ul>\n<li>训练子集和模型训练集数量一样，采用有放回的抽样构成样本差异性</li>\n<li>每棵树都用全部样本，通过缩减特征的规模构成样本的差异性</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>每个决策树可以不使用全部特征，减少规模和复杂度</li>\n<li>不需要剪枝，即可有效避免过拟合</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.ensemble import RandomForestClassifier  </span><br><span class=\"line\">RandomForestClassifier().fit( ).predict( )</span><br></pre></td></tr></table></figure>\n<h5 id=\"回归\"><a href=\"#回归\" class=\"headerlink\" title=\"回归\"></a>回归</h5><h6 id=\"线性回归\"><a href=\"#线性回归\" class=\"headerlink\" title=\"线性回归\"></a>线性回归</h6><p><code>一元线性回归</code><br>适用条件：适用于线性可分的场景</p>\n<p>基本概念：</p>\n<ul>\n<li>损失函数</li>\n<li>参数优化目标</li>\n<li>最小二乘法</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.linear_model import LinearRegression  </span><br><span class=\"line\">&#x2F;&#x2F;线性回归  </span><br><span class=\"line\">LinearRegression().fit( ).predict( )</span><br></pre></td></tr></table></figure>\n<p><code>多元线性回归</code></p>\n<p>基本概念：</p>\n<ul>\n<li>损失函数</li>\n<li>优化目标</li>\n<li>矩阵求解</li>\n<li>惩罚(正则化):通常在模型损失函数中增加一个正则项(惩罚项)来控制模型的复杂度。有两类惩罚项：<ul>\n<li>L1正则系数：Lasso回归</li>\n<li>L2正则系数：ridge回归(岭回归)</li>\n</ul>\n</li>\n</ul>\n<p>求解方法：<code>梯度下降法</code><br>一种无约束多元函数极值求解方法，通过迭代得到最小化的损失函数所对应的模型参数。<br>基本思路：在求解目标函数 E(a) 的最小值时，a 沿梯度下降的方向不断变化求解最小值。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.linear_model import Ridge,Lasso  </span><br><span class=\"line\">&#x2F;&#x2F;岭回归  </span><br><span class=\"line\">Ridge(alpha &#x3D; 5).fit( ).predict( ) &#x2F;&#x2F;alpha默认为0  </span><br><span class=\"line\">&#x2F;&#x2F;Lasso回归  </span><br><span class=\"line\">Lasso(alpha &#x3D; ).fit( ).predict( )</span><br></pre></td></tr></table></figure>\n\n<h6 id=\"Logistic回归\"><a href=\"#Logistic回归\" class=\"headerlink\" title=\"Logistic回归\"></a>Logistic回归</h6><p>基本概念：</p>\n<ul>\n<li>激活函数</li>\n<li>损失函数：对数似然损失函数</li>\n<li>梯度下降</li>\n</ul>\n<p>注意事项：同线性回归，也是求最小值，也可用梯度下降方法求解</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.linear_model import LogisticRegression  </span><br><span class=\"line\">LogisticRegression( ).fit( ).predict( ) &#x2F;&#x2F;alpha默认为0</span><br></pre></td></tr></table></figure>\n\n<h6 id=\"人工神经网络\"><a href=\"#人工神经网络\" class=\"headerlink\" title=\"人工神经网络\"></a>人工神经网络</h6><p>适用条件：适用于各种非线性映射。</p>\n<p>基本概念：</p>\n<ul>\n<li>感知器：处理线性映射关系</li>\n<li>感知器并联</li>\n<li>神经网络：<ul>\n<li>输入层：数据必须归一化；</li>\n<li>隐含层；</li>\n<li>输出层：必须是 one-hot 格式</li>\n</ul>\n</li>\n</ul>\n<p>求解方法：求解所有参数</p>\n<ul>\n<li>梯度下降算法：参数多，很复杂</li>\n<li>反向传播算法(PyBrain)<ul>\n<li>前向计算</li>\n<li>计算误差</li>\n<li>反向单层调整</li>\n<li>传播</li>\n<li>不断迭代，直到输出收敛到误差范围内或迭代固定次数</li>\n</ul>\n</li>\n<li>随机梯度下降算法(SGD,stochastic Gradient Decent)<ul>\n<li>每次调整权值时，选取部分样本进行梯度下降</li>\n<li>优点是收敛更快，计算开销小；缺点是容易陷入局部最优解。</li>\n<li>使用范围广</li>\n</ul>\n</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>人工神经网络的深度加深就形成深度神经网络。</li>\n<li>算法易受离群点影响，易过拟合。解决办法有两种：<ul>\n<li>正则化</li>\n<li>dropout：每次随机选取部分节点，组成多个神经网络模型，将多个结果投票选出得票最多的模型取其值；对于回归模型取其均值。类似与集成方法。</li>\n</ul>\n</li>\n<li>属性特征和结果要在0-1之间，且结果是 one-hot 形式</li>\n<li>输出结果进行 softmax 转化，确保其和为1</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python中没有神经网络的包，需要手写。  </span><br><span class=\"line\">&#x2F;&#x2F;步骤一：安装keras  </span><br><span class=\"line\">pip install tensorflow  </span><br><span class=\"line\">conda install pip &#x2F;&#x2F;仅window需要这一步，且已安装 Anconoda  </span><br><span class=\"line\">pip install keras  </span><br><span class=\"line\">&#x2F;&#x2F;步骤二：python中调用keras  </span><br><span class=\"line\">from keras.models import Sequential &#x2F;&#x2F; 类似容器  </span><br><span class=\"line\">from keras.models import Dense,Activation &#x2F;&#x2F;神经网络层，激活函数  </span><br><span class=\"line\">from keras.models import SGD &#x2F;&#x2F;随机梯度下降算法</span><br></pre></td></tr></table></figure>\n<h6 id=\"回归树和提升树\"><a href=\"#回归树和提升树\" class=\"headerlink\" title=\"回归树和提升树\"></a>回归树和提升树</h6><p>回归树：</p>\n<ul>\n<li>与分类树(决策树)的区别：<ul>\n<li>分类树中只需叶子结点有分类的判断值</li>\n<li>回归树中每个节点都有一个预测值，一般来说预测值是连续标注的平均值</li>\n</ul>\n</li>\n<li>回归树的切分方法：<ul>\n<li>切分后两部分的方差和最小。其中一个特征可以使用多次，直到满足回归树的停止条件。</li>\n</ul>\n</li>\n<li>回归树的停止条件有两种：<ul>\n<li>剪枝的限制：<ul>\n<li>树的最大深度</li>\n<li>叶子的最大样本数量</li>\n<li>…</li>\n</ul>\n</li>\n<li>最小方差值</li>\n</ul>\n</li>\n<li>回归树最终取叶子节点的平均值作为预测值。</li>\n</ul>\n<p>提升树：由多棵回归树集成。其中最佳的一种提升树是 <code>GBDT</code> (Gradient boosting Decision)梯度提升决策树</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.ensemble import GradientBoostingClassifier  </span><br><span class=\"line\">&#x2F;&#x2F;最佳提升树：梯度提升决策树  </span><br><span class=\"line\">GradientBoostingClassifier(max_depth &#x3D; 6,n_estimators &#x3D; 100 ).fit( ).predict( ) &#x2F;&#x2F;100棵树，每棵树深度为6</span><br></pre></td></tr></table></figure>\n<h4 id=\"非监督学习\"><a href=\"#非监督学习\" class=\"headerlink\" title=\"非监督学习\"></a>非监督学习</h4><p>将集合分成有类似的对象组成的多个类的过程，适用于样本集无标注的情况。</p>\n<p>非监督学习模型分为<code>聚类</code>和<code>关联</code>。</p>\n<ul>\n<li>分类适用于标注(标签)是离散的情况</li>\n<li>回归适用于标注是连续数值的情况</li>\n<li>分类和回归没有绝对的划分界限，分类可视为一种有限的回归；回归可视为一种无限定序数据的分类。有些算法都可运用在这两类模型上。</li>\n</ul>\n<p>非监督学习使用场景举例：App客群分类、词向量转化等</p>\n<p>聚类常用模型：基于切割的k-means、基于层次的聚类、基于密度的DBSCAN、基于图的split<br>关联常用模型：Apriori、Apriori-All</p>\n<h5 id=\"聚类\"><a href=\"#聚类\" class=\"headerlink\" title=\"聚类\"></a>聚类</h5><h6 id=\"k-means\"><a href=\"#k-means\" class=\"headerlink\" title=\"k-means\"></a>k-means</h6><p>算法思想：所有类都有一个中心，属于一个类的点到它的中心的距离相比于其他类的中心更近。中心是指质心，距离常用欧式距离。</p>\n<p>实现步骤：</p>\n<ul>\n<li>从n个样本中随机选取k个作为初始化的质心</li>\n<li>对每个样本测量其到每个质心的距离，并把它归到最近的质心的类</li>\n<li>重新计算已经得到的各个类的质心</li>\n<li>迭代第二、三步，直至新的质心与原质心相等或小于阈值，算法结束</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>初始质心位置可能回影响最终结果<ul>\n<li>多试几次，取最稳定的结果</li>\n</ul>\n</li>\n<li>个别离群值会影响整体聚类效果<ul>\n<li>将取质心换成取中心(k-medoids)。k-medoids 中点与其他同类点的距离和最小</li>\n</ul>\n</li>\n<li>必须指定k值<ul>\n<li>其他衡量因子辅助，如轮廓系数、最小误差…</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.cluster import KMeans  </span><br><span class=\"line\">KMeans(n_clusters &#x3D; 2).fit(X) &#x2F;&#x2F;分成两类  </span><br><span class=\"line\">KMeans.labels_.astype(np.int) &#x2F;&#x2F;将聚类后得到的labels转换成int格式  </span><br><span class=\"line\">plt.scatter( ) &#x2F;&#x2F;画散点图</span><br></pre></td></tr></table></figure>\n<h6 id=\"层次聚类\"><a href=\"#层次聚类\" class=\"headerlink\" title=\"层次聚类\"></a>层次聚类</h6><p>算法思想：相近的点尽可能接近。把相近的点视为一个簇，根据分类个数不断迭代。</p>\n<p>距离衡量指标：</p>\n<ul>\n<li>ward距离</li>\n<li>平方残差和：值越小，两个簇越可以合成一个簇</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>层次聚类灵活，但是计算复杂度比较高，离群点影响比较大</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.cluster import AgglomerativeClustering  </span><br><span class=\"line\">AgglomerativeClustering(n_clusters &#x3D; 2,linkage&#x3D;&quot;ward&quot;) &#x2F;&#x2F;分成两类</span><br></pre></td></tr></table></figure>\n\n<h6 id=\"DBSCAN\"><a href=\"#DBSCAN\" class=\"headerlink\" title=\"DBSCAN\"></a>DBSCAN</h6><p>算法思想：一定区域内，密度达到一定程度才是一个类，否则是离群点。</p>\n<p>基本概念：</p>\n<ul>\n<li>E邻域：给定对象半径为 E 内的区域称为该对象的 E邻域</li>\n<li>核心对象：如果给定对象 E邻域内的样本点大于或等于 MinPts，则称该对象为核心对象</li>\n<li>直接密度可达：对于样本集合 D，如果样本点 q在 p的E邻域内，并且 p为核心对象，那么对象 q从对象 p直接密度可达。</li>\n<li>密度可达：对于样本集合 D，给定一串样本点 p1、p2、…、pn, p = p1, q = pn,假设对象 pi从 pi-1直接密度可达，那么对象 q从对象 p密度可达</li>\n<li>密度相连：存在样本集合 D中的一点 o，如果对象 o到对象 p和对象 q都是密度可达的，那么 p和 q密度相连</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>DBSCAN算法就是找到密度相连对象的最大集合</li>\n<li>DBSCAN算法优点：对离群点不敏感</li>\n<li>缺点：<ul>\n<li>计算相邻两个点之间的点不容易<ul>\n<li>借助 KD-Tree等数据结构的辅助</li>\n</ul>\n</li>\n<li>需要指定两个参数：E 、 MinPts<ul>\n<li>多尝试</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，供参考，具体见官网  </span><br><span class=\"line\">from sklearn.cluster import DBSCAN  </span><br><span class=\"line\">DBSCAN(min_samples &#x3D; 3,eps&#x3D; 5) &#x2F;&#x2F;min_samples：最小点数；eps:E邻域</span><br></pre></td></tr></table></figure>\n\n<h6 id=\"图分裂\"><a href=\"#图分裂\" class=\"headerlink\" title=\"图分裂\"></a>图分裂</h6><p>实现步骤：</p>\n<ul>\n<li>根据坐标点位置距离关系形成连通图(可采用DBSCAN等算法思路找到最大范围的点数，然后用边连接起来)</li>\n<li>将形成的多个连通图进行逐一分裂</li>\n</ul>\n<p>分类的依据：</p>\n<ul>\n<li>承受系数t</li>\n<li>分裂阈值$\\lambda$</li>\n<li>若 t &gt; $\\lambda$,则将该组边切分</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>与基于层次的聚类思路相反，是从顶至下</li>\n<li>图建立方式、分裂方式非常灵活</li>\n</ul>\n<h5 id=\"关联\"><a href=\"#关联\" class=\"headerlink\" title=\"关联\"></a>关联</h5><p>基本概念：</p>\n<ul>\n<li>项目：一个字段，对交易来说一般是指一次交易中的一个物品，如：尿布</li>\n<li>事物：某个客户在一次交易中，发生的所有项目的集合，如：{尿布，啤酒}</li>\n<li>项集：包含若干个项目的集合(一次事务中)</li>\n<li>频繁项集：某个项集的支持度大于设定阈值(人为设定或者根据数据分布出经验来定)，即称这个项集为频繁项集</li>\n<li>支持度：项集{x, y}在总项集中出现的频率(support)</li>\n<li>置信度：在先决条件 x发生的条件下，有关联规则{x -&gt; y}推出 y 的概率(Confidence)</li>\n<li>提升度：表示含有 x的条件下同时含有 y的概率，与无论含不含 x都含有 y的概率之比。(Confidence({x} -&gt; {y}) / support({y}))。若提升度大于1，则表示购买 x对购买 y有提升作用。</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>两个频繁项集组成的项集不一定是频繁项集</li>\n<li>两个非频繁项集组成的项集一定不是频繁项集</li>\n<li>一个频繁项集和一个非频繁项集组成的项集一定不是频繁项集</li>\n</ul>\n<p>关联可分为两类：</p>\n<ul>\n<li>关联规则：反映一个事物与其他事物之间的相互依存性和关联性，如Apriori算法</li>\n<li>序列规则：与关联规则相似。不同的是将时间因素考虑进来，剔除关联规则中时间点靠后的项对时间点靠前的项的支持，如Apriori-All算法</li>\n</ul>\n<h6 id=\"Apriori\"><a href=\"#Apriori\" class=\"headerlink\" title=\"Apriori\"></a>Apriori</h6><p>算法思想：先指定支持度的阈值，若一个项集的支持度大于这个阈值，则称其为频繁集，然后找出频繁项集。</p>\n<p>找出频繁集的方法：</p>\n<ul>\n<li>先找出一频繁项集，去掉一非频繁项集</li>\n<li>然后将一频繁项集组成二频繁项集，根据阈值去掉二非频繁项集</li>\n<li>再将一频繁项集和二频繁项集组成三频繁项集，根据阈值去掉三非频繁项集</li>\n<li>以此类推</li>\n<li>直至找出最高阶的频繁项集，所有组合遍历完毕，整理全部项集</li>\n</ul>\n<h6 id=\"Apriori-All\"><a href=\"#Apriori-All\" class=\"headerlink\" title=\"Apriori-All\"></a>Apriori-All</h6><p>适用场景：预测用户在购买某种东西后，下次购买时还会买其他什么东西作为搭配</p>\n<p>实现步骤：</p>\n<ul>\n<li>Forward: Apriori</li>\n<li>Backward: 去掉时间序列之后的项对之前的项的支持</li>\n</ul>\n<p>注意事项：</p>\n<ul>\n<li>sklearn中不支持序列规则，自己写</li>\n</ul>\n<h4 id=\"半监督学习\"><a href=\"#半监督学习\" class=\"headerlink\" title=\"半监督学习\"></a>半监督学习</h4><p>适用于样本集部分有标注，部分无标注的情况。通常无标注样本数量远大于有标注样本的数量。</p>\n<p>原因：获取标注的成本较大；无标注样本可能很容易获得。</p>\n<p>算法思路：</p>\n<ul>\n<li>生成模型思路：先对所有有标注的样本计算出一个分布，然后判别无标注的样本如何标注。也可采用分批迭代的方式，如先将与有标注样本比较近的样本进行标注，然后调整分布，在标注接下来的样本。</li>\n<li>判别模型的思路。也就是指物以类聚，如标签传播算法。</li>\n</ul>\n<p>常用算法：标签传播算法</p>\n<h5 id=\"标签传播算法\"><a href=\"#标签传播算法\" class=\"headerlink\" title=\"标签传播算法\"></a>标签传播算法</h5><p>算法思想：根据没有标注的样本和周围有标注的样本进行相似度比较，相似度高的将其标注为临近的标注。<br>其中传播是指迭代由近及远的过程。相似度判别方法：KNN、RBF等</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;python实现，参数自行查看sklearn官网  </span><br><span class=\"line\">from sklearn.semi_supervised import LabelPropagation  </span><br><span class=\"line\">LabelPropagation( ).fit( ).predict( )</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"深度学习\"><a href=\"#深度学习\" class=\"headerlink\" title=\"深度学习\"></a>深度学习</h4><p>暂略</p>\n<hr>\n<h2 id=\"模型评估方法\"><a href=\"#模型评估方法\" class=\"headerlink\" title=\"模型评估方法\"></a>模型评估方法</h2><p>python中用法：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.metrics import accuracy_score,recall_score,f1_score  </span><br><span class=\"line\">&#x2F;&#x2F;准确率  </span><br><span class=\"line\">print(&quot;ACC&quot;,accuracy_score(Y_validation,Ypred))  </span><br><span class=\"line\">&#x2F;&#x2F;召回率  </span><br><span class=\"line\">print(&quot;REC&quot;,recall_score(Y_validation,Ypred))  </span><br><span class=\"line\">&#x2F;&#x2F;F值  </span><br><span class=\"line\">print(&quot;F_score&quot;,f1_score(Y_validation,Ypred))</span><br></pre></td></tr></table></figure>\n<h3 id=\"分类模型的常用评价指标\"><a href=\"#分类模型的常用评价指标\" class=\"headerlink\" title=\"分类模型的常用评价指标\"></a>分类模型的常用评价指标</h3><h4 id=\"二分类评价指标\"><a href=\"#二分类评价指标\" class=\"headerlink\" title=\"二分类评价指标\"></a>二分类评价指标</h4><p>分正类(1)和负类(0)<br><code>基本指标</code></p>\n<ul>\n<li>误差率：错分类样本占总体样本的比例</li>\n<li>准确率(正确率)(Accuracy Rate)：正确分类样本占总体样本的比例</li>\n</ul>\n<p><code>混淆矩阵</code><br>| 真实情况 | 预测为正例 | 预测为负例<br>| —- | —- | —- |<br>| 正例 | TP(真正例) | FN(假反例) 漏<br>| 负例 | FP(假正例) 错 | TN(真反例)</p>\n<p><code>衍生指标</code></p>\n<ul>\n<li>查准率(precision):所有真正例占所有预测为正的样本的比例</li>\n<li>查全率(招回率,Recall,TPR):所有真正例占所有真实为正的样本的比例</li>\n<li>F-measure(F-score):</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2 * Recall * Accuracy &#x2F; (Reacll + Accuracy)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>错误接收率(FPR,False Postive Rate):</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FP &#x2F; (FP + TN)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>错误拒绝率(FRR,False Rejction Rate):</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FN &#x2F; (TP + FN)</span><br></pre></td></tr></table></figure>\n<h4 id=\"多分类评价指标\"><a href=\"#多分类评价指标\" class=\"headerlink\" title=\"多分类评价指标\"></a>多分类评价指标</h4><ul>\n<li>多元混淆矩阵</li>\n<li>准确率：同二分类</li>\n<li>召回率与F值：两种思路处理：<ul>\n<li>先计算所有的TP、FN等值，再以二值方法计算</li>\n<li>分别把每个分类当作正类，各计算一个召回率或F值，然后取加权或不加权的平均值</li>\n</ul>\n</li>\n<li>ROC曲线与AUC值：衡量分类效果，并且可以限定阈值<ul>\n<li>ROC曲线：以召回率(TPR)为纵轴，错误接收率(FPR)为横轴，采用不同的截断点，绘制ROC曲线。ROC曲线能够很容易地查出任意界限值对性能的识别能力。</li>\n<li>AUC值：ROC曲线与坐标轴构成的图形面积。AUC值越接近1，说明越准确。</li>\n</ul>\n</li>\n<li>增益图和KS图：衡量分类效果<ul>\n<li>增益图：宏观上反映分类器的效果</li>\n<li>KS图：反映对正类样本份额例的</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"回归模型的常用评价指标\"><a href=\"#回归模型的常用评价指标\" class=\"headerlink\" title=\"回归模型的常用评价指标\"></a>回归模型的常用评价指标</h3><ul>\n<li>样本误差：衡量模型在一个样本上的预测准确度<ul>\n<li>样本误差 = 样本预测值 - 样本实际值</li>\n</ul>\n</li>\n<li>平均误差方(MSE)：最常用的评价指标<ul>\n<li>所有样本的样本误差的平方和的均值。MSE越接近0，模型越准确。</li>\n</ul>\n</li>\n<li>平均绝对误差(MAE)：较好解释的评价指标<ul>\n<li>所有样本的样本误差的绝对值的均值。MAE的单位与因变量的单位一致，其越接近0，模型越准确。</li>\n</ul>\n</li>\n<li>平均绝对比例误差(MAPE)：平均绝对误差的衍生指标<ul>\n<li>所有样本的样本误差的绝对值占实际值的比例。指标越接近0，模型越准确。</li>\n</ul>\n</li>\n<li>决定系数： R2-score<ul>\n<li>因变量的方差能被自变量解释的程度。指标越接近1，则代表自变量对于因变量的解释程度越高。通常 &gt;0.5 ，就还不错。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"聚类模型的常用评价指标\"><a href=\"#聚类模型的常用评价指标\" class=\"headerlink\" title=\"聚类模型的常用评价指标\"></a>聚类模型的常用评价指标</h3><ul>\n<li>RMS(Root Mean Square)：值越小，分类效果越好<br>  $$ RMS = \\frac{1}{n} \\sqrt{\\sum_{i=0}^{n} (x_i - \\bar{x})^{2}}$$</li>\n<li>轮廓系数<ul>\n<li>a(i)为样本i与簇内其他样本的平均距离，也称为内聚度</li>\n<li>b(i)为样本i与其他某簇样本的平均距离，也称为分离度</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;s(i)越接近1，分类效果越好；越接近-1，分类效果最差  </span><br><span class=\"line\">s(i) &#x3D; (b(i) - a(i)) &#x2F; max&#123;a(i),b(i)&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"关联模型的常用评价指标\"><a href=\"#关联模型的常用评价指标\" class=\"headerlink\" title=\"关联模型的常用评价指标\"></a>关联模型的常用评价指标</h2><ul>\n<li>支持度：项集{x, y}在总项集中出现的频率(support)</li>\n<li>置信度：在先决条件 x发生的条件下，有关联规则{x -&gt; y}推出 y 的概率(Confidence)</li>\n<li>提升度：表示含有 x的条件下同时含有 y的概率，与无论含不含 x都含有 y的概率之比。(Confidence({x} -&gt; {y}) / support({y}))。若提升度大于1，则表示购买 x对购买 y有提升作用。</li>\n</ul>\n<h2 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h2><p>python中保存和加载模型的方式：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.externals import joblib  </span><br><span class=\"line\">&#x2F;&#x2F;保存模型  </span><br><span class=\"line\">joblib.dump(knn_clf,&quot;knn_clf&quot;)  </span><br><span class=\"line\">&#x2F;&#x2F;加载模型  </span><br><span class=\"line\">joblib.load(knn_clf,&quot;knn_clf&quot;)</span><br></pre></td></tr></table></figure>\n<h2 id=\"后续\"><a href=\"#后续\" class=\"headerlink\" title=\"后续\"></a>后续</h2><blockquote>\n<p>注：本文持续更新中</p>\n</blockquote>\n"},{"title":"排序算法总结（内含python实现）","date":"2022-04-23T12:36:33.000Z","_content":"\n本文总结了常用的四大类别的算法：插入排序（直接插入排序，希尔排序）、交换排序（冒泡排序，快速排序）、选择排序（直接选择排序，堆排序）、归并排序，并用python实现。\n\n\n> 注：其中一些排序算法可稳定，也可不稳定，关键在于在处理相同的元素时，跳过还是依次遍历。\n> 注：下列代码实现为升序排序。\n\n## 插入排序\n\n#### 直接插入排序\n\n``` python\n# 从左往右遍历，依次将当前元素插入到前面已经有序的序列中\n# 稳定\n# 时间复杂度： O(n^2)\n# 空间复杂度： O(1)\n# 改进：可使用二分法插入排序（暂略）\n\ndef insert_sort(nums: list) -> list:\n    # 从左往右开始遍历\n    for i in range(1, len(nums)):\n        index = i\n        temp = nums[i]\n        while index > 0 and nums[index - 1] > temp:\n            # 若当前元素较小，则将有序的序列向后移\n            nums[index] = nums[index - 1]\n            index -= 1\n        # 找到正确的位置，赋值\n        nums[index] = temp\n    return nums\n\nnums = [1, 2, 4, 9, 3, 55, 64, 25]\nprint(insert_sort(nums))\n```\n\n#### 希尔排序\n\n``` python\n# 在直接插入的基础上， 分组（间隔为n的为一组，这里初始值设置为数据的一半）依次插入，然后将n缩小一半，依次插入，如此迭代，直至 n 为 0\n# 这样做的好处在于，避免直接插入时，若元素为逆序，需要过多次的插入和元素移动操作\n# 不稳定\n# 时间复杂度： \n# 空间复杂度： O(1)\n\ndef shell_sort(nums: list) -> list:\n    # 初始分组设置为数组的一半\n    group = 2\n    n = len(nums)\n    gap = n // group\n    while gap > 0:\n        for i in range(gap, n):\n            temp = nums[i]\n            j = i\n            # 从右往左，依次插入\n            while j >= gap and nums[j - gap] > temp:\n                nums[j] = nums[j - gap]\n                j -= gap\n            nums[j] = temp\n        # 分组长度减半\n        gap //= group\n    return nums\n\nnums = [1, 2, 4, 9, 3, 55, 64, 25]\nprint(shell_sort(nums))\n```\n\n## 交换排序\n\n#### 冒泡排序\n\n``` python\n# 相邻两进行比较，每一轮将最大值移到最后\n# 不稳定\n# 时间复杂度： O(n^2)\n# 空间复杂度： O(1)\n\ndef bubble_sort(nums: list) -> list:\n    # 每轮迭代，将最大值移到最后，故待排序序列长度减一\n    for end in range(len(nums) - 1, 1, -1):\n        # 从左往右依次两两比较，将较大值移到后面\n        for i in range(1, end):\n            if nums[i - 1] > nums[i]:\n                nums[i - 1], nums[i] = nums[i], nums[i - 1]\n    return nums\n\nnums = [1, 2, 4, 9, 3, 55, 64, 25]\nprint(bubble_sort(nums))\n```\n\n#### 快速排序\n\n``` python\n# 选择一个值，将比它大的移动右边，比它小的移到左边,故该值的位置已经找到；然后分别在左边和右边的序列中，继续选择一个值、划分，直至划分的序列中只有一个值\n# 稳定\n# 时间复杂度： O(nlogn)\n# 空间复杂度： O(1)\n\n# 找出选择的值的位置\n# 这里默认取序列第一个元素，来划分左右序列\ndef partition(nums: list, left: int, right: int) -> int:\n    targt = nums[left]\n    mid, fast = left, right\n    while mid != fast:\n        if nums[fast] > targt and fast > mid:\n            fast -= 1\n        elif nums[fast] < targt and fast < mid:\n            fast += 1\n        else:\n            nums[fast], nums[mid] = nums[mid], nums[fast]\n            mid, fast = fast, mid\n            if fast >= mid:\n                fast -= 1\n            elif fast < mid:\n                fast += 1\n    return mid\n\n# 递归实现： 划分左右序列\ndef quick_sort(nums, left, right):\n    if not nums: return nums\n    mid = partition(nums, left, right)\n    if left < mid - 1: quick_sort(nums, left, mid - 1)\n    if right > mid + 1: quick_sort(nums, mid + 1, right)\n    return nums\n\n\nnums = [1, 5, 4, 9, 3, 55, 64, 25]\nprint(quick_sort(0, len(nums) - 1, nums))\n```\n\n## 选择排序\n\n#### 直接选择排序\n\n``` python\n# 确定最小值或最大值的位置，最后才交换\n# 稳定\n# 时间复杂度： O(n^2)\n# 空间复杂度： O(1)\n\ndef select_sort(nums: list) -> list:\n    n = len(nums)\n    # 从左往右依次遍历\n    for start in range(n - 1):\n        index = start\n        # 确定比当前元素小的元素的位置\n        for i in range(start + 1, n):\n            if nums[index] > nums[i]:\n                index = i\n        # 赋值\n        nums[start], nums[index] = nums[index], nums[start]\n    return nums\n\nnums = [1, 2, 4, 9, 3, 55, 64, 25]\nprint(select_sort(nums))    \n```\n\n#### 堆排序\n\n``` python\n# 从下往上，从左往右，创建最大堆\n# 最大堆特征：父亲节点比其左右子树的节点的值都大\n# \n# 时间复杂度： \n# 空间复杂度： O(1)\n\n# 创建堆\ndef bulid_heap(nums: list, head: int, end: int) -> list:\n    larget = head\n    # 当前节点的左孩子\n    left = 2 * head + 1\n    # 当前节点的右孩子\n    right = 2 * head + 2\n    # 找到左右孩子中，比父亲节点大的节点的位置\n    if left < end and nums[left] > nums[larget]:\n        larget = left\n    if right < end and nums[right] > nums[larget]:\n        larget = right\n    # 若孩子节点比父亲节点的值大，则交换\n    if larget != head:\n        nums[larget], nums[head] = nums[head], nums[larget]\n        # 被交换的孩子节点作为父节点，继续向其孩子节点遍历\n        bulid_heap(nums, larget, end)\n    return nums\n\n# 堆排序\ndef heap_sort(nums):\n    n = len(nums)\n    # 创建堆，从右往左依次插入节点\n    for i in range(n - 1, -1, -1):\n        bulid_heap(nums, i, n)\n    # 排序，将最大值和末尾节点进行交换，然后将末尾节点之前的序列继续构建最大堆；如此遍历\n    for j in range(n - 1, 0, -1):\n        nums[0], nums[j] = nums[j], nums[0]\n        bulid_heap(nums, 0, j)\n    return nums\n\n\nnums = [1, 2, 4, 9, 3, 55, 64, 25]\nprint(heap_sort(nums)) \n```\n\n## 归并排序\n\n``` python\n# 分治的思想\n# \n# 时间复杂度： \n# 空间复杂度： O(n)\n\n# 将两个有序的序列，依次从左往右比较，将较小的值插入新的序列，最后合成一个有序的序列\ndef merge(nums, left, mid, right):\n    numsa = nums[left:mid + 1]\n    numsb = nums[mid + 1: right + 1]\n    result = list()\n    p, q = 0, 0\n    # 两数组分别开始比较\n    while p < len(numsa) and q < len(numsb):\n        if numsa[p] >= numsb[q]:\n            result.append(numsb[q])\n            q += 1\n        else:\n            result.append(numsa[p])\n            p += 1\n    #若其中一个数组还有值，则追加后末尾\n    result.extend(numsa[p:])\n    result.extend(numsb[q:])\n    nums[left:right + 1] = result\n\n# 递归拆分\n# 没有返回值，传入数组，默认在此词数组上移动排序\ndef merge_sort(nums: list, start: int, end: int):\n    if start < end:\n        # 默认是对半拆分\n        mid = (start + end) // 2\n        merge_sort(nums, start, mid)\n        merge_sort(nums, mid + 1, end)\n        merge(nums, start, mid, end)\n\nnums = [1, 2, 4, 9, 3, 55, 64, 25]\nmerge_sort(nums, 0, len(nums) - 1)\nprint(nums)\n```\n","source":"_posts/algorithm-sort.md","raw":"---\ntitle: 排序算法总结（内含python实现）\ndate: 2022-04-23 20:36:33\ntags: sort algorithm python\ncategories: 算法\n---\n\n本文总结了常用的四大类别的算法：插入排序（直接插入排序，希尔排序）、交换排序（冒泡排序，快速排序）、选择排序（直接选择排序，堆排序）、归并排序，并用python实现。\n\n\n> 注：其中一些排序算法可稳定，也可不稳定，关键在于在处理相同的元素时，跳过还是依次遍历。\n> 注：下列代码实现为升序排序。\n\n## 插入排序\n\n#### 直接插入排序\n\n``` python\n# 从左往右遍历，依次将当前元素插入到前面已经有序的序列中\n# 稳定\n# 时间复杂度： O(n^2)\n# 空间复杂度： O(1)\n# 改进：可使用二分法插入排序（暂略）\n\ndef insert_sort(nums: list) -> list:\n    # 从左往右开始遍历\n    for i in range(1, len(nums)):\n        index = i\n        temp = nums[i]\n        while index > 0 and nums[index - 1] > temp:\n            # 若当前元素较小，则将有序的序列向后移\n            nums[index] = nums[index - 1]\n            index -= 1\n        # 找到正确的位置，赋值\n        nums[index] = temp\n    return nums\n\nnums = [1, 2, 4, 9, 3, 55, 64, 25]\nprint(insert_sort(nums))\n```\n\n#### 希尔排序\n\n``` python\n# 在直接插入的基础上， 分组（间隔为n的为一组，这里初始值设置为数据的一半）依次插入，然后将n缩小一半，依次插入，如此迭代，直至 n 为 0\n# 这样做的好处在于，避免直接插入时，若元素为逆序，需要过多次的插入和元素移动操作\n# 不稳定\n# 时间复杂度： \n# 空间复杂度： O(1)\n\ndef shell_sort(nums: list) -> list:\n    # 初始分组设置为数组的一半\n    group = 2\n    n = len(nums)\n    gap = n // group\n    while gap > 0:\n        for i in range(gap, n):\n            temp = nums[i]\n            j = i\n            # 从右往左，依次插入\n            while j >= gap and nums[j - gap] > temp:\n                nums[j] = nums[j - gap]\n                j -= gap\n            nums[j] = temp\n        # 分组长度减半\n        gap //= group\n    return nums\n\nnums = [1, 2, 4, 9, 3, 55, 64, 25]\nprint(shell_sort(nums))\n```\n\n## 交换排序\n\n#### 冒泡排序\n\n``` python\n# 相邻两进行比较，每一轮将最大值移到最后\n# 不稳定\n# 时间复杂度： O(n^2)\n# 空间复杂度： O(1)\n\ndef bubble_sort(nums: list) -> list:\n    # 每轮迭代，将最大值移到最后，故待排序序列长度减一\n    for end in range(len(nums) - 1, 1, -1):\n        # 从左往右依次两两比较，将较大值移到后面\n        for i in range(1, end):\n            if nums[i - 1] > nums[i]:\n                nums[i - 1], nums[i] = nums[i], nums[i - 1]\n    return nums\n\nnums = [1, 2, 4, 9, 3, 55, 64, 25]\nprint(bubble_sort(nums))\n```\n\n#### 快速排序\n\n``` python\n# 选择一个值，将比它大的移动右边，比它小的移到左边,故该值的位置已经找到；然后分别在左边和右边的序列中，继续选择一个值、划分，直至划分的序列中只有一个值\n# 稳定\n# 时间复杂度： O(nlogn)\n# 空间复杂度： O(1)\n\n# 找出选择的值的位置\n# 这里默认取序列第一个元素，来划分左右序列\ndef partition(nums: list, left: int, right: int) -> int:\n    targt = nums[left]\n    mid, fast = left, right\n    while mid != fast:\n        if nums[fast] > targt and fast > mid:\n            fast -= 1\n        elif nums[fast] < targt and fast < mid:\n            fast += 1\n        else:\n            nums[fast], nums[mid] = nums[mid], nums[fast]\n            mid, fast = fast, mid\n            if fast >= mid:\n                fast -= 1\n            elif fast < mid:\n                fast += 1\n    return mid\n\n# 递归实现： 划分左右序列\ndef quick_sort(nums, left, right):\n    if not nums: return nums\n    mid = partition(nums, left, right)\n    if left < mid - 1: quick_sort(nums, left, mid - 1)\n    if right > mid + 1: quick_sort(nums, mid + 1, right)\n    return nums\n\n\nnums = [1, 5, 4, 9, 3, 55, 64, 25]\nprint(quick_sort(0, len(nums) - 1, nums))\n```\n\n## 选择排序\n\n#### 直接选择排序\n\n``` python\n# 确定最小值或最大值的位置，最后才交换\n# 稳定\n# 时间复杂度： O(n^2)\n# 空间复杂度： O(1)\n\ndef select_sort(nums: list) -> list:\n    n = len(nums)\n    # 从左往右依次遍历\n    for start in range(n - 1):\n        index = start\n        # 确定比当前元素小的元素的位置\n        for i in range(start + 1, n):\n            if nums[index] > nums[i]:\n                index = i\n        # 赋值\n        nums[start], nums[index] = nums[index], nums[start]\n    return nums\n\nnums = [1, 2, 4, 9, 3, 55, 64, 25]\nprint(select_sort(nums))    \n```\n\n#### 堆排序\n\n``` python\n# 从下往上，从左往右，创建最大堆\n# 最大堆特征：父亲节点比其左右子树的节点的值都大\n# \n# 时间复杂度： \n# 空间复杂度： O(1)\n\n# 创建堆\ndef bulid_heap(nums: list, head: int, end: int) -> list:\n    larget = head\n    # 当前节点的左孩子\n    left = 2 * head + 1\n    # 当前节点的右孩子\n    right = 2 * head + 2\n    # 找到左右孩子中，比父亲节点大的节点的位置\n    if left < end and nums[left] > nums[larget]:\n        larget = left\n    if right < end and nums[right] > nums[larget]:\n        larget = right\n    # 若孩子节点比父亲节点的值大，则交换\n    if larget != head:\n        nums[larget], nums[head] = nums[head], nums[larget]\n        # 被交换的孩子节点作为父节点，继续向其孩子节点遍历\n        bulid_heap(nums, larget, end)\n    return nums\n\n# 堆排序\ndef heap_sort(nums):\n    n = len(nums)\n    # 创建堆，从右往左依次插入节点\n    for i in range(n - 1, -1, -1):\n        bulid_heap(nums, i, n)\n    # 排序，将最大值和末尾节点进行交换，然后将末尾节点之前的序列继续构建最大堆；如此遍历\n    for j in range(n - 1, 0, -1):\n        nums[0], nums[j] = nums[j], nums[0]\n        bulid_heap(nums, 0, j)\n    return nums\n\n\nnums = [1, 2, 4, 9, 3, 55, 64, 25]\nprint(heap_sort(nums)) \n```\n\n## 归并排序\n\n``` python\n# 分治的思想\n# \n# 时间复杂度： \n# 空间复杂度： O(n)\n\n# 将两个有序的序列，依次从左往右比较，将较小的值插入新的序列，最后合成一个有序的序列\ndef merge(nums, left, mid, right):\n    numsa = nums[left:mid + 1]\n    numsb = nums[mid + 1: right + 1]\n    result = list()\n    p, q = 0, 0\n    # 两数组分别开始比较\n    while p < len(numsa) and q < len(numsb):\n        if numsa[p] >= numsb[q]:\n            result.append(numsb[q])\n            q += 1\n        else:\n            result.append(numsa[p])\n            p += 1\n    #若其中一个数组还有值，则追加后末尾\n    result.extend(numsa[p:])\n    result.extend(numsb[q:])\n    nums[left:right + 1] = result\n\n# 递归拆分\n# 没有返回值，传入数组，默认在此词数组上移动排序\ndef merge_sort(nums: list, start: int, end: int):\n    if start < end:\n        # 默认是对半拆分\n        mid = (start + end) // 2\n        merge_sort(nums, start, mid)\n        merge_sort(nums, mid + 1, end)\n        merge(nums, start, mid, end)\n\nnums = [1, 2, 4, 9, 3, 55, 64, 25]\nmerge_sort(nums, 0, len(nums) - 1)\nprint(nums)\n```\n","slug":"algorithm-sort","published":1,"updated":"2022-04-23T14:41:32.833Z","_id":"cl2bxmehv000082ul1omh4b2e","comments":1,"layout":"post","photos":[],"link":"","content":"<p>本文总结了常用的四大类别的算法：插入排序（直接插入排序，希尔排序）、交换排序（冒泡排序，快速排序）、选择排序（直接选择排序，堆排序）、归并排序，并用python实现。</p>\n<blockquote>\n<p>注：其中一些排序算法可稳定，也可不稳定，关键在于在处理相同的元素时，跳过还是依次遍历。<br>注：下列代码实现为升序排序。</p>\n</blockquote>\n<h2 id=\"插入排序\"><a href=\"#插入排序\" class=\"headerlink\" title=\"插入排序\"></a>插入排序</h2><h4 id=\"直接插入排序\"><a href=\"#直接插入排序\" class=\"headerlink\" title=\"直接插入排序\"></a>直接插入排序</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 从左往右遍历，依次将当前元素插入到前面已经有序的序列中</span></span><br><span class=\"line\"><span class=\"comment\"># 稳定</span></span><br><span class=\"line\"><span class=\"comment\"># 时间复杂度： O(n^2)</span></span><br><span class=\"line\"><span class=\"comment\"># 空间复杂度： O(1)</span></span><br><span class=\"line\"><span class=\"comment\"># 改进：可使用二分法插入排序（暂略）</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insert_sort</span>(<span class=\"params\">nums: <span class=\"built_in\">list</span></span>) -&gt; list:</span></span><br><span class=\"line\">    <span class=\"comment\"># 从左往右开始遍历</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, <span class=\"built_in\">len</span>(nums)):</span><br><span class=\"line\">        index = i</span><br><span class=\"line\">        temp = nums[i]</span><br><span class=\"line\">        <span class=\"keyword\">while</span> index &gt; <span class=\"number\">0</span> <span class=\"keyword\">and</span> nums[index - <span class=\"number\">1</span>] &gt; temp:</span><br><span class=\"line\">            <span class=\"comment\"># 若当前元素较小，则将有序的序列向后移</span></span><br><span class=\"line\">            nums[index] = nums[index - <span class=\"number\">1</span>]</span><br><span class=\"line\">            index -= <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"comment\"># 找到正确的位置，赋值</span></span><br><span class=\"line\">        nums[index] = temp</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums</span><br><span class=\"line\"></span><br><span class=\"line\">nums = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">3</span>, <span class=\"number\">55</span>, <span class=\"number\">64</span>, <span class=\"number\">25</span>]</span><br><span class=\"line\">print(insert_sort(nums))</span><br></pre></td></tr></table></figure>\n<h4 id=\"希尔排序\"><a href=\"#希尔排序\" class=\"headerlink\" title=\"希尔排序\"></a>希尔排序</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 在直接插入的基础上， 分组（间隔为n的为一组，这里初始值设置为数据的一半）依次插入，然后将n缩小一半，依次插入，如此迭代，直至 n 为 0</span></span><br><span class=\"line\"><span class=\"comment\"># 这样做的好处在于，避免直接插入时，若元素为逆序，需要过多次的插入和元素移动操作</span></span><br><span class=\"line\"><span class=\"comment\"># 不稳定</span></span><br><span class=\"line\"><span class=\"comment\"># 时间复杂度： </span></span><br><span class=\"line\"><span class=\"comment\"># 空间复杂度： O(1)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">shell_sort</span>(<span class=\"params\">nums: <span class=\"built_in\">list</span></span>) -&gt; list:</span></span><br><span class=\"line\">    <span class=\"comment\"># 初始分组设置为数组的一半</span></span><br><span class=\"line\">    group = <span class=\"number\">2</span></span><br><span class=\"line\">    n = <span class=\"built_in\">len</span>(nums)</span><br><span class=\"line\">    gap = n // group</span><br><span class=\"line\">    <span class=\"keyword\">while</span> gap &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(gap, n):</span><br><span class=\"line\">            temp = nums[i]</span><br><span class=\"line\">            j = i</span><br><span class=\"line\">            <span class=\"comment\"># 从右往左，依次插入</span></span><br><span class=\"line\">            <span class=\"keyword\">while</span> j &gt;= gap <span class=\"keyword\">and</span> nums[j - gap] &gt; temp:</span><br><span class=\"line\">                nums[j] = nums[j - gap]</span><br><span class=\"line\">                j -= gap</span><br><span class=\"line\">            nums[j] = temp</span><br><span class=\"line\">        <span class=\"comment\"># 分组长度减半</span></span><br><span class=\"line\">        gap //= group</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums</span><br><span class=\"line\"></span><br><span class=\"line\">nums = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">3</span>, <span class=\"number\">55</span>, <span class=\"number\">64</span>, <span class=\"number\">25</span>]</span><br><span class=\"line\">print(shell_sort(nums))</span><br></pre></td></tr></table></figure>\n<h2 id=\"交换排序\"><a href=\"#交换排序\" class=\"headerlink\" title=\"交换排序\"></a>交换排序</h2><h4 id=\"冒泡排序\"><a href=\"#冒泡排序\" class=\"headerlink\" title=\"冒泡排序\"></a>冒泡排序</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 相邻两进行比较，每一轮将最大值移到最后</span></span><br><span class=\"line\"><span class=\"comment\"># 不稳定</span></span><br><span class=\"line\"><span class=\"comment\"># 时间复杂度： O(n^2)</span></span><br><span class=\"line\"><span class=\"comment\"># 空间复杂度： O(1)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bubble_sort</span>(<span class=\"params\">nums: <span class=\"built_in\">list</span></span>) -&gt; list:</span></span><br><span class=\"line\">    <span class=\"comment\"># 每轮迭代，将最大值移到最后，故待排序序列长度减一</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> end <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(nums) - <span class=\"number\">1</span>, <span class=\"number\">1</span>, -<span class=\"number\">1</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 从左往右依次两两比较，将较大值移到后面</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, end):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> nums[i - <span class=\"number\">1</span>] &gt; nums[i]:</span><br><span class=\"line\">                nums[i - <span class=\"number\">1</span>], nums[i] = nums[i], nums[i - <span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums</span><br><span class=\"line\"></span><br><span class=\"line\">nums = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">3</span>, <span class=\"number\">55</span>, <span class=\"number\">64</span>, <span class=\"number\">25</span>]</span><br><span class=\"line\">print(bubble_sort(nums))</span><br></pre></td></tr></table></figure>\n<h4 id=\"快速排序\"><a href=\"#快速排序\" class=\"headerlink\" title=\"快速排序\"></a>快速排序</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 选择一个值，将比它大的移动右边，比它小的移到左边,故该值的位置已经找到；然后分别在左边和右边的序列中，继续选择一个值、划分，直至划分的序列中只有一个值</span></span><br><span class=\"line\"><span class=\"comment\"># 稳定</span></span><br><span class=\"line\"><span class=\"comment\"># 时间复杂度： O(nlogn)</span></span><br><span class=\"line\"><span class=\"comment\"># 空间复杂度： O(1)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 找出选择的值的位置</span></span><br><span class=\"line\"><span class=\"comment\"># 这里默认取序列第一个元素，来划分左右序列</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partition</span>(<span class=\"params\">nums: <span class=\"built_in\">list</span>, left: <span class=\"built_in\">int</span>, right: <span class=\"built_in\">int</span></span>) -&gt; int:</span></span><br><span class=\"line\">    targt = nums[left]</span><br><span class=\"line\">    mid, fast = left, right</span><br><span class=\"line\">    <span class=\"keyword\">while</span> mid != fast:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> nums[fast] &gt; targt <span class=\"keyword\">and</span> fast &gt; mid:</span><br><span class=\"line\">            fast -= <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> nums[fast] &lt; targt <span class=\"keyword\">and</span> fast &lt; mid:</span><br><span class=\"line\">            fast += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            nums[fast], nums[mid] = nums[mid], nums[fast]</span><br><span class=\"line\">            mid, fast = fast, mid</span><br><span class=\"line\">            <span class=\"keyword\">if</span> fast &gt;= mid:</span><br><span class=\"line\">                fast -= <span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">elif</span> fast &lt; mid:</span><br><span class=\"line\">                fast += <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> mid</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 递归实现： 划分左右序列</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">quick_sort</span>(<span class=\"params\">nums, left, right</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> nums: <span class=\"keyword\">return</span> nums</span><br><span class=\"line\">    mid = partition(nums, left, right)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> left &lt; mid - <span class=\"number\">1</span>: quick_sort(nums, left, mid - <span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> right &gt; mid + <span class=\"number\">1</span>: quick_sort(nums, mid + <span class=\"number\">1</span>, right)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">nums = [<span class=\"number\">1</span>, <span class=\"number\">5</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">3</span>, <span class=\"number\">55</span>, <span class=\"number\">64</span>, <span class=\"number\">25</span>]</span><br><span class=\"line\">print(quick_sort(<span class=\"number\">0</span>, <span class=\"built_in\">len</span>(nums) - <span class=\"number\">1</span>, nums))</span><br></pre></td></tr></table></figure>\n<h2 id=\"选择排序\"><a href=\"#选择排序\" class=\"headerlink\" title=\"选择排序\"></a>选择排序</h2><h4 id=\"直接选择排序\"><a href=\"#直接选择排序\" class=\"headerlink\" title=\"直接选择排序\"></a>直接选择排序</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 确定最小值或最大值的位置，最后才交换</span></span><br><span class=\"line\"><span class=\"comment\"># 稳定</span></span><br><span class=\"line\"><span class=\"comment\"># 时间复杂度： O(n^2)</span></span><br><span class=\"line\"><span class=\"comment\"># 空间复杂度： O(1)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">select_sort</span>(<span class=\"params\">nums: <span class=\"built_in\">list</span></span>) -&gt; list:</span></span><br><span class=\"line\">    n = <span class=\"built_in\">len</span>(nums)</span><br><span class=\"line\">    <span class=\"comment\"># 从左往右依次遍历</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> start <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n - <span class=\"number\">1</span>):</span><br><span class=\"line\">        index = start</span><br><span class=\"line\">        <span class=\"comment\"># 确定比当前元素小的元素的位置</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(start + <span class=\"number\">1</span>, n):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> nums[index] &gt; nums[i]:</span><br><span class=\"line\">                index = i</span><br><span class=\"line\">        <span class=\"comment\"># 赋值</span></span><br><span class=\"line\">        nums[start], nums[index] = nums[index], nums[start]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums</span><br><span class=\"line\"></span><br><span class=\"line\">nums = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">3</span>, <span class=\"number\">55</span>, <span class=\"number\">64</span>, <span class=\"number\">25</span>]</span><br><span class=\"line\">print(select_sort(nums))    </span><br></pre></td></tr></table></figure>\n<h4 id=\"堆排序\"><a href=\"#堆排序\" class=\"headerlink\" title=\"堆排序\"></a>堆排序</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 从下往上，从左往右，创建最大堆</span></span><br><span class=\"line\"><span class=\"comment\"># 最大堆特征：父亲节点比其左右子树的节点的值都大</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># 时间复杂度： </span></span><br><span class=\"line\"><span class=\"comment\"># 空间复杂度： O(1)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建堆</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bulid_heap</span>(<span class=\"params\">nums: <span class=\"built_in\">list</span>, head: <span class=\"built_in\">int</span>, end: <span class=\"built_in\">int</span></span>) -&gt; list:</span></span><br><span class=\"line\">    larget = head</span><br><span class=\"line\">    <span class=\"comment\"># 当前节点的左孩子</span></span><br><span class=\"line\">    left = <span class=\"number\">2</span> * head + <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\"># 当前节点的右孩子</span></span><br><span class=\"line\">    right = <span class=\"number\">2</span> * head + <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"comment\"># 找到左右孩子中，比父亲节点大的节点的位置</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> left &lt; end <span class=\"keyword\">and</span> nums[left] &gt; nums[larget]:</span><br><span class=\"line\">        larget = left</span><br><span class=\"line\">    <span class=\"keyword\">if</span> right &lt; end <span class=\"keyword\">and</span> nums[right] &gt; nums[larget]:</span><br><span class=\"line\">        larget = right</span><br><span class=\"line\">    <span class=\"comment\"># 若孩子节点比父亲节点的值大，则交换</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> larget != head:</span><br><span class=\"line\">        nums[larget], nums[head] = nums[head], nums[larget]</span><br><span class=\"line\">        <span class=\"comment\"># 被交换的孩子节点作为父节点，继续向其孩子节点遍历</span></span><br><span class=\"line\">        bulid_heap(nums, larget, end)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 堆排序</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">heap_sort</span>(<span class=\"params\">nums</span>):</span></span><br><span class=\"line\">    n = <span class=\"built_in\">len</span>(nums)</span><br><span class=\"line\">    <span class=\"comment\"># 创建堆，从右往左依次插入节点</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n - <span class=\"number\">1</span>, -<span class=\"number\">1</span>, -<span class=\"number\">1</span>):</span><br><span class=\"line\">        bulid_heap(nums, i, n)</span><br><span class=\"line\">    <span class=\"comment\"># 排序，将最大值和末尾节点进行交换，然后将末尾节点之前的序列继续构建最大堆；如此遍历</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n - <span class=\"number\">1</span>, <span class=\"number\">0</span>, -<span class=\"number\">1</span>):</span><br><span class=\"line\">        nums[<span class=\"number\">0</span>], nums[j] = nums[j], nums[<span class=\"number\">0</span>]</span><br><span class=\"line\">        bulid_heap(nums, <span class=\"number\">0</span>, j)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">nums = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">3</span>, <span class=\"number\">55</span>, <span class=\"number\">64</span>, <span class=\"number\">25</span>]</span><br><span class=\"line\">print(heap_sort(nums)) </span><br></pre></td></tr></table></figure>\n<h2 id=\"归并排序\"><a href=\"#归并排序\" class=\"headerlink\" title=\"归并排序\"></a>归并排序</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 分治的思想</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># 时间复杂度： </span></span><br><span class=\"line\"><span class=\"comment\"># 空间复杂度： O(n)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将两个有序的序列，依次从左往右比较，将较小的值插入新的序列，最后合成一个有序的序列</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">merge</span>(<span class=\"params\">nums, left, mid, right</span>):</span></span><br><span class=\"line\">    numsa = nums[left:mid + <span class=\"number\">1</span>]</span><br><span class=\"line\">    numsb = nums[mid + <span class=\"number\">1</span>: right + <span class=\"number\">1</span>]</span><br><span class=\"line\">    result = <span class=\"built_in\">list</span>()</span><br><span class=\"line\">    p, q = <span class=\"number\">0</span>, <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"comment\"># 两数组分别开始比较</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> p &lt; <span class=\"built_in\">len</span>(numsa) <span class=\"keyword\">and</span> q &lt; <span class=\"built_in\">len</span>(numsb):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> numsa[p] &gt;= numsb[q]:</span><br><span class=\"line\">            result.append(numsb[q])</span><br><span class=\"line\">            q += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            result.append(numsa[p])</span><br><span class=\"line\">            p += <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\">#若其中一个数组还有值，则追加后末尾</span></span><br><span class=\"line\">    result.extend(numsa[p:])</span><br><span class=\"line\">    result.extend(numsb[q:])</span><br><span class=\"line\">    nums[left:right + <span class=\"number\">1</span>] = result</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 递归拆分</span></span><br><span class=\"line\"><span class=\"comment\"># 没有返回值，传入数组，默认在此词数组上移动排序</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">merge_sort</span>(<span class=\"params\">nums: <span class=\"built_in\">list</span>, start: <span class=\"built_in\">int</span>, end: <span class=\"built_in\">int</span></span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> start &lt; end:</span><br><span class=\"line\">        <span class=\"comment\"># 默认是对半拆分</span></span><br><span class=\"line\">        mid = (start + end) // <span class=\"number\">2</span></span><br><span class=\"line\">        merge_sort(nums, start, mid)</span><br><span class=\"line\">        merge_sort(nums, mid + <span class=\"number\">1</span>, end)</span><br><span class=\"line\">        merge(nums, start, mid, end)</span><br><span class=\"line\"></span><br><span class=\"line\">nums = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">3</span>, <span class=\"number\">55</span>, <span class=\"number\">64</span>, <span class=\"number\">25</span>]</span><br><span class=\"line\">merge_sort(nums, <span class=\"number\">0</span>, <span class=\"built_in\">len</span>(nums) - <span class=\"number\">1</span>)</span><br><span class=\"line\">print(nums)</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>本文总结了常用的四大类别的算法：插入排序（直接插入排序，希尔排序）、交换排序（冒泡排序，快速排序）、选择排序（直接选择排序，堆排序）、归并排序，并用python实现。</p>\n<blockquote>\n<p>注：其中一些排序算法可稳定，也可不稳定，关键在于在处理相同的元素时，跳过还是依次遍历。<br>注：下列代码实现为升序排序。</p>\n</blockquote>\n<h2 id=\"插入排序\"><a href=\"#插入排序\" class=\"headerlink\" title=\"插入排序\"></a>插入排序</h2><h4 id=\"直接插入排序\"><a href=\"#直接插入排序\" class=\"headerlink\" title=\"直接插入排序\"></a>直接插入排序</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 从左往右遍历，依次将当前元素插入到前面已经有序的序列中</span></span><br><span class=\"line\"><span class=\"comment\"># 稳定</span></span><br><span class=\"line\"><span class=\"comment\"># 时间复杂度： O(n^2)</span></span><br><span class=\"line\"><span class=\"comment\"># 空间复杂度： O(1)</span></span><br><span class=\"line\"><span class=\"comment\"># 改进：可使用二分法插入排序（暂略）</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insert_sort</span>(<span class=\"params\">nums: <span class=\"built_in\">list</span></span>) -&gt; list:</span></span><br><span class=\"line\">    <span class=\"comment\"># 从左往右开始遍历</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, <span class=\"built_in\">len</span>(nums)):</span><br><span class=\"line\">        index = i</span><br><span class=\"line\">        temp = nums[i]</span><br><span class=\"line\">        <span class=\"keyword\">while</span> index &gt; <span class=\"number\">0</span> <span class=\"keyword\">and</span> nums[index - <span class=\"number\">1</span>] &gt; temp:</span><br><span class=\"line\">            <span class=\"comment\"># 若当前元素较小，则将有序的序列向后移</span></span><br><span class=\"line\">            nums[index] = nums[index - <span class=\"number\">1</span>]</span><br><span class=\"line\">            index -= <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"comment\"># 找到正确的位置，赋值</span></span><br><span class=\"line\">        nums[index] = temp</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums</span><br><span class=\"line\"></span><br><span class=\"line\">nums = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">3</span>, <span class=\"number\">55</span>, <span class=\"number\">64</span>, <span class=\"number\">25</span>]</span><br><span class=\"line\">print(insert_sort(nums))</span><br></pre></td></tr></table></figure>\n<h4 id=\"希尔排序\"><a href=\"#希尔排序\" class=\"headerlink\" title=\"希尔排序\"></a>希尔排序</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 在直接插入的基础上， 分组（间隔为n的为一组，这里初始值设置为数据的一半）依次插入，然后将n缩小一半，依次插入，如此迭代，直至 n 为 0</span></span><br><span class=\"line\"><span class=\"comment\"># 这样做的好处在于，避免直接插入时，若元素为逆序，需要过多次的插入和元素移动操作</span></span><br><span class=\"line\"><span class=\"comment\"># 不稳定</span></span><br><span class=\"line\"><span class=\"comment\"># 时间复杂度： </span></span><br><span class=\"line\"><span class=\"comment\"># 空间复杂度： O(1)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">shell_sort</span>(<span class=\"params\">nums: <span class=\"built_in\">list</span></span>) -&gt; list:</span></span><br><span class=\"line\">    <span class=\"comment\"># 初始分组设置为数组的一半</span></span><br><span class=\"line\">    group = <span class=\"number\">2</span></span><br><span class=\"line\">    n = <span class=\"built_in\">len</span>(nums)</span><br><span class=\"line\">    gap = n // group</span><br><span class=\"line\">    <span class=\"keyword\">while</span> gap &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(gap, n):</span><br><span class=\"line\">            temp = nums[i]</span><br><span class=\"line\">            j = i</span><br><span class=\"line\">            <span class=\"comment\"># 从右往左，依次插入</span></span><br><span class=\"line\">            <span class=\"keyword\">while</span> j &gt;= gap <span class=\"keyword\">and</span> nums[j - gap] &gt; temp:</span><br><span class=\"line\">                nums[j] = nums[j - gap]</span><br><span class=\"line\">                j -= gap</span><br><span class=\"line\">            nums[j] = temp</span><br><span class=\"line\">        <span class=\"comment\"># 分组长度减半</span></span><br><span class=\"line\">        gap //= group</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums</span><br><span class=\"line\"></span><br><span class=\"line\">nums = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">3</span>, <span class=\"number\">55</span>, <span class=\"number\">64</span>, <span class=\"number\">25</span>]</span><br><span class=\"line\">print(shell_sort(nums))</span><br></pre></td></tr></table></figure>\n<h2 id=\"交换排序\"><a href=\"#交换排序\" class=\"headerlink\" title=\"交换排序\"></a>交换排序</h2><h4 id=\"冒泡排序\"><a href=\"#冒泡排序\" class=\"headerlink\" title=\"冒泡排序\"></a>冒泡排序</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 相邻两进行比较，每一轮将最大值移到最后</span></span><br><span class=\"line\"><span class=\"comment\"># 不稳定</span></span><br><span class=\"line\"><span class=\"comment\"># 时间复杂度： O(n^2)</span></span><br><span class=\"line\"><span class=\"comment\"># 空间复杂度： O(1)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bubble_sort</span>(<span class=\"params\">nums: <span class=\"built_in\">list</span></span>) -&gt; list:</span></span><br><span class=\"line\">    <span class=\"comment\"># 每轮迭代，将最大值移到最后，故待排序序列长度减一</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> end <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(nums) - <span class=\"number\">1</span>, <span class=\"number\">1</span>, -<span class=\"number\">1</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 从左往右依次两两比较，将较大值移到后面</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, end):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> nums[i - <span class=\"number\">1</span>] &gt; nums[i]:</span><br><span class=\"line\">                nums[i - <span class=\"number\">1</span>], nums[i] = nums[i], nums[i - <span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums</span><br><span class=\"line\"></span><br><span class=\"line\">nums = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">3</span>, <span class=\"number\">55</span>, <span class=\"number\">64</span>, <span class=\"number\">25</span>]</span><br><span class=\"line\">print(bubble_sort(nums))</span><br></pre></td></tr></table></figure>\n<h4 id=\"快速排序\"><a href=\"#快速排序\" class=\"headerlink\" title=\"快速排序\"></a>快速排序</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 选择一个值，将比它大的移动右边，比它小的移到左边,故该值的位置已经找到；然后分别在左边和右边的序列中，继续选择一个值、划分，直至划分的序列中只有一个值</span></span><br><span class=\"line\"><span class=\"comment\"># 稳定</span></span><br><span class=\"line\"><span class=\"comment\"># 时间复杂度： O(nlogn)</span></span><br><span class=\"line\"><span class=\"comment\"># 空间复杂度： O(1)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 找出选择的值的位置</span></span><br><span class=\"line\"><span class=\"comment\"># 这里默认取序列第一个元素，来划分左右序列</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partition</span>(<span class=\"params\">nums: <span class=\"built_in\">list</span>, left: <span class=\"built_in\">int</span>, right: <span class=\"built_in\">int</span></span>) -&gt; int:</span></span><br><span class=\"line\">    targt = nums[left]</span><br><span class=\"line\">    mid, fast = left, right</span><br><span class=\"line\">    <span class=\"keyword\">while</span> mid != fast:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> nums[fast] &gt; targt <span class=\"keyword\">and</span> fast &gt; mid:</span><br><span class=\"line\">            fast -= <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> nums[fast] &lt; targt <span class=\"keyword\">and</span> fast &lt; mid:</span><br><span class=\"line\">            fast += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            nums[fast], nums[mid] = nums[mid], nums[fast]</span><br><span class=\"line\">            mid, fast = fast, mid</span><br><span class=\"line\">            <span class=\"keyword\">if</span> fast &gt;= mid:</span><br><span class=\"line\">                fast -= <span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"keyword\">elif</span> fast &lt; mid:</span><br><span class=\"line\">                fast += <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> mid</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 递归实现： 划分左右序列</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">quick_sort</span>(<span class=\"params\">nums, left, right</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> nums: <span class=\"keyword\">return</span> nums</span><br><span class=\"line\">    mid = partition(nums, left, right)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> left &lt; mid - <span class=\"number\">1</span>: quick_sort(nums, left, mid - <span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> right &gt; mid + <span class=\"number\">1</span>: quick_sort(nums, mid + <span class=\"number\">1</span>, right)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">nums = [<span class=\"number\">1</span>, <span class=\"number\">5</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">3</span>, <span class=\"number\">55</span>, <span class=\"number\">64</span>, <span class=\"number\">25</span>]</span><br><span class=\"line\">print(quick_sort(<span class=\"number\">0</span>, <span class=\"built_in\">len</span>(nums) - <span class=\"number\">1</span>, nums))</span><br></pre></td></tr></table></figure>\n<h2 id=\"选择排序\"><a href=\"#选择排序\" class=\"headerlink\" title=\"选择排序\"></a>选择排序</h2><h4 id=\"直接选择排序\"><a href=\"#直接选择排序\" class=\"headerlink\" title=\"直接选择排序\"></a>直接选择排序</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 确定最小值或最大值的位置，最后才交换</span></span><br><span class=\"line\"><span class=\"comment\"># 稳定</span></span><br><span class=\"line\"><span class=\"comment\"># 时间复杂度： O(n^2)</span></span><br><span class=\"line\"><span class=\"comment\"># 空间复杂度： O(1)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">select_sort</span>(<span class=\"params\">nums: <span class=\"built_in\">list</span></span>) -&gt; list:</span></span><br><span class=\"line\">    n = <span class=\"built_in\">len</span>(nums)</span><br><span class=\"line\">    <span class=\"comment\"># 从左往右依次遍历</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> start <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n - <span class=\"number\">1</span>):</span><br><span class=\"line\">        index = start</span><br><span class=\"line\">        <span class=\"comment\"># 确定比当前元素小的元素的位置</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(start + <span class=\"number\">1</span>, n):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> nums[index] &gt; nums[i]:</span><br><span class=\"line\">                index = i</span><br><span class=\"line\">        <span class=\"comment\"># 赋值</span></span><br><span class=\"line\">        nums[start], nums[index] = nums[index], nums[start]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums</span><br><span class=\"line\"></span><br><span class=\"line\">nums = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">3</span>, <span class=\"number\">55</span>, <span class=\"number\">64</span>, <span class=\"number\">25</span>]</span><br><span class=\"line\">print(select_sort(nums))    </span><br></pre></td></tr></table></figure>\n<h4 id=\"堆排序\"><a href=\"#堆排序\" class=\"headerlink\" title=\"堆排序\"></a>堆排序</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 从下往上，从左往右，创建最大堆</span></span><br><span class=\"line\"><span class=\"comment\"># 最大堆特征：父亲节点比其左右子树的节点的值都大</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># 时间复杂度： </span></span><br><span class=\"line\"><span class=\"comment\"># 空间复杂度： O(1)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建堆</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bulid_heap</span>(<span class=\"params\">nums: <span class=\"built_in\">list</span>, head: <span class=\"built_in\">int</span>, end: <span class=\"built_in\">int</span></span>) -&gt; list:</span></span><br><span class=\"line\">    larget = head</span><br><span class=\"line\">    <span class=\"comment\"># 当前节点的左孩子</span></span><br><span class=\"line\">    left = <span class=\"number\">2</span> * head + <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\"># 当前节点的右孩子</span></span><br><span class=\"line\">    right = <span class=\"number\">2</span> * head + <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"comment\"># 找到左右孩子中，比父亲节点大的节点的位置</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> left &lt; end <span class=\"keyword\">and</span> nums[left] &gt; nums[larget]:</span><br><span class=\"line\">        larget = left</span><br><span class=\"line\">    <span class=\"keyword\">if</span> right &lt; end <span class=\"keyword\">and</span> nums[right] &gt; nums[larget]:</span><br><span class=\"line\">        larget = right</span><br><span class=\"line\">    <span class=\"comment\"># 若孩子节点比父亲节点的值大，则交换</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> larget != head:</span><br><span class=\"line\">        nums[larget], nums[head] = nums[head], nums[larget]</span><br><span class=\"line\">        <span class=\"comment\"># 被交换的孩子节点作为父节点，继续向其孩子节点遍历</span></span><br><span class=\"line\">        bulid_heap(nums, larget, end)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 堆排序</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">heap_sort</span>(<span class=\"params\">nums</span>):</span></span><br><span class=\"line\">    n = <span class=\"built_in\">len</span>(nums)</span><br><span class=\"line\">    <span class=\"comment\"># 创建堆，从右往左依次插入节点</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n - <span class=\"number\">1</span>, -<span class=\"number\">1</span>, -<span class=\"number\">1</span>):</span><br><span class=\"line\">        bulid_heap(nums, i, n)</span><br><span class=\"line\">    <span class=\"comment\"># 排序，将最大值和末尾节点进行交换，然后将末尾节点之前的序列继续构建最大堆；如此遍历</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n - <span class=\"number\">1</span>, <span class=\"number\">0</span>, -<span class=\"number\">1</span>):</span><br><span class=\"line\">        nums[<span class=\"number\">0</span>], nums[j] = nums[j], nums[<span class=\"number\">0</span>]</span><br><span class=\"line\">        bulid_heap(nums, <span class=\"number\">0</span>, j)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nums</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">nums = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">3</span>, <span class=\"number\">55</span>, <span class=\"number\">64</span>, <span class=\"number\">25</span>]</span><br><span class=\"line\">print(heap_sort(nums)) </span><br></pre></td></tr></table></figure>\n<h2 id=\"归并排序\"><a href=\"#归并排序\" class=\"headerlink\" title=\"归并排序\"></a>归并排序</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 分治的思想</span></span><br><span class=\"line\"><span class=\"comment\"># </span></span><br><span class=\"line\"><span class=\"comment\"># 时间复杂度： </span></span><br><span class=\"line\"><span class=\"comment\"># 空间复杂度： O(n)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将两个有序的序列，依次从左往右比较，将较小的值插入新的序列，最后合成一个有序的序列</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">merge</span>(<span class=\"params\">nums, left, mid, right</span>):</span></span><br><span class=\"line\">    numsa = nums[left:mid + <span class=\"number\">1</span>]</span><br><span class=\"line\">    numsb = nums[mid + <span class=\"number\">1</span>: right + <span class=\"number\">1</span>]</span><br><span class=\"line\">    result = <span class=\"built_in\">list</span>()</span><br><span class=\"line\">    p, q = <span class=\"number\">0</span>, <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"comment\"># 两数组分别开始比较</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> p &lt; <span class=\"built_in\">len</span>(numsa) <span class=\"keyword\">and</span> q &lt; <span class=\"built_in\">len</span>(numsb):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> numsa[p] &gt;= numsb[q]:</span><br><span class=\"line\">            result.append(numsb[q])</span><br><span class=\"line\">            q += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            result.append(numsa[p])</span><br><span class=\"line\">            p += <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"comment\">#若其中一个数组还有值，则追加后末尾</span></span><br><span class=\"line\">    result.extend(numsa[p:])</span><br><span class=\"line\">    result.extend(numsb[q:])</span><br><span class=\"line\">    nums[left:right + <span class=\"number\">1</span>] = result</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 递归拆分</span></span><br><span class=\"line\"><span class=\"comment\"># 没有返回值，传入数组，默认在此词数组上移动排序</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">merge_sort</span>(<span class=\"params\">nums: <span class=\"built_in\">list</span>, start: <span class=\"built_in\">int</span>, end: <span class=\"built_in\">int</span></span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> start &lt; end:</span><br><span class=\"line\">        <span class=\"comment\"># 默认是对半拆分</span></span><br><span class=\"line\">        mid = (start + end) // <span class=\"number\">2</span></span><br><span class=\"line\">        merge_sort(nums, start, mid)</span><br><span class=\"line\">        merge_sort(nums, mid + <span class=\"number\">1</span>, end)</span><br><span class=\"line\">        merge(nums, start, mid, end)</span><br><span class=\"line\"></span><br><span class=\"line\">nums = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">3</span>, <span class=\"number\">55</span>, <span class=\"number\">64</span>, <span class=\"number\">25</span>]</span><br><span class=\"line\">merge_sort(nums, <span class=\"number\">0</span>, <span class=\"built_in\">len</span>(nums) - <span class=\"number\">1</span>)</span><br><span class=\"line\">print(nums)</span><br></pre></td></tr></table></figure>"}],"PostAsset":[{"_id":"source/_posts/built-blog/edit-blog-content.png","slug":"edit-blog-content.png","post":"ckjc7vn7v00002xul46abceyh","modified":0,"renderable":0},{"_id":"source/_posts/built-blog/initialize-blog.png","slug":"initialize-blog.png","post":"ckjc7vn7v00002xul46abceyh","modified":0,"renderable":0},{"_id":"source/_posts/built-blog/modify-setting1.png","slug":"modify-setting1.png","post":"ckjc7vn7v00002xul46abceyh","modified":0,"renderable":0},{"_id":"source/_posts/built-blog/new-repository.png","slug":"new-repository.png","post":"ckjc7vn7v00002xul46abceyh","modified":0,"renderable":0},{"_id":"source/_posts/built-blog/new-repository2.png","slug":"new-repository2.png","post":"ckjc7vn7v00002xul46abceyh","modified":0,"renderable":0},{"_id":"source/_posts/built-blog/ssh1.png","slug":"ssh1.png","post":"ckjc7vn7v00002xul46abceyh","modified":0,"renderable":0},{"_id":"source/_posts/built-blog/ssh2.png","slug":"ssh2.png","post":"ckjc7vn7v00002xul46abceyh","modified":0,"renderable":0},{"_id":"source/_posts/built-blog/ssh3.png","slug":"ssh3.png","post":"ckjc7vn7v00002xul46abceyh","modified":0,"renderable":0},{"_id":"source/_posts/built-blog/modify-theme.png","slug":"modify-theme.png","post":"ckjc7vn7v00002xul46abceyh","modified":0,"renderable":0}],"PostCategory":[{"post_id":"ckj9nnqbb0000z7ulbe84czii","category_id":"ckj9tclpp0000enulenk9bjff","_id":"ckj9tclps0003enul83kg5eav"},{"post_id":"ckjc7vn7v00002xul46abceyh","category_id":"ckj9znbx200015hul2ht3cdit","_id":"ckjc7vn8700032xulftwj04no"},{"post_id":"cl2bxmehv000082ul1omh4b2e","category_id":"cl2bxmehx000182ul599n5zjx","_id":"cl2bxmehz000682ul5t3a0b65"},{"post_id":"cl2bubgcy0000wwulcc3nargo","category_id":"cl2bxmehy000382ulal7i9t9u","_id":"cl2bxmehz000882ul0luhbjiv"}],"PostTag":[{"post_id":"ckj9nnqbb0000z7ulbe84czii","tag_id":"ckj9tfvg10000h2ul4cgshibo","_id":"ckj9tfvg30001h2ul1wnx3d3n"},{"post_id":"ckjc7vn7v00002xul46abceyh","tag_id":"ckj9znbx400025hula9b86x9w","_id":"ckjc7vn8600012xulauvrhw46"},{"post_id":"ckjc7vn7v00002xul46abceyh","tag_id":"ckj9znbx500035hulds2cccck","_id":"ckjc7vn8700022xul1pxcfqye"},{"post_id":"ckjc7vn7v00002xul46abceyh","tag_id":"ckj9znbx600055hulcikf6wdq","_id":"ckjc7vn8700042xulgeqh4i81"},{"post_id":"cl2bxmehv000082ul1omh4b2e","tag_id":"cl2bxmehy000282ul5guz9hg9","_id":"cl2bxmehz000582ulbqdxb9cc"},{"post_id":"cl2bubgcy0000wwulcc3nargo","tag_id":"cl2bubgd00002wwul7bh8ewhd","_id":"cl2bz5y210000v4ulf22x11wk"}],"Tag":[{"name":"hahahah","_id":"ckj9o7ts500016mulh6qs7w61"},{"name":"test","_id":"ckj9s94c100001hul5zvya7mt"},{"name":"默认","_id":"ckj9tclpr0001enula3gr1fp4"},{"name":"default","_id":"ckj9tfvg10000h2ul4cgshibo"},{"name":"GitHub","_id":"ckj9znbx400025hula9b86x9w"},{"name":"Hexo","_id":"ckj9znbx500035hulds2cccck"},{"name":"Mac","_id":"ckj9znbx600055hulcikf6wdq"},{"name":"data science","_id":"cl2bubgd00002wwul7bh8ewhd"},{"name":"sort algorithm python","_id":"cl2bxmehy000282ul5guz9hg9"},{"name":"数据科学 数据分析","_id":"cl2bxmehy000482uldqqkfiso"}]}}